{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>  [python : 3.12]   [codestyle : black]   [linter : ruff]   [package manager : pixi]  </p>"},{"location":"#pypsa-at-documentation","title":"pypsa-at Documentation","text":"<p>This site hosts the documentation for <code>pypsa-at</code></p> <p>first, build the scenarios using the public DB access <pre><code>snakemake build_scenarios -f --cores 'all'\n</code></pre> This is necessary, because PyPSA-AT uses assumptions from PyPSA-DE scenarios.  </p> <p>Second, start the analysis: <pre><code>snakemake all --cores 'all'\n</code></pre></p>"},{"location":"#table-of-contents","title":"Table Of Contents","text":"<p>The documentation follows the best practice for project documentation as described in the Di\u00e1taxis documentation framework and consists of four separate parts:</p> <ol> <li>How-To Guides</li> <li>Tutorials</li> <li>Reference</li> <li>Explanation</li> </ol> <p>Quickly find what you're looking for depending on your use case by looking at the different pages.</p> <p>This documentation is built using MkDocs, mkdocstrings, and the Material for MkDocs theme.</p>"},{"location":"explanations/","title":"Explanations","text":"<p>This part of the project documentation focuses on an understanding-oriented approach. You'll get a chance to read about the background of the project, as well as reasoning about how it was implemented.</p> <p>Note: Expand this section by considering the following points:</p> <ul> <li>Give context and background on your library</li> <li>Explain why you created it</li> <li>Provide multiple examples and approaches of how     to work with it</li> <li>Help the reader make connections</li> <li>Avoid writing instructions or technical descriptions     here</li> </ul>"},{"location":"explanations/#dependencies","title":"Dependencies","text":"<p>The package esmtools uses a variety of inputs from PyPSA model runs.</p>"},{"location":"explanations/#folder-structure","title":"Folder Structure","text":"<ul> <li>results folder</li> <li>scenario subfolder</li> <li>resource folder</li> <li>interpolated data folder</li> </ul>"},{"location":"explanations/#input-files","title":"Input Files","text":"<ul> <li>(post)networks</li> <li>config.yaml</li> <li>nodal_energy.csv</li> <li>industry.csv</li> <li>costs.csv</li> </ul>"},{"location":"explanations/#the-postnetwork","title":"The postnetwork","text":"<ul> <li>FK-relations (and their exceptions)</li> </ul>"},{"location":"explanations/#pypsa-statistics","title":"PyPSA Statistics","text":"<ul> <li>explain reindex operation and groupers.</li> <li>explain statistics local functions</li> <li>explain aggregation functions: aggregate_time and aggregate_components</li> </ul>"},{"location":"explanations/#esmtools-concept-and-workflow","title":"esmtools Concept and Workflow","text":"<ol> <li>fetch statistics via esmtools.statistic.collect_myopic_statistics</li> <li>combine statistics to a metric</li> <li>configure metric for export</li> <li>export metric</li> </ol>"},{"location":"how-to-guides/","title":"How To Guides","text":"<p>This part of the project documentation focuses on a problem-oriented approach. You'll tackle common tasks that you might have, with the help of the code provided in this project.</p>"},{"location":"how-to-guides/soft-fork-merge-upstream/","title":"Soft Fork Upstream Merge","text":"<p>add the wanted upstream remotes to git <pre><code>git remote add pypsa-de https://github.org/PyPSA/pypsa-de.git\n</code></pre></p> <p>fetch the upstream remotes <pre><code>git fetch pypsa-de \n</code></pre></p> <p>checkout a new branch to prevent conflicts in your main branch  <pre><code>git checkout -b pypsa-de-merge\n</code></pre></p> <p>merge the upstream branch into your branch  <pre><code>git merge pypsa-de/main\n</code></pre></p> <p>resolve any merge conflicts in your IDE and assert that the code is working as expected.  Open a pull request or merge directly into origin/main if you've got permissions to do so.  <pre><code>git merge main\n</code></pre></p>"},{"location":"reference/","title":"Function Reference Guide","text":"<p>This part of the project documentation focuses on an information-oriented approach. Use it as a reference for the technical implementation of the <code>esmtools</code> project code.</p>"},{"location":"reference/evals/","title":"The Evaluations module","text":""},{"location":"reference/evals/cli/","title":"cli.py","text":"<p>Command Line Interface to run evaluations.</p> <p>Commands may be run from anywhere as long as the virtual environment is activated and the esmtools project is installed.</p> <p>Examples:</p> <pre><code># run a single evaluation by name\nrun_eval \"/opt/data/esm/results\" -n \"eval_capacity_factor\"\n</code></pre> <pre><code># run multiple evaluations by name\nrun_eval \"/opt/data/esm/results\" -n \"eval_capacity_factor\" -n \"eval_transmission_grid\"\n</code></pre> <pre><code># run all evaluations\nrun_eval \"/opt/data/esm/results\"\n</code></pre> <pre><code># run evaluations as a script and from project root without installing the package\n(but with your virtual env activated of course)\n(pypsa-at)$ PYTHONPATH=\"./\" python evals/cli.py \"results/v2025.02/KN2045_Mix\" -n \"view_balance_heat\"\n</code></pre>"},{"location":"reference/evals/cli/#evals.cli.run_eval","title":"<code>run_eval(result_path, sub_directory, names, config_override, fail_fast)</code>","text":"<p>Execute evaluation functions from the evals module.</p> <p>Find evaluation functions must be registered under evals.__init__.__all__ to be exposed and ultimately be found by this function. Keep that in mind when adding new evaluations.</p> <p>All evaluation function are expected to expose the same interface. The evaluation function arguments are listed in the evals module reference section.</p> <p>Parameters:</p> Name Type Description Default <code>result_path</code> <code>click.Path</code> <p>The path to the result folder, usually ./pypsa-eur-sec/results. Note, that running on copied result folders might fail due to missing resource files.</p> required <code>sub_directory</code> <code>str</code> <p>The subdirectory in the results folder that contains the network files.</p> required <code>names</code> <code>list</code> <p>A list of evaluation name, e.g. \"eval_electricity_amounts\", optional. Defaults to running all evaluations from evals.all.</p> required <code>config_override</code> <code>str</code> <p>A path to a config.toml file with the same section as the config.defaults.toml used to override configurations used by view functions.</p> required <code>fail_fast</code> <code>bool</code> <p>Whether to raise Exceptions or to run all functions, defaults to running all functions.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Exits the program with the number of failed evaluations as exit code.</p> Source code in <code>evals/cli.py</code> <pre><code>@click.command()\n@click.argument(\"result_path\", type=click.Path(exists=True), required=True)\n@click.option(\n    \"--sub_directory\",\n    \"-s\",\n    type=str,\n    required=False,\n    default=\"networks\",\n)\n@click.option(\"--names\", \"-n\", multiple=True, required=False, default=[])\n@click.option(\n    \"--config_override\",\n    \"-c\",\n    type=click.Path(exists=True),\n    multiple=False,\n    required=False,\n    default=None,\n)\n@click.option(\n    \"--fail_fast\", \"-f\", type=bool, multiple=False, required=False, default=False\n)\ndef run_eval(\n    result_path: click.Path,\n    sub_directory: str,\n    names: list,\n    config_override: str,\n    fail_fast: bool,\n) -&gt; None:\n    r\"\"\"\n    Execute evaluation functions from the evals module.\n\n    Find evaluation functions must be registered under\n    evals.\\__init__.\\__all__ to be exposed and ultimately be found\n    by this function. Keep that in mind when adding new evaluations.\n\n    All evaluation function are expected to expose the same interface.\n    The evaluation function arguments are listed in the evals module\n    [reference section](evals/index.md).\n\n    Parameters\n    ----------\n    result_path\n        The path to the result folder, usually ./pypsa-eur-sec/results.\n        Note, that running on copied result folders might fail\n        due to missing resource files.\n    sub_directory\n        The subdirectory in the results folder that contains the network files.\n    names\n        A list of evaluation name, e.g. \"eval_electricity_amounts\",\n        optional. Defaults to running all evaluations from\n        evals.__all__.\n    config_override\n        A path to a config.toml file with the same section as\n        the config.defaults.toml used to override configurations\n        used by view functions.\n    fail_fast\n        Whether to raise Exceptions or to run all functions, defaults to\n        running all functions.\n\n    Returns\n    -------\n    :\n        Exits the program with the number of failed evaluations as exit\n        code.\n    \"\"\"\n    import evals.views as views\n    from evals.fileio import read_networks, read_views_config\n\n    eval_functions = [\n        getattr(views, fn) for fn in views.__all__ if (not names or fn in names)\n    ]\n    n_evals = len(eval_functions)\n\n    if n_evals == 0:\n        sys.exit(f\"Found no evaluation functions named: {names}\")\n    logger.info(f\"Selected {n_evals} evaluation functions.\")\n\n    networks = read_networks(result_path, sub_directory=sub_directory)\n\n    fails = []\n    run_start = time()\n    for i, func in enumerate(eval_functions, start=1):\n        logger.info(f\"({i}/{n_evals}) Start {func.__name__}...\")\n        eval_start = time()\n        try:\n            config = read_views_config(func, config_override)\n            func(result_path=result_path, networks=networks, config=config)\n        except Exception as e:\n            logger.exception(f\"Exception during {func.__name__}.\", exc_info=True)\n            fails.append(func.__name__)\n            if fail_fast:\n                raise e\n        else:\n            logger.info(\n                f\"Executing {func.__name__} took {time() - eval_start:.2f} seconds.\"\n            )\n        finally:\n            logger.info(f\"Finished {func.__name__}.\")\n\n    logger.info(\n        f\"Full run took {time() - run_start:.2f} seconds.\"\n        f\"\\nNumber of Errors: {len(fails)} {fails or ''}\"\n    )\n    sys.exit(len(fails))\n</code></pre>"},{"location":"reference/evals/cli/#evals.cli.run_tests","title":"<code>run_tests()</code>","text":"<p>Run test suite in a dev environment.</p> Source code in <code>evals/cli.py</code> <pre><code>@click.command()\ndef run_tests() -&gt; None:\n    \"\"\"Run test suite in a dev environment.\"\"\"\n    # delayed import to skip dependency in production environments\n    import pytest\n\n    rc = pytest.main()\n    sys.exit(rc)\n</code></pre>"},{"location":"reference/evals/configs/","title":"configs.py","text":"<p>Module to collect configuration items and their default values.</p>"},{"location":"reference/evals/configs/#evals.configs.ExcelConfig","title":"<code>ExcelConfig</code>  <code>dataclass</code>","text":"<p>Holds configuration items for Excel file.</p> Source code in <code>evals/configs.py</code> <pre><code>@dataclass()\nclass ExcelConfig:\n    \"\"\"Holds configuration items for Excel file.\"\"\"\n\n    axis_labels: list = None\n    chart: str = \"stacked\"  # 'stacked', 'clustered', 'standard', 'percentStacked', None\n    chart_title: str = None\n    chart_width: int = 20  # cm\n    chart_switch_axis: bool = False  # switch categories with x-axis\n    chart_colors: dict = field(\n        default_factory=lambda: {k: v.lstrip(\"#\") for k, v in COLOUR_SCHEME_BMK.items()}\n    )\n    # pivot tables to use the following labels as index or column\n    pivot_index: str | list = field(\n        default_factory=lambda: [DataModel.LOCATION, DataModel.CARRIER]\n    )\n    pivot_columns: str | list = DataModel.YEAR\n</code></pre>"},{"location":"reference/evals/configs/#evals.configs.PlotConfig","title":"<code>PlotConfig</code>  <code>dataclass</code>","text":"<p>Holds configuration items for Plotly figures.</p> Source code in <code>evals/configs.py</code> <pre><code>@dataclass()\nclass PlotConfig:\n    \"\"\"Holds configuration items for Plotly figures.\"\"\"\n\n    title: str = None\n    chart = None  # ESMBarChart | ESMGroupedBarChart | ESMTimeSeriesChart\n    file_name_template: str = \"{metric}_{year}_{location}\"\n    unit: str = \"\"  # default is metric.df.attrs[\"unit\"]\n\n    # the metric data frame is grouped this index level before plotting.\n    # One html figure is created per resulting group.\n    plotby: list = field(default_factory=lambda: [DataModel.LOCATION])\n\n    # Used to pivot the data frame before sending it to the plotter. The\n    # specified index/column levels will be in the plot data frame. The\n    # rest is aggregated (summed up).\n    pivot_index: list = field(default_factory=lambda: DataModel.YEAR_IDX_NAMES)\n    pivot_columns: list = field(default_factory=lambda: [])\n\n    plot_category: str = DataModel.CARRIER\n    plot_xaxis: str = DataModel.YEAR\n\n    # defines the subplots in GroupedBarChart\n    facet_column: str = DataModel.BUS_CARRIER\n\n    category_orders: tuple = ()\n    colors: dict = field(default_factory=lambda: COLOUR_SCHEME_BMK)\n    pattern: dict = field(\n        default_factory=lambda: dict.fromkeys(\n            [\n                Group.import_foreign,\n                Group.export_foreign,\n                Group.import_domestic,\n                Group.export_domestic,\n                Group.import_net,\n                Group.export_net,\n                Group.import_global,\n            ],\n            \"/\",\n        )\n    )\n    fill: dict = field(default_factory=dict)\n    stacked: bool = True\n    line_dash: dict = field(default_factory=dict)\n    line_width: dict = field(default_factory=dict)\n    line_shape: str = \"hv\"\n    legend_header: str = \"Categories\"\n    xaxis_title: str = \"&lt;b&gt;Years&lt;/b&gt;\"\n    yaxis_color: str = \"DarkSlateGrey\"\n    footnotes: tuple = (\"\", \"\")\n    cutoff: float = 0.0001  # needs update depending on unit\n    cutoff_drop: bool = True  # only effective in BarCharts\n\n    legend_font_size: int = 20\n    title_font_size: int = 30\n    font_size: int = 20\n    xaxis_font_size: int = 20\n    yaxes_showgrid: bool = False\n    yaxes_visible: bool = False\n</code></pre>"},{"location":"reference/evals/configs/#evals.configs.ViewDefaults","title":"<code>ViewDefaults</code>  <code>dataclass</code>","text":"<p>Holds all configuration items needed to export Metrics.</p> <p>The 'excel' and 'plotly' fields are processed by the export_excel and export_plotly methods, respectively. Both configuration spaces are kept separate to keep the variable space small during export.</p> Source code in <code>evals/configs.py</code> <pre><code>@dataclass()\nclass ViewDefaults:\n    \"\"\"\n    Holds all configuration items needed to export Metrics.\n\n    The 'excel' and 'plotly' fields are processed by the export_excel\n    and export_plotly methods, respectively. Both configuration spaces\n    are kept separate to keep the variable space small during export.\n    \"\"\"\n\n    excel: ExcelConfig = field(default_factory=lambda: ExcelConfig())\n    plotly: PlotConfig = field(default_factory=lambda: PlotConfig())\n</code></pre>"},{"location":"reference/evals/constants/","title":"constants.py","text":"<p>Collect constant values and identifiers used for evaluations.</p> <p>Values in this module do not need to be changed during runtime.</p>"},{"location":"reference/evals/constants/#evals.constants.BusCarrier","title":"<code>BusCarrier</code>","text":"<p>Container to collect all bus carrier names.</p> Source code in <code>evals/constants.py</code> <pre><code>class BusCarrier:\n    \"\"\"Container to collect all bus carrier names.\"\"\"\n\n    AC: str = \"AC\"\n    DC: str = \"DC\"\n    CH4: str = \"gas\"\n    H2: str = \"H2\"\n    TRANSPORT_P: str = \"passenger transport\"\n    TRANSPORT_P_LONG: str = \"passenger transport long\"\n    FT: str = \"Fischer-Tropsch\"\n    FT_1: str = \"Fischer-Tropsch 1\"\n    FT_2: str = \"Fischer-Tropsch 2\"\n    HEAT_URBAN_CENTRAL: str = \"urban central heat\"\n    HEAT_URBAN_DECENTRAL: str = \"urban decentral heat\"\n    HEAT_RURAL: str = \"rural heat\"\n    # ESM heat buses:\n    # HEAT_URBAN_SERVICES: str = \"services urban decentral heat\"\n    # HEAT_URBAN_RESIDENTIAL: str = \"residential urban decentral heat\"\n    # HEAT_RURAL_SERVICES: str = \"services rural heat\"\n    # HEAT_RURAL_RESIDENTIAL: str = \"residential rural heat\"\n    LI_ION: str = \"Li ion\"\n    BATTERY: str = \"battery\"\n    HOME_BATTERY: str = \"home battery\"\n    EV_BATTERY: str = \"EV battery\"\n    SOLID_BIOMASS: str = \"solid biomass\"\n\n    @classmethod\n    def ac_stores(cls) -&gt; list:\n        return [\n            cls.AC,\n            cls.DC,\n            cls.LI_ION,\n            cls.BATTERY,\n            cls.HOME_BATTERY,\n            cls.EV_BATTERY,\n        ]\n\n    @classmethod\n    def heat_buses(cls) -&gt; list:\n        return [cls.HEAT_URBAN_CENTRAL, cls.HEAT_URBAN_DECENTRAL, cls.HEAT_RURAL]\n</code></pre>"},{"location":"reference/evals/constants/#evals.constants.COLOUR","title":"<code>COLOUR</code>","text":"<p>Container to collect colour codes in hex format.</p> Source code in <code>evals/constants.py</code> <pre><code>class COLOUR:\n    \"\"\"Container to collect colour codes in hex format.\"\"\"\n\n    coral: str = \"#E8B5B1\"\n    raspberry: str = \"#961454\"\n    salmon: str = \"#E19990\"\n    rose: str = \"#D5A1BB\"\n    peach: str = \"#EBBFBA\"\n\n    red: str = \"#CA0638\"\n    red_chestnut: str = \"#96332C\"\n    red_bright: str = \"#E53212\"\n    red_deep: str = \"#B20633\"\n    red_fire: str = \"#E63313\"\n\n    green: str = \"#3C703E\"\n    green_light: str = \"#509554\"\n    green_ocean: str = \"#3DCCBF\"\n    green_mint: str = \"#B0D4B2\"\n    green_sage: str = \"#82B973\"\n    turquoise: str = \"#e8e8e8\"\n\n    grey_light: str = \"#ECECEC\"\n    grey_dark: str = \"#535353\"\n    grey_charcoal: str = \"#485055\"\n    grey_deep: str = \"#3C3C3C\"\n    grey_cool: str = \"#919699\"\n    grey_silver: str = \"#D0D0D0\"\n    grey_neutral: str = \"#9F9F9F\"\n\n    black: str = \"#000000\"\n\n    brown: str = \"#C58000\"\n    brown_dark: str = \"#b37400\"\n    brown_sallow: str = \"#bf9c5c\"\n    brown_light: str = \"#e8cc99\"\n    brown_deep: str = \"#4d3200\"\n\n    blue_pastel: str = \"#B5C9D5\"\n    blue_moonstone: str = \"#3DACBF\"\n    blue_dark: str = \"#5F5F5F\"\n    blue_persian: str = \"#0064A2\"\n    blue_celestial: str = \"#4F8FCD\"\n    blue_cerulean: str = \"#005082\"\n    blue_sky: str = \"#99C1DA\"\n    blue_lavender: str = \"#636EFA\"\n\n    orange: str = \"#FF6600\"\n    orange_mellow: str = \"#FECB52\"\n\n    yellow_bright: str = \"#FED500\"\n    yellow_vivid: str = \"#FEC500\"\n    yellow_canary: str = \"#FFDE53\"\n    yellow_golden: str = \"#FFB200\"\n</code></pre>"},{"location":"reference/evals/constants/#evals.constants.Carrier","title":"<code>Carrier</code>","text":"<p>Container to collect all carrier names.</p> Source code in <code>evals/constants.py</code> <pre><code>class Carrier:\n    \"\"\"Container to collect all carrier names.\"\"\"\n\n    chp_urban_central_lignite_cc: str = \"urban central lignite CHP CC electric\"\n    chp_urban_central_lignite: str = \"urban central lignite CHP electric\"\n    chp_urban_central_coal_cc: str = \"urban central coal CHP CC electric\"\n    chp_urban_central_coal: str = \"urban central coal CHP electric\"\n    chp_urban_central_ch4_cc: str = \"urban central gas CHP CC electric\"\n    chp_urban_central_ch4: str = \"urban central gas CHP electric\"\n    chp_urban_central_solid_biomass_cc: str = \"urban central solid biomass CHP CC\"\n    chp_urban_central_solid_biomass: str = \"urban central solid biomass CHP\"\n\n    chp_urban_decentral_micro_ch4: str = \"residential urban decentral micro gas CHP\"\n    chp_urban_decentral_services_micro_ch4: str = (\n        \"services urban decentral micro gas CHP\"\n    )\n\n    chp_rural_residential_micro_ch4: str = \"residential rural micro gas CHP\"\n    chp_rural_services_micro_ch4: str = \"services rural micro gas CHP\"\n\n    pemfc_urban_services_decentral_ch4_smr: str = (\n        \"services urban decentral CH4-powered PEMFC with internal SMR\"\n    )\n    pemfc_rural_services_decentral_ch4_smr: str = (\n        \"residential rural CH4-powered PEMFC with internal SMR\"\n    )\n    pemfc_rural_services_ch4_smr: str = (\n        \"services rural CH4-powered PEMFC with internal SMR\"\n    )\n    pemfc_urban_residential_decentral_ch4_smr: str = (\n        \"residential urban decentral CH4-powered PEMFC with internal SMR\"\n    )\n    pemfc_rural_services_h2_smr: str = \"services rural H2-powered PEMFC\"\n\n    pemfc_urban_services_decentral_h2: str = \"services urban decentral H2-powered PEMFC\"\n    pemfc_rural_residential_h2: str = \"residential rural H2-powered PEMFC\"\n    pemfc_urban_residential_decentral_h2_smr: str = (\n        \"residential urban decentral H2-powered PEMFC\"\n    )\n\n    pp_lignite_cc: str = \"lignite power plant (CC)\"\n    pp_lignite: str = \"lignite power plant\"\n    pp_coal: str = \"coal power plant\"\n    pp_coal_cc: str = \"coal power plant (CC)\"\n    pp_oil: str = \"oil power plant\"\n\n    ocgt: str = \"OCGT\"\n\n    nuclear: str = \"nuclear\"\n    onwind_1: str = \"onwind-1\"\n    onwind_2: str = \"onwind-2\"\n    onwind_3: str = \"onwind-3\"\n    onwind_4: str = \"onwind-4\"\n    offwind_ac: str = \"offwind-ac\"\n    offwind_dc: str = \"offwind-dc\"\n    ror: str = \"ror\"\n    phs: str = \"PHS\"\n    hydro: str = \"hydro\"\n\n    solar_rooftop: str = \"solar-rooftop\"\n    solar_utility: str = \"solar-utility\"\n    h2_fuel_cell: str = \"H2 Fuel Cell\"\n    lost_load: str = \"value of lost load\"\n    battery_discharger: str = \"battery discharger\"\n    battery: str = \"battery\"\n\n    ft_1: str = \"Fischer-Tropsch 1\"\n    ft_2: str = \"Fischer-Tropsch 2\"\n    h2_electrolysis: str = \"H2 Electrolysis\"\n    h2_electrolysis_ht: str = \"H2 HT Electrolysis\"\n    smr: str = \"SMR\"\n    smr_cc: str = \"SMR CC\"\n    sabatier: str = \"Sabatier\"\n    biogas_approximation: str = \"biogas approximation\"\n    helmeth: str = \"helmeth\"\n    ch4: str = \"gas\"\n    h2_cavern: str = \"H2 cavern\"\n    h2_tube: str = \"H2 tube\"\n    ft_import_link_1: str = \"Fischer-Tropsch import link 1\"\n    ft_import_link_2: str = \"Fischer-Tropsch import link 2\"\n    h2_import_capacity_foreign: str = \"import capacity H2 foreign\"\n    h2_import_capacity_domestic: str = \"import capacity H2 domestic\"\n    ch4_import_capacity_foreign: str = \"import capacity gas foreign\"\n    ch4_import_capacity_domestic: str = \"import capacity gas domestic\"\n    domestic_homes_and_trade: str = \"domestic homes and trade\"\n    road_freight_ac: str = \"electricity road freight\"\n    industry: str = \"industry\"\n    industry_new_electricity: str = \"industry new electricity\"\n    grid_losses: str = \"urban central heat losses\"\n    electricity_rail: str = \"electricity rail\"\n    phev_short: str = \"PHEV short\"\n    phev_long: str = \"PHEV long\"\n\n    v2g: str = \"V2G\"\n    bev: str = \"BEV\"\n    bev_charger: str = \"BEV charger\"\n    bev_passenger_withdrawal: str = \"BEV to passenger used\"\n    bev_charger_supply: str = \"BEV charger out\"\n    bev_charger_draw: str = \"BEV charger in\"\n    bev_charger_losses: str = \"BEV charger losses\"\n    v2g_supply: str = \"V2G energy back to network\"\n    v2g_withdrawal: str = \"V2G energy draw\"\n\n    dac: str = \"DAC\"\n    heat_pump_residential_rural_ground: str = \"residential rural ground heat pump\"\n    heat_pump_ground_services_rural: str = \"services rural ground heat pump\"\n    resistive_heater_rural_services: str = \"services rural resistive heater\"\n    resistive_heater_rural_residential: str = \"residential rural resistive heater\"\n    heat_pump_air_urban_residential_decentral: str = (\n        \"residential urban decentral air heat pump\"\n    )\n    resistive_heater_urban_decentral_residential: str = (\n        \"residential urban decentral resistive heater\"\n    )\n    heat_pump_air_services_urban_decentral: str = (\n        \"services urban decentral air heat pump\"\n    )\n    resistive_heater_services_urban_decentral: str = (\n        \"services urban decentral resistive heater\"\n    )\n\n    heat_pump_air_urban_central: str = \"urban central air heat pump\"\n    resistive_heater_urban_central: str = \"urban central resistive heater\"\n    export_foreign: str = \"foreign export\"\n    export_domestic: str = \"domestic export\"\n    phs_dispatched_power_inflow: str = \"PHS Dispatched Power from Inflow\"\n    hydro_dispatched_power: str = \"hydro Dispatched Power\"\n    import_domestic: str = \"domestic import\"\n    import_foreign: str = \"foreign import\"\n    ch4_from_sabatier: str = \"Gas from Sabatier\"\n    biogas_to_ch4: str = \"biogas to gas\"\n    AC: str = \"AC\"\n    DC: str = \"DC\"\n    gas_pipepline: str = \"gas pipeline\"\n    gas_pipepline_new: str = \"gas pipeline new\"\n    ch4_generator: str = \"gas generator\"\n    ch4_import_foreign: str = \"gas foreign import\"\n    cng_long: str = \"CNG long\"\n    cng_short: str = \"CNG short\"\n    ch4_store: str = \"gas Store\"\n    ch4_navigation_domestic: str = \"gas domestic navigation\"\n    ch4_feedstock: str = \"gas feedstock\"\n    ch4_industry: str = \"gas for industry\"\n    ch4_industry_cc: str = \"gas for industry CC\"\n    ch4_navigation_international: str = \"gas international navigation\"\n    road_freight_ch4: str = \"gas road freight\"\n    ch4_boiler_residential_rural: str = \"residential rural gas boiler\"\n    ch4_boiler_services_rural: str = \"services rural gas boiler\"\n\n    chp_urban_central_ch4_heat_cc: str = \"urban central gas CHP CC heat\"\n    chp_urban_central_ch4_heat: str = \"urban central gas CHP heat\"\n    ch4_boiler_urban_central: str = \"urban central gas boiler\"\n    export_net: str = \"Net Export\"\n    ch4_for_smr_cc: str = \"Gas for SMR CC\"\n    ch4_for_smr: str = \"Gas for SMR\"\n    ch4_boiler_urban_decentral_services: str = \"services urban decentral gas boiler\"\n    ch4_boiler_urban_decentral_residential: str = (\n        \"residential urban decentral gas boiler\"\n    )\n    export_ch4_foreign: str = \"gas foreign export\"\n    export_ch4_domestic: str = \"gas domestic export\"\n    import_net: str = \"Net Import\"\n\n    h2_from_smr: str = \"H2 from SMR\"\n    h2_from_smr_cc: str = \"H2 from SMR CC\"\n    h2_import_russia: str = \"H2 Import RU\"\n    h2_import_naf: str = \"H2 Import NAF\"\n    h2_import_foreign: str = \"H2 foreign import\"\n    h2_import_domestic: str = \"H2 domestic import\"\n    h2_import_foreign_retro: str = \"H2 retro foreign import\"\n    h2_import_domestic_retro: str = \"H2 retro domestic import\"\n    fcev_long: str = \"FCEV long\"\n    fcev_short: str = \"FCEV short\"\n    h2_sabatier: str = \"H2 for Sabatier\"\n    h2_pipeline: str = \"H2 pipeline\"\n    h2_pipeline_retro: str = \"H2 pipeline retrofitted\"\n    h2_pipeline_kernnetz: str = \"H2 pipeline (Kernnetz)\"\n    road_freight_h2: str = \"H2 road freight\"\n    h2_industry: str = \"H2 for industry\"\n    h2_shipping: str = \"H2 for shipping\"\n    h2_rail: str = \"H2 for rail\"\n    h2_aviation: str = \"H2 for aviation\"\n    h2_export_foreign: str = \"H2 foreign export\"\n    h2_export_foreign_retro: str = \"H2 retro foreign export\"\n    h2_export_domestic: str = \"H2 domestic export\"\n    h2_export_domestic_retro: str = \"H2 retro domestic export\"\n    road_freight_ft: str = \"Fischer-Tropsch road freight\"\n    ft_rail: str = \"Fischer-Tropsch rail\"\n    ft_domestic_navigation: str = \"Fischer-Tropsch domestic navigation\"\n    ft_domestic_aviation: str = \"Fischer-Tropsch domestic aviation\"\n    ft_industry: str = \"Fischer-Tropsch industry\"\n    hard_coal_industry: str = \"hard coal industry\"\n    process_emissions: str = \"process emissions\"\n    process_emissions_cc: str = \"process emissions CC\"\n    ice_short: str = \"ICE short\"\n    ice_long: str = \"ICE long\"\n    hev_short: str = \"HEV short\"\n    hev_long: str = \"HEV long\"\n\n    chp_urban_central_coal_heat: str = \"urban central coal CHP heat\"\n    chp_urban_central_lignite_heat: str = \"urban central lignite CHP heat\"\n    chp_urban_central_coal_heat_cc: str = \"urban central coal CHP CC heat\"\n    chp_urban_central_lignite_heat_cc: str = \"urban central lignite CHP CC heat\"\n    oil: str = \"oil\"\n    oil_boiler_rural_services: str = \"services rural oil boiler\"\n    oil_boiler_rural_residential: str = \"residential rural oil boiler\"\n    oil_boiler_urban_residential: str = \"residential urban decentral oil boiler\"\n    ft_import_1: str = \"Fischer-Tropsch import 1\"\n    co2_vent: str = \"co2 vent\"\n    h2_store: str = \"H2 Store\"\n    solid_biomass_boiler_urban_central: str = \"urban central solid biomass boiler\"\n    solid_biomass_boiler_urban_central_cc: str = \"urban central solid biomass boiler CC\"\n    solar_thermal_collector_urban_central: str = \"urban central solar thermal collector\"\n    water_tanks_discharger_urban_central: str = \"urban central water tanks discharger\"\n    water_tanks_charger_urban_central: str = \"urban central water tanks charger\"\n    low_temperature_heat_for_industry: str = \"low-temperature heat for industry\"\n    hh_and_services: str = \"hh and services\"\n    value_lost_load: str = \"value of lost load\"\n\n    # derivative metric names\n    bev_demand: str = \"BEV to passenger demand\"\n    bev_losses: str = \"BEV to passenger losses\"\n    v2g_demand: str = \"V2G energy demand\"\n    v2g_losses: str = \"V2G energy total losses\"\n</code></pre>"},{"location":"reference/evals/constants/#evals.constants.DataModel","title":"<code>DataModel</code>","text":"<p>Metric data model constants.</p> Source code in <code>evals/constants.py</code> <pre><code>class DataModel:\n    \"\"\"Metric data model constants.\"\"\"\n\n    LOCATION: str = \"location\"\n    COMPONENT: str = \"component\"\n    CARRIER: str = \"carrier\"\n    BUS_CARRIER: str = \"bus_carrier\"\n    METRIC: str = \"metric\"\n    YEAR: str = \"year\"\n    SNAPSHOTS: str = \"snapshots\"\n    IDX_NAMES: list = [LOCATION, CARRIER, BUS_CARRIER]\n    YEAR_IDX_NAMES: list = [YEAR, LOCATION, CARRIER, BUS_CARRIER]\n</code></pre>"},{"location":"reference/evals/constants/#evals.constants.Group","title":"<code>Group</code>","text":"<p>Container to collect all carrier nice names.</p> Source code in <code>evals/constants.py</code> <pre><code>class Group:\n    \"\"\"Container to collect all carrier nice names.\"\"\"\n\n    phs_inflow: str = \"Inflow Hydro Storage\"\n    base_load: str = \"Base Load\"\n    battery_storage: str = \"Battery Storage\"\n    biomass: str = \"Biomass\"\n    ch4_bio_processing: str = \"Bio Methane Processing\"\n    chp_biomass: str = \"Biomass CHP\"\n    cng_long: str = \"CNG long\"\n    cng_short: str = \"CNG short\"\n    coal: str = \"Coal\"\n    chp_coal: str = \"Coal CHP\"\n    chp_coal_cc: str = \"Coal CHP CC\"\n    pp_coal: str = \"Coal PP\"\n    pp_coal_cc: str = \"Coal PP CC\"\n    heat_decentral: str = \"Decentral Heat\"\n    dac: str = \"Direct Air Capture\"\n    heat_district: str = \"District Heat\"\n    heat: str = \"Heat\"\n    electrictiy: str = \"Electricity\"\n    rail_elecricity: str = \"Electricity Rail\"\n    ocgt_electricity: str = \"Electricity OCGT\"\n    chp_electricity: str = \"Electricity CHP\"\n    industry_electrification: str = \"Electrif. Industry\"\n    electrolysis: str = \"Electrolysis\"\n    electrolysis_ht: str = \"Electrolysis HT\"\n    export_domestic: str = \"Export Domestic\"\n    export_foreign: str = \"Export Foreign\"\n    ft: str = \"Fischer-Tropsch\"\n    ft_1: str = \"Fischer-Tropsch 1\"\n    ft_2: str = \"Fischer-Tropsch 2\"\n    ft_domestic_aviation: str = \"Fischer-Tropsch domestic aviation\"\n    ft_domestic_navigation: str = \"Fischer-Tropsch domestic navigation\"\n    ft_industry: str = \"Fischer-Tropsch industry\"\n    ft_rail: str = \"Fischer-Tropsch rail\"\n    ft_road_freight: str = \"Fischer-Tropsch road freight\"\n    fuel_cell: str = \"Fuel Cell\"\n    fuel_cell_heat: str = \"Fuel Cell (Heat)\"\n    ch4_boiler: str = \"Gas Boiler\"\n    chp_ch4: str = \"Gas CHP\"\n    chp_ch4_cc: str = \"Gas CHP CC\"\n    global_market: str = \"Global Market*\"\n    grid_losses: str = \"Grid Losses\"\n    hev_long: str = \"HEV long\"\n    hev_short: str = \"HEV short\"\n    hh_and_services_heat: str = \"HH and Services (Heat)\"\n    # HT_Electrolysis: str = \"HT Electrolysis\"\n    heat_pump: str = \"Heat Pump\"\n    helmeth: str = \"Helmeth\"\n    hh_and_services: str = \"Households &amp; Services\"\n    h2: str = \"Hydrogen\"\n    h2_fuel_cell: str = \"Hydrogen Fuel Cell\"\n    h2_tube_storage: str = \"Hydrogen Tube Storage\"\n    h2_underground_storage: str = \"Hydrogen Underground Storage\"\n    ice_long: str = \"ICE long\"\n    ice_short: str = \"ICE short\"\n    import_biofuels: str = \"Import Biofuels\"\n    import_domestic: str = \"Import Domestic\"\n    import_foreign: str = \"Import Foreign\"\n    import_global: str = \"Import Global\"\n    industry: str = \"Industry\"\n    industry_cc: str = \"Industry CC\"\n    methanation: str = \"Methanation\"\n    ch4: str = \"Methane\"\n    ch4_store: str = \"Methane Store\"\n    misc: str = \"Miscellaneous\"\n    ch4_capacity_domestic_net: str = \"Net Capacity Gas Domestic\"\n    ch4_capacity_foreign_net: str = \"Net Capacity Gas Foreign\"\n    h2_capacity_domestic_net: str = \"Net Capacity H2 Domestic\"\n    h2_capacity_foreign_net: str = \"Net Capacity H2 Foreign\"\n    export_net: str = \"Net Export\"\n    import_net: str = \"Net Import\"\n    import_non_eu: str = \"Non-EU Import\"\n    nuclear_power: str = \"Nuclear Power\"\n    ocgt: str = \"OCGT\"\n    wind_offshore: str = \"Offshore\"\n    wind: str = \"Wind Power\"\n    oil: str = \"Oil\"\n    oil_boiler: str = \"Oil Boiler\"\n    pp_oil: str = \"Oil PP\"\n    pp_thermal: str = \"Thermal Powerplants\"\n    import_capacity_oil: str = \"Oil import capacity\"\n    wind_onshore: str = \"Onshore\"\n    p2g: str = \"P2G\"\n    phev_long: str = \"PHEV long\"\n    phev_short: str = \"PHEV short\"\n    pv: str = \"Photovoltaics\"\n    pv_rooftop: str = \"PV-Rooftop\"\n    pv_utility: str = \"PV-Utility\"\n    bev_passenger_transport: str = \"Passenger Transport BEV\"\n    phev: str = \"Passenger Transport PHEV\"\n    power_disconnect: str = \"Power Disconnect\"\n    phs: str = \"Pumped Hydro Storage\"\n    reservoir: str = \"Reservoir\"\n    resistive_heater: str = \"Resistive Heater\"\n    road_freight: str = \"Road Freight\"\n    ror: str = \"Run-of-River\"\n    smr: str = \"SMR\"\n    smr_cc: str = \"SMR CC\"\n    solar_thermal: str = \"Solar Thermal\"\n    solid_biomass_boiler: str = \"Solid Biomass Boiler\"\n    storage_in: str = \"Storage In\"\n    storage_out: str = \"Storage Out\"\n    storage_net: str = \"Storage Net\"\n    synth_fuels: str = \"Synth. Fuels\"\n    transport: str = \"Transport\"\n    co2_vent: str = \"co2 vent\"\n    ch4_domestic_navigation: str = \"gas domestic navigation\"\n    ch4_industry: str = \"gas for industry\"\n    ch4_industry_cc: str = \"gas for industry CC\"\n    ch4_road_freight: str = \"gas road freight\"\n    coal_industry: str = \"hard coal industry\"\n    process_emissions: str = \"process emissions\"\n    process_emissions_cc: str = \"process emissions CC\"\n    ch4_rural_residential_pemfc_smr: str = (\n        \"residential rural CH4-powered PEMFC with internal SMR\"\n    )\n    ch4_boiler_rural_residential: str = \"residential rural gas boiler\"\n    ch4_rural_residential_chp: str = \"residential rural micro gas CHP\"\n    ch4_rural_residential_oil_boiler: str = \"residential rural oil boiler\"\n    ch4_rural_services_pemfc_smr: str = (\n        \"services rural CH4-powered PEMFC with internal SMR\"\n    )\n    ch4_rural_services_boiler: str = \"services rural gas boiler\"\n    ch4_rural_services_chp: str = \"services rural micro gas CHP\"\n    oil_rural_services_boiler: str = \"services rural oil boiler\"\n    ch4_urban_central_chp: str = \"urban central gas CHP electric\"\n    ch4_urban_central_chp_heat: str = \"urban central gas CHP heat\"\n    ch4_urban_central_boiler: str = \"urban central gas boiler\"\n    soc: str = \"State of Charge\"\n    soc_max: str = \"Max State of Charge\"\n    turbine_cum: str = \"Accumulated Turbining\"\n    pumping_cum: str = \"Accumulated Pumping\"\n    spill_cum: str = \"Accumulated Outflow Spill\"\n    inflow_cum: str = \"Accumulated Natural Inflow\"\n</code></pre>"},{"location":"reference/evals/constants/#evals.constants.Regex","title":"<code>Regex</code>","text":"<p>A collection of regular expression patterns.</p> Source code in <code>evals/constants.py</code> <pre><code>class Regex:\n    \"\"\"A collection of regular expression patterns.\"\"\"\n\n    # ends with 4 digits\n    year: re.Pattern = re.compile(r\"\\d{4}$\")\n\n    # matches: startswith 2 capital letters, followed by up to 3 digits,\n    # 1 space, and any number of digits for optional subnets.\n    region: re.Pattern = re.compile(r\"^(?!.*CH4)[A-Z]{2}[\\d,A-G]{0,3}\\s*\\d*\")\n\n    # matches: startswith 2 capital letters, followed by up to 3 digits,\n    # groups: only the first 2 letters that are the country code\n    country: re.Pattern = re.compile(r\"^([A-Z]{2})[\\d,A-G]{0,3}\\s*\")\n\n    # match anything inside parenthesis.\n    unit: re.Pattern = re.compile(r\"\\([^()]*\\)\")\n</code></pre>"},{"location":"reference/evals/constants/#evals.constants.TradeTypes","title":"<code>TradeTypes</code>","text":"<p>Collect trade type names.</p> Source code in <code>evals/constants.py</code> <pre><code>class TradeTypes:\n    \"\"\"Collect trade type names.\"\"\"\n\n    LOCAL: str = \"local\"  # same node\n    DOMESTIC: str = \"domestic\"  # same country, but different node\n    FOREIGN: str = \"foreign\"  # different country\n</code></pre>"},{"location":"reference/evals/excel/","title":"Excel","text":""},{"location":"reference/evals/excel/#evals.excel.export_excel_countries","title":"<code>export_excel_countries(metric, writer, excel_defaults, view_config)</code>","text":"<p>Add one sheet per country to an Excel file.</p> <p>The function appends one sheet per location to the workbook of the opened writer instance.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>pandas.DataFrame</code> <p>The data frame without carrier mapping applied.</p> required <code>writer</code> <code>pandas.ExcelWriter</code> <p>The ExcelWriter instance to add the sheets to.</p> required <code>excel_defaults</code> <code>evals.configs.ExcelConfig</code> <p>The default settings for Excel file export.</p> required <code>view_config</code> <code>dict</code> <p>The view configuration items.</p> required Source code in <code>evals/excel.py</code> <pre><code>def export_excel_countries(\n    metric: pd.DataFrame,\n    writer: pd.ExcelWriter,\n    excel_defaults: ExcelConfig,\n    view_config: dict,\n) -&gt; None:\n    \"\"\"\n    Add one sheet per country to an Excel file.\n\n    The function appends one sheet per location to the workbook of the\n    opened writer instance.\n\n    Parameters\n    ----------\n    metric\n        The data frame without carrier mapping applied.\n    writer\n        The ExcelWriter instance to add the sheets to.\n    excel_defaults\n        The default settings for Excel file export.\n    view_config\n        The view configuration items.\n    \"\"\"\n    categories = view_config[\"categories\"]\n    carrier = metric.index.unique(DataModel.CARRIER)\n    df = rename_aggregate(metric, level=DataModel.CARRIER, mapper=categories)\n    df = filter_by(df, location=list(ALIAS_COUNTRY_REV))  # exclude regions\n    df = df.pivot_table(\n        index=excel_defaults.pivot_index,\n        columns=excel_defaults.pivot_columns,\n        aggfunc=\"sum\",\n    )\n\n    for country, data in df.groupby(DataModel.LOCATION):\n        data = data.droplevel(DataModel.LOCATION)\n        _write_excel_sheet(data, excel_defaults, writer, str(country))\n\n    _write_categories_sheet(categories, carrier, writer, sheet_name=\"Categories\")\n</code></pre>"},{"location":"reference/evals/excel/#evals.excel.export_excel_regions_at","title":"<code>export_excel_regions_at(metric, writer, excel_defaults, view_config)</code>","text":"<p>Write one Excel sheet for Europe, Austria, and Austrian regions.</p> <p>The function appends one sheet per location to the workbook of the opened writer instance.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>pandas.DataFrame</code> <p>The data frame without carrier mapping applied.</p> required <code>writer</code> <code>pandas.ExcelWriter</code> <p>The ExcelWriter instance to add the sheets to.</p> required <code>excel_defaults</code> <code>evals.configs.ExcelConfig</code> <p>The default settings for Excel file export.</p> required <code>view_config</code> <code>dict</code> <p>The view configuration items.</p> required Source code in <code>evals/excel.py</code> <pre><code>def export_excel_regions_at(\n    metric: pd.DataFrame,\n    writer: pd.ExcelWriter,\n    excel_defaults: ExcelConfig,\n    view_config: dict,\n) -&gt; None:\n    \"\"\"\n    Write one Excel sheet for Europe, Austria, and Austrian regions.\n\n    The function appends one sheet per location to the workbook of the\n    opened writer instance.\n\n    Parameters\n    ----------\n    metric\n        The data frame without carrier mapping applied.\n    writer\n        The ExcelWriter instance to add the sheets to.\n    excel_defaults\n        The default settings for Excel file export.\n    view_config\n        The view configuration items.\n    \"\"\"\n    categories = view_config[\"categories\"]\n    carrier = metric.index.unique(DataModel.CARRIER)\n    df = rename_aggregate(metric, level=DataModel.CARRIER, mapper=categories)\n    df = filter_by(df, location=list(ALIAS_REGION_REV))\n    df_xlsx = df.pivot_table(\n        index=excel_defaults.pivot_index,\n        columns=excel_defaults.pivot_columns,\n        aggfunc=\"sum\",\n    )\n\n    for country, data in df_xlsx.groupby(DataModel.LOCATION):\n        data = data.droplevel(DataModel.LOCATION)\n        _write_excel_sheet(data, excel_defaults, writer, str(country))\n\n    # append carrier tables to special region sheet\n    df_region = df.pivot_table(\n        index=DataModel.CARRIER,\n        columns=[DataModel.LOCATION, DataModel.YEAR],\n        aggfunc=\"sum\",\n    ).droplevel(DataModel.METRIC, axis=1)\n\n    excel_defaults.chart_title = \"Region AT\"\n    _write_excel_sheet(\n        df_region,\n        excel_defaults,\n        writer,\n        sheet_name=\"Regions AT\",\n        position=3,\n    )\n    groups = df_region.drop(\n        [\"Europe\", \"Austria\"], level=DataModel.LOCATION, axis=1, errors=\"ignore\"\n    ).groupby(DataModel.CARRIER)\n\n    # update config for pivoted carrier tables and graphs\n    excel_defaults.chart = \"clustered\"\n    excel_defaults.chart_switch_axis = True\n\n    for carrier, df_reg in groups:\n        excel_defaults.chart_title = str(carrier).title()\n        _write_excel_sheet(\n            df_reg.T.unstack(1),\n            excel_defaults,\n            writer,\n            sheet_name=\"Regions AT\",\n            position=3,\n        )\n\n    _write_categories_sheet(categories, carrier, writer, sheet_name=\"Categories\")\n</code></pre>"},{"location":"reference/evals/fileio/","title":"fileio.py","text":"<p>Input - Output related functions.</p>"},{"location":"reference/evals/fileio/#evals.fileio.Exporter","title":"<code>Exporter</code>","text":"<p>A class to export statistics.</p> <p>The exporter data frame consists of multiple joined statistics, aggregated to countries and scaled to a specified unit. The data frame format is verified and expected by export functions.</p> <p>Parameters:</p> Name Type Description Default <code>statistics</code> <code>list</code> <p>A list of Series for time aggregated statistics or list of data frames for statistics with snapshots as columns.</p> required <code>statistics_unit</code> <p>The input statistics unit.</p> required <code>keep_regions</code> <code>tuple</code> <p>A tuple of location prefixes that are used to match locations to keep during aggregation.</p> <code>('AT', 'GB', 'ES', 'FR', 'DE', 'IT')</code> <code>region_nice_names</code> <code>bool</code> <p>Whether, or not to rename country codes after aggregation to show the full country name.</p> <code>True</code> Source code in <code>evals/fileio.py</code> <pre><code>class Exporter:\n    \"\"\"\n    A class to export statistics.\n\n    The exporter data frame consists of multiple joined statistics,\n    aggregated to countries and scaled to a specified unit. The\n    data frame format is verified and expected by export functions.\n\n    Parameters\n    ----------\n    statistics\n        A list of Series for time aggregated statistics or list of\n        data frames for statistics with snapshots as columns.\n    statistics_unit\n        The input statistics unit.\n    keep_regions\n        A tuple of location prefixes that are used to match\n        locations to keep during aggregation.\n    region_nice_names\n        Whether, or not to rename country codes after aggregation\n        to show the full country name.\n    \"\"\"\n\n    def __init__(\n        self,\n        statistics: list,\n        view_config: dict,\n        keep_regions: tuple = (\n            \"AT\",\n            \"GB\",\n            \"ES\",\n            \"FR\",\n            \"DE\",\n            \"IT\",\n        ),  # todo: move to global config\n        region_nice_names: bool = True,\n    ) -&gt; None:\n        self.statistics = statistics\n        units = {stat.attrs[\"unit\"] for stat in statistics}\n        assert len(units) == 1, f\"Mixed units cannot be exported: {units}.\"\n        self.is_unit = units.pop()\n        self.metric_name = view_config[\"name\"]\n        self.to_unit = view_config[\"unit\"]\n        self.keep_regions = keep_regions\n        self.region_nice_names = region_nice_names\n        self.view_config = view_config\n        self.defaults = ViewDefaults()\n\n        # update defaults from config for this view\n        self.defaults.excel.title = view_config[\"name\"] + TITLE_SUFFIX\n        self.defaults.plotly.title = view_config[\"name\"] + TITLE_SUFFIX\n        self.defaults.plotly.file_name_template = view_config[\"file_name\"]\n        self.defaults.plotly.cutoff = view_config[\"cutoff\"]\n        self.defaults.plotly.category_orders = view_config[\"legend_order\"]\n\n    @cached_property\n    def df(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Build the metric and store it as a cached property.\n\n        (This is useful, because users do not need to remember\n        building the metric data frame. It will be built once if needed)\n\n        Returns\n        -------\n        :\n            The cached metric data frame.\n        \"\"\"\n        return combine_statistics(\n            self.statistics,\n            self.metric_name,\n            self.is_unit,\n            self.to_unit,\n            self.keep_regions,\n            self.region_nice_names,\n        )\n\n    def export_plotly(self, output_path: Path) -&gt; None:\n        \"\"\"\n        Create the plotly figure and export it as HTML and JSON.\n\n        Parameters\n        ----------\n        output_path\n            The path to the HTML folder with all the html files are\n            stored.\n        \"\"\"\n        cfg = self.defaults.plotly\n        df = rename_aggregate(\n            self.df, level=cfg.plot_category, mapper=self.view_config[\"categories\"]\n        )\n\n        df_plot = df.pivot_table(\n            index=cfg.pivot_index, columns=cfg.pivot_columns, aggfunc=\"sum\"\n        )\n\n        df_plot = _add_dummy_rows(df_plot, self.keep_regions)\n\n        for idx, data in df_plot.groupby(cfg.plotby):\n            chart = cfg.chart(data, cfg)\n            chart.plot()\n            chart.to_html(output_path, cfg.plotby, idx)\n            chart.to_json(output_path, cfg.plotby, idx)\n\n    def export_excel(self, output_path: Path) -&gt; None:\n        \"\"\"\n        Export metrics to Excel files for countries and regions.\n\n        Parameters\n        ----------\n        output_path\n            The path where the Excel files will be saved.\n        \"\"\"\n        file_name_stem = self.view_config[\"file_name\"].split(\"_{\")[0]\n        file_path = output_path / \"XLSX\" / f\"{file_name_stem}_{NOW}.xlsx\"\n        with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n            export_excel_countries(\n                self.df, writer, self.defaults.excel, self.view_config\n            )\n\n        if self.df.columns.name == DataModel.SNAPSHOTS:\n            return  # skips region sheets for time series\n\n        file_path_at = output_path / f\"{file_name_stem}_AT_{NOW}.xlsx\"\n        with pd.ExcelWriter(file_path_at, engine=\"openpyxl\") as writer:\n            export_excel_regions_at(\n                self.df, writer, self.defaults.excel, self.view_config\n            )\n\n    def export_csv(self, output_path: Path) -&gt; None:\n        \"\"\"\n        Encode the metric da frame to a CSV file.\n\n        Parameters\n        ----------\n        output_path\n            The path to the CSV folder with all the csv files are\n            stored.\n\n        Returns\n        -------\n        :\n            Writes the metric to a CSV file.\n        \"\"\"\n        file_name = self.defaults.plotly.file_name_template.split(\"_{\", maxsplit=1)[0]\n        file_path = output_path / \"CSV\" / f\"{file_name}_{NOW}.csv\"\n        self.df.to_csv(file_path, encoding=\"utf-8\")\n\n    def export(self, result_path: Path, subdir: str) -&gt; None:\n        \"\"\"\n        Export the metric to formats specified in the config.\n\n        Parameters\n        ----------\n        result_path\n            The path to the results folder.\n        subdir\n            The subdirectory inside the results folder to store evaluation results under.\n\n        Returns\n        -------\n        :\n        \"\"\"\n        output_path = self.make_evaluation_result_directories(result_path, subdir)\n\n        self.export_plotly(output_path)\n\n        if \"excel\" in self.view_config.get(\"exports\", []):\n            self.export_excel(output_path)\n        if \"csv\" in self.view_config.get(\"exports\", []):\n            self.export_csv(output_path)\n\n        # always run tests after the export\n        self.consistency_checks()\n\n    def consistency_checks(self) -&gt; None:\n        \"\"\"\n        Run plausibility and consistency checks on a metric.\n\n        The method typically is called after exporting the metric.\n        Unmapped categories do not cause evaluations to fail, but\n        the evaluation function should return in error state to obviate\n        missing entries in the mapping.\n\n        Parameter\n        ---------\n        config_checks\n            A dictionary with flags for every test to run.\n\n        Returns\n        -------\n        :\n\n        Raises\n        ------\n        AssertionError\n            In case one of the checks fails.\n        \"\"\"\n        self.default_checks()\n\n        if \"balances_almost_zero\" in self.view_config.get(\"checks\", []):\n            groups = [DataModel.YEAR, DataModel.LOCATION]\n            yearly_sum = self.df.groupby(groups).sum().abs()\n            balanced = yearly_sum &lt; self.view_config[\"cutoff\"]\n            if isinstance(balanced, pd.DataFrame):\n                assert balanced.all().all(), (\n                    f\"Imbalances detected: {yearly_sum[balanced == False].dropna(how='all').sort_values(by=balanced.columns[0], na_position='first').tail()}\"\n                )\n            else:  # Series\n                assert balanced.all().item(), (\n                    f\"Imbalances detected: {yearly_sum[balanced.squeeze() == False].squeeze().sort_values().tail()}\"\n                )\n\n    def default_checks(self) -&gt; None:\n        \"\"\"Perform integrity checks for views.\"\"\"\n        category = self.defaults.plotly.plot_category\n        categories = self.view_config[\"categories\"]\n\n        assert self.df.index.unique(category).isin(categories.keys()).all(), (\n            f\"Incomplete categories detected. There are technologies in the metric \"\n            f\"data frame, that are not assigned to a group (nice name).\"\n            f\"\\nMissing items: \"\n            f\"{self.df.index.unique(category).difference(categories.keys())}\"\n        )\n\n        superfluous_categories = self.df.index.unique(category).difference(\n            categories.keys()\n        )\n        assert len(superfluous_categories) == 0, (\n            f\"Superfluous categories found: {superfluous_categories}\"\n        )\n\n        a = set(self.view_config[\"legend_order\"])\n        b = set(categories.values())\n        additional = a.difference(b)\n        assert not additional, (\n            f\"Superfluous categories defined in legend order: {additional}\"\n        )\n        missing = b.difference(a)\n        assert not missing, (\n            f\"Some categories are not defined in legend order: {missing}\"\n        )\n\n        no_color = [c for c in categories.values() if c not in COLOUR_SCHEME_BMK]\n        assert len(no_color) == 0, (\n            f\"Some categories used in the view do not have a color assigned: {no_color}\"\n        )\n\n    def make_evaluation_result_directories(\n        self, result_path: Path, subdir: Path | str\n    ) -&gt; Path:\n        \"\"\"\n        Create all directories needed to store evaluations results.\n\n        Parameters\n        ----------\n        result_path\n            The path of the result folder.\n        subdir\n            A relative path inside the result folder.\n\n        Returns\n        -------\n        :\n            The joined path: result_dir / subdir.\n        \"\"\"\n        output_path = self.make_directory(result_path, subdir)\n        self.make_directory(output_path, \"HTML\")\n        self.make_directory(output_path, \"JSON\")\n        self.make_directory(output_path, \"CSV\")\n        self.make_directory(output_path, \"XLSX\")\n\n        return output_path\n\n    @staticmethod\n    def make_directory(base: Path, subdir: Path | str) -&gt; Path:\n        \"\"\"\n        Create a directory and return its path.\n\n        Parameters\n        ----------\n        base\n            The path to base of the new folder.\n        subdir\n            A relative path inside the base folder.\n\n        Returns\n        -------\n        :\n            The joined path: result_dir / subdir / now.\n        \"\"\"\n        base = Path(base).resolve()\n        assert base.is_dir(), f\"Base path does not exist: {base}.\"\n        directory_path = base / subdir\n        directory_path.mkdir(parents=True, exist_ok=True)\n\n        return directory_path\n</code></pre>"},{"location":"reference/evals/fileio/#evals.fileio.Exporter.df","title":"<code>df</code>  <code>cached</code> <code>property</code>","text":"<p>Build the metric and store it as a cached property.</p> <p>(This is useful, because users do not need to remember building the metric data frame. It will be built once if needed)</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The cached metric data frame.</p>"},{"location":"reference/evals/fileio/#evals.fileio.Exporter.consistency_checks","title":"<code>consistency_checks()</code>","text":"<p>Run plausibility and consistency checks on a metric.</p> <p>The method typically is called after exporting the metric. Unmapped categories do not cause evaluations to fail, but the evaluation function should return in error state to obviate missing entries in the mapping.</p> Parameter <p>config_checks     A dictionary with flags for every test to run.</p> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>In case one of the checks fails.</p> Source code in <code>evals/fileio.py</code> <pre><code>def consistency_checks(self) -&gt; None:\n    \"\"\"\n    Run plausibility and consistency checks on a metric.\n\n    The method typically is called after exporting the metric.\n    Unmapped categories do not cause evaluations to fail, but\n    the evaluation function should return in error state to obviate\n    missing entries in the mapping.\n\n    Parameter\n    ---------\n    config_checks\n        A dictionary with flags for every test to run.\n\n    Returns\n    -------\n    :\n\n    Raises\n    ------\n    AssertionError\n        In case one of the checks fails.\n    \"\"\"\n    self.default_checks()\n\n    if \"balances_almost_zero\" in self.view_config.get(\"checks\", []):\n        groups = [DataModel.YEAR, DataModel.LOCATION]\n        yearly_sum = self.df.groupby(groups).sum().abs()\n        balanced = yearly_sum &lt; self.view_config[\"cutoff\"]\n        if isinstance(balanced, pd.DataFrame):\n            assert balanced.all().all(), (\n                f\"Imbalances detected: {yearly_sum[balanced == False].dropna(how='all').sort_values(by=balanced.columns[0], na_position='first').tail()}\"\n            )\n        else:  # Series\n            assert balanced.all().item(), (\n                f\"Imbalances detected: {yearly_sum[balanced.squeeze() == False].squeeze().sort_values().tail()}\"\n            )\n</code></pre>"},{"location":"reference/evals/fileio/#evals.fileio.Exporter.default_checks","title":"<code>default_checks()</code>","text":"<p>Perform integrity checks for views.</p> Source code in <code>evals/fileio.py</code> <pre><code>def default_checks(self) -&gt; None:\n    \"\"\"Perform integrity checks for views.\"\"\"\n    category = self.defaults.plotly.plot_category\n    categories = self.view_config[\"categories\"]\n\n    assert self.df.index.unique(category).isin(categories.keys()).all(), (\n        f\"Incomplete categories detected. There are technologies in the metric \"\n        f\"data frame, that are not assigned to a group (nice name).\"\n        f\"\\nMissing items: \"\n        f\"{self.df.index.unique(category).difference(categories.keys())}\"\n    )\n\n    superfluous_categories = self.df.index.unique(category).difference(\n        categories.keys()\n    )\n    assert len(superfluous_categories) == 0, (\n        f\"Superfluous categories found: {superfluous_categories}\"\n    )\n\n    a = set(self.view_config[\"legend_order\"])\n    b = set(categories.values())\n    additional = a.difference(b)\n    assert not additional, (\n        f\"Superfluous categories defined in legend order: {additional}\"\n    )\n    missing = b.difference(a)\n    assert not missing, (\n        f\"Some categories are not defined in legend order: {missing}\"\n    )\n\n    no_color = [c for c in categories.values() if c not in COLOUR_SCHEME_BMK]\n    assert len(no_color) == 0, (\n        f\"Some categories used in the view do not have a color assigned: {no_color}\"\n    )\n</code></pre>"},{"location":"reference/evals/fileio/#evals.fileio.Exporter.export","title":"<code>export(result_path, subdir)</code>","text":"<p>Export the metric to formats specified in the config.</p> <p>Parameters:</p> Name Type Description Default <code>result_path</code> <code>pathlib.Path</code> <p>The path to the results folder.</p> required <code>subdir</code> <code>str</code> <p>The subdirectory inside the results folder to store evaluation results under.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/fileio.py</code> <pre><code>def export(self, result_path: Path, subdir: str) -&gt; None:\n    \"\"\"\n    Export the metric to formats specified in the config.\n\n    Parameters\n    ----------\n    result_path\n        The path to the results folder.\n    subdir\n        The subdirectory inside the results folder to store evaluation results under.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    output_path = self.make_evaluation_result_directories(result_path, subdir)\n\n    self.export_plotly(output_path)\n\n    if \"excel\" in self.view_config.get(\"exports\", []):\n        self.export_excel(output_path)\n    if \"csv\" in self.view_config.get(\"exports\", []):\n        self.export_csv(output_path)\n\n    # always run tests after the export\n    self.consistency_checks()\n</code></pre>"},{"location":"reference/evals/fileio/#evals.fileio.Exporter.export_csv","title":"<code>export_csv(output_path)</code>","text":"<p>Encode the metric da frame to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>pathlib.Path</code> <p>The path to the CSV folder with all the csv files are stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Writes the metric to a CSV file.</p> Source code in <code>evals/fileio.py</code> <pre><code>def export_csv(self, output_path: Path) -&gt; None:\n    \"\"\"\n    Encode the metric da frame to a CSV file.\n\n    Parameters\n    ----------\n    output_path\n        The path to the CSV folder with all the csv files are\n        stored.\n\n    Returns\n    -------\n    :\n        Writes the metric to a CSV file.\n    \"\"\"\n    file_name = self.defaults.plotly.file_name_template.split(\"_{\", maxsplit=1)[0]\n    file_path = output_path / \"CSV\" / f\"{file_name}_{NOW}.csv\"\n    self.df.to_csv(file_path, encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/evals/fileio/#evals.fileio.Exporter.export_excel","title":"<code>export_excel(output_path)</code>","text":"<p>Export metrics to Excel files for countries and regions.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>pathlib.Path</code> <p>The path where the Excel files will be saved.</p> required Source code in <code>evals/fileio.py</code> <pre><code>def export_excel(self, output_path: Path) -&gt; None:\n    \"\"\"\n    Export metrics to Excel files for countries and regions.\n\n    Parameters\n    ----------\n    output_path\n        The path where the Excel files will be saved.\n    \"\"\"\n    file_name_stem = self.view_config[\"file_name\"].split(\"_{\")[0]\n    file_path = output_path / \"XLSX\" / f\"{file_name_stem}_{NOW}.xlsx\"\n    with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n        export_excel_countries(\n            self.df, writer, self.defaults.excel, self.view_config\n        )\n\n    if self.df.columns.name == DataModel.SNAPSHOTS:\n        return  # skips region sheets for time series\n\n    file_path_at = output_path / f\"{file_name_stem}_AT_{NOW}.xlsx\"\n    with pd.ExcelWriter(file_path_at, engine=\"openpyxl\") as writer:\n        export_excel_regions_at(\n            self.df, writer, self.defaults.excel, self.view_config\n        )\n</code></pre>"},{"location":"reference/evals/fileio/#evals.fileio.Exporter.export_plotly","title":"<code>export_plotly(output_path)</code>","text":"<p>Create the plotly figure and export it as HTML and JSON.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>pathlib.Path</code> <p>The path to the HTML folder with all the html files are stored.</p> required Source code in <code>evals/fileio.py</code> <pre><code>def export_plotly(self, output_path: Path) -&gt; None:\n    \"\"\"\n    Create the plotly figure and export it as HTML and JSON.\n\n    Parameters\n    ----------\n    output_path\n        The path to the HTML folder with all the html files are\n        stored.\n    \"\"\"\n    cfg = self.defaults.plotly\n    df = rename_aggregate(\n        self.df, level=cfg.plot_category, mapper=self.view_config[\"categories\"]\n    )\n\n    df_plot = df.pivot_table(\n        index=cfg.pivot_index, columns=cfg.pivot_columns, aggfunc=\"sum\"\n    )\n\n    df_plot = _add_dummy_rows(df_plot, self.keep_regions)\n\n    for idx, data in df_plot.groupby(cfg.plotby):\n        chart = cfg.chart(data, cfg)\n        chart.plot()\n        chart.to_html(output_path, cfg.plotby, idx)\n        chart.to_json(output_path, cfg.plotby, idx)\n</code></pre>"},{"location":"reference/evals/fileio/#evals.fileio.Exporter.make_directory","title":"<code>make_directory(base, subdir)</code>  <code>staticmethod</code>","text":"<p>Create a directory and return its path.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>pathlib.Path</code> <p>The path to base of the new folder.</p> required <code>subdir</code> <code>pathlib.Path | str</code> <p>A relative path inside the base folder.</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>The joined path: result_dir / subdir / now.</p> Source code in <code>evals/fileio.py</code> <pre><code>@staticmethod\ndef make_directory(base: Path, subdir: Path | str) -&gt; Path:\n    \"\"\"\n    Create a directory and return its path.\n\n    Parameters\n    ----------\n    base\n        The path to base of the new folder.\n    subdir\n        A relative path inside the base folder.\n\n    Returns\n    -------\n    :\n        The joined path: result_dir / subdir / now.\n    \"\"\"\n    base = Path(base).resolve()\n    assert base.is_dir(), f\"Base path does not exist: {base}.\"\n    directory_path = base / subdir\n    directory_path.mkdir(parents=True, exist_ok=True)\n\n    return directory_path\n</code></pre>"},{"location":"reference/evals/fileio/#evals.fileio.Exporter.make_evaluation_result_directories","title":"<code>make_evaluation_result_directories(result_path, subdir)</code>","text":"<p>Create all directories needed to store evaluations results.</p> <p>Parameters:</p> Name Type Description Default <code>result_path</code> <code>pathlib.Path</code> <p>The path of the result folder.</p> required <code>subdir</code> <code>pathlib.Path | str</code> <p>A relative path inside the result folder.</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>The joined path: result_dir / subdir.</p> Source code in <code>evals/fileio.py</code> <pre><code>def make_evaluation_result_directories(\n    self, result_path: Path, subdir: Path | str\n) -&gt; Path:\n    \"\"\"\n    Create all directories needed to store evaluations results.\n\n    Parameters\n    ----------\n    result_path\n        The path of the result folder.\n    subdir\n        A relative path inside the result folder.\n\n    Returns\n    -------\n    :\n        The joined path: result_dir / subdir.\n    \"\"\"\n    output_path = self.make_directory(result_path, subdir)\n    self.make_directory(output_path, \"HTML\")\n    self.make_directory(output_path, \"JSON\")\n    self.make_directory(output_path, \"CSV\")\n    self.make_directory(output_path, \"XLSX\")\n\n    return output_path\n</code></pre>"},{"location":"reference/evals/fileio/#evals.fileio.get_resources_directory","title":"<code>get_resources_directory(n)</code>","text":"<p>Return a path provider to the resources directory for a network.</p> Source code in <code>evals/fileio.py</code> <pre><code>def get_resources_directory(n: pypsa.Network) -&gt; Callable:\n    \"\"\"Return a path provider to the resources directory for a network.\"\"\"\n    run = n.meta[\"run\"]\n    return path_provider(\n        \"../resources/\",  # assuming CWD is evals/cli.py\n        get_rdir(run),\n        run[\"shared_resources\"][\"policy\"],\n        run[\"shared_resources\"][\"exclude\"],\n    )\n</code></pre>"},{"location":"reference/evals/fileio/#evals.fileio.read_csv_files","title":"<code>read_csv_files(result_path, glob, sub_directory)</code>","text":"<p>Read CSV files from disk.</p> <p>Assumes, that if the file name ends with an underscore and 4 digits, the 4 digits represent the year. The year is prepended to the result dataframe index. Otherwise, the first column in the CSV file will be the index.</p> <p>The function caches result with the same input arguments.</p> <p>Parameters:</p> Name Type Description Default <code>result_path</code> <code>str | pathlib.Path</code> <p>Absolute or relative path to the run results folder that contains all model results (typically ends with \"results\", or is a time-stamp).</p> required <code>glob</code> <code>str</code> <p>The search pattern to filter file names. The asterix can be used as wildcard character that matches anything.</p> required <code>sub_directory</code> <code>str</code> <p>The subdirectory name to read files from relative to the result folder.</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>All CSV files concatenated into one DataFrame along the index axis.</p> Source code in <code>evals/fileio.py</code> <pre><code>def read_csv_files(\n    result_path: str | Path, glob: str, sub_directory: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Read CSV files from disk.\n\n    Assumes, that if the file name ends with an underscore and 4\n    digits, the 4 digits represent the year. The year is prepended\n    to the result dataframe index. Otherwise, the first column in the\n    CSV file will be the index.\n\n    The function caches result with the same input arguments.\n\n    Parameters\n    ----------\n    result_path\n        Absolute or relative path to the run results folder that\n        contains all model results (typically ends with \"results\",\n        or is a time-stamp).\n    glob\n        The search pattern to filter file names. The asterix can\n        be used as wildcard character that matches anything.\n    sub_directory\n        The subdirectory name to read files from relative to the\n        result folder.\n\n    Returns\n    -------\n    :\n        All CSV files concatenated into one DataFrame along the\n        index axis.\n    \"\"\"\n    input_path = Path(result_path) / sub_directory\n    assert input_path.is_dir(), f\"Input path does not exist: {input_path.resolve()}\"\n    file_paths = input_path.glob(glob)\n\n    df_list = []\n    for file_path in file_paths:\n        _df = pd.read_csv(file_path, index_col=0)\n        if year := re.search(Regex.year, file_path.stem):\n            _df = insert_index_level(_df, year.group(), DataModel.YEAR)\n        df_list.append(_df)\n\n    # must assert after the loop, because file_paths is a generator\n    assert df_list, f\"No files named like '{glob}' in {input_path.resolve()}.\"\n\n    return pd.concat(df_list, sort=True)\n</code></pre>"},{"location":"reference/evals/fileio/#evals.fileio.read_networks","title":"<code>read_networks(result_path, sub_directory='networks')</code>","text":"<p>Read network results from NetCDF (.nc) files.</p> <p>The function returns a dictionary of data frames. The planning horizon (year) is used as dictionary key and added to the network as an attribute to associate the year with it. Network snapshots are equal for all networks, although the year changes. This is required to align timestamp columns in a data frame. Snapshots will become fixed late in the evaluation process (just before export to file).</p> <p>In addition, the function patches the statistics accessor attached to loaded networks and adds the configuration under n.meta if it is missing.</p> <p>Parameters:</p> Name Type Description Default <code>result_path</code> <code>str | pathlib.Path | list</code> <p>Absolute or relative path to the run results folder that contains all model results (typically ends with \"results\", or is a time-stamp).</p> required <code>sub_directory</code> <code>str</code> <p>The subdirectory name to read files from relative to the result folder.</p> <code>'networks'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A Dictionary that contains pypsa.Network objects as values the year from the end of the file name as keys.</p> Source code in <code>evals/fileio.py</code> <pre><code>def read_networks(\n    result_path: str | Path | list, sub_directory: str = \"networks\"\n) -&gt; dict:\n    \"\"\"\n    Read network results from NetCDF (.nc) files.\n\n    The function returns a dictionary of data frames. The planning\n    horizon (year) is used as dictionary key and added to the network\n    as an attribute to associate the year with it. Network snapshots\n    are equal for all networks, although the year changes. This is\n    required to align timestamp columns in a data frame. Snapshots\n    will become fixed late in the evaluation process (just before\n    export to file).\n\n    In addition, the function patches the statistics accessor attached\n    to loaded networks and adds the configuration under n.meta if it is\n    missing.\n\n    Parameters\n    ----------\n    result_path\n        Absolute or relative path to the run results folder that\n        contains all model results (typically ends with \"results\",\n        or is a time-stamp).\n    sub_directory\n        The subdirectory name to read files from relative to the\n        result folder.\n\n    Returns\n    -------\n    :\n        A Dictionary that contains pypsa.Network objects as values the\n        year from the end of the file name as keys.\n    \"\"\"\n    # delayed import to prevent circular dependency error\n    from evals.statistic import ESMStatistics\n\n    if isinstance(result_path, list):\n        file_paths = [Path(p) for p in result_path]  # assuming snakemake.input.networks\n    else:\n        input_path = Path(result_path) / sub_directory\n        file_paths = input_path.glob(r\"*[0-9].nc\")\n\n    networks = {}\n    for file_path in file_paths:\n        year = re.search(Regex.year, file_path.stem).group()\n        n = pypsa.Network(file_path)\n        n.statistics = ESMStatistics(n, result_path)\n        n.name = f\"PyPSA-AT Network {year}\"\n        n.year = year\n        networks[year] = n\n\n    assert networks, f\"No networks found in {file_paths}.\"\n\n    return networks\n</code></pre>"},{"location":"reference/evals/fileio/#evals.fileio.read_views_config","title":"<code>read_views_config(func, config_override='config.override.toml')</code>","text":"<p>Return the configuration for a view function.</p> <p>The function reads the default configuration from the TOML file and optionally updates it using the config file from the override file. The configuration returned is stripped down to the relevant parts that matter for the called view function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>typing.Callable</code> <p>The view function to be called by the CLI module.</p> required <code>config_override</code> <code>str</code> <p>A file name as a string as passed to the CLI module.</p> <code>'config.override.toml'</code> <p>Returns:</p> Type Description <code>dict</code> <p>The default configuration with optional overrides from a second configuration file.</p> Source code in <code>evals/fileio.py</code> <pre><code>def read_views_config(\n    func: Callable, config_override: str = \"config.override.toml\"\n) -&gt; dict:\n    \"\"\"\n    Return the configuration for a view function.\n\n    The function reads the default configuration from the\n    TOML file and optionally updates it using the config\n    file from the override file. The configuration returned\n    is stripped down to the relevant parts that matter for the\n    called view function.\n\n    Parameters\n    ----------\n    func\n        The view function to be called by the CLI module.\n    config_override\n        A file name as a string as passed to the CLI module.\n\n    Returns\n    -------\n    :\n        The default configuration with optional overrides from\n        a second configuration file.\n    \"\"\"\n    default_fp = resources.files(\"evals\") / \"config.default.toml\"\n    default = tomllib.load(default_fp.open(\"rb\"))\n    default_global = default[\"global\"]\n    default_view = default[func.__name__]\n\n    if config_override:\n        override_fp = Path(resources.files(\"evals\")) / config_override\n        override = tomllib.load(override_fp.open(\"rb\"))\n        default_global = deep_update(default_global, override[\"global\"])\n\n        if override_view := override.get(func.__name__, {}):\n            default_view = deep_update(default_view, override_view)\n\n    config = {\"global\": default_global, \"view\": default_view}\n\n    logger = logging.getLogger()\n    logger.debug(f\"Configuration items: {config}\")\n\n    return config\n</code></pre>"},{"location":"reference/evals/statistic/","title":"statistic.py","text":"<p>Collect statistics for evaluations.</p>"},{"location":"reference/evals/statistic/#evals.statistic.ESMStatistics","title":"<code>ESMStatistics</code>","text":"<p>               Bases: <code>pypsa.statistics.StatisticsAccessor</code></p> <p>Provides additional statistics for ESM evaluations.</p> <p>Extends the StatisticsAccessor with additional metrics.</p> <p>Note, that the call method of the base class is not updated. Metrics registered with this class need to be called explicitly and are not included in the output of n.statistics().</p> <p>The actual patching is done directly after reading in the network files in read_networks(). This means, that io.read_networks() must be used to load networks, or the statistics will not be available under n.statistics().</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>pypsa.Network</code> <p>The loaded postnetwork.</p> required <code>result_path</code> <code>pathlib.Path</code> <p>The output path including the subdirectory, i.e. the path where the evaluation results are stored.</p> required Source code in <code>evals/statistic.py</code> <pre><code>class ESMStatistics(StatisticsAccessor):\n    \"\"\"\n    Provides additional statistics for ESM evaluations.\n\n    Extends the StatisticsAccessor with additional metrics.\n\n    Note, that the __call__ method of the base class is not\n    updated. Metrics registered with this class need to\n    be called explicitly and are not included in the output\n    of n.statistics().\n\n    The actual patching is done directly after reading in the\n    network files in read_networks(). This means, that\n    io.read_networks() must be used to load networks, or the\n    statistics will not be available under n.statistics().\n\n    Parameters\n    ----------\n    n\n        The loaded postnetwork.\n\n    result_path\n        The output path including the subdirectory, i.e. the path\n        where the evaluation results are stored.\n    \"\"\"\n\n    def __init__(self, n: pypsa.Network, result_path: Path) -&gt; None:\n        super().__init__(n)\n        self.result_path = result_path\n        pypsa.options.params.statistics.nice_names = False\n        pypsa.options.params.statistics.drop_zero = True\n        groupers.add_grouper(\"location\", get_location)\n        groupers.add_grouper(\n            \"bus0\", partial(get_location_from_name_at_port, location_port=\"0\")\n        )\n        groupers.add_grouper(\n            \"bus1\", partial(get_location_from_name_at_port, location_port=\"1\")\n        )\n\n    def ac_load_split(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Split energy amounts for electricity Loads.\n\n        The following AC loads can be distinguished:\n          - industry,\n          - rail, and\n          - households and services.\n\n        Industry and rail data are read from CSV files.\n        HH &amp; services data is the remainder of total\n        electricity minus rail and industry parts.\n\n        Returns\n        -------\n        :\n            The data series with split AC loads.\n\n        Notes\n        -----\n        Currently broken: Energy demands only exist for historical years and\n        industry and rail demands probably are not substracted from the correct\n        series.\n        \"\"\"\n        year = self._n.meta[\"wildcards\"][\"planning_horizons\"]\n        clusters = self._n.meta[\"wildcards\"][\"clusters\"]\n        run = self._n.meta[\"run\"]\n        res = get_resources_directory(self._n)(run[\"prefix\"])\n\n        indu = read_csv_files(\n            res,\n            glob=f\"industrial_energy_demand_base_s_{clusters}_*.csv\",\n            sub_directory=run[\"name\"][0],\n        )\n        indu = indu.loc[year, \"current electricity\"] * UNITS[\"TW\"]  # to MWH\n\n        rail = (\n            read_csv_files(\n                res,\n                glob=\"pop_weighted_energy_totals_s_adm.csv\",\n                sub_directory=run[\"name\"][0],\n            )[\"electricity rail\"]\n            * UNITS[\"TW\"]\n        )  # fixme: data for base year only\n\n        p = (\n            self.energy_balance(\n                comps=\"Load\",\n                groupby=[\"location\", \"carrier\", \"bus_carrier\"],\n                bus_carrier=\"low voltage\",\n            )\n            .droplevel(DataModel.BUS_CARRIER)\n            .unstack()\n        )\n\n        # load p is negative, because it is demand (withdrawal), but csv\n        # data (industry, transport) has positive values only. Must\n        # reverse the sign for industry and rail demands.\n        homes_and_trade = Carrier.domestic_homes_and_trade\n        p[Carrier.industry] = indu.mul(-1)\n        p[Carrier.electricity_rail] = rail.mul(-1)\n        p[homes_and_trade] = p[\"electricity\"] + indu + rail\n\n        if any(p[homes_and_trade] &gt; 0):\n            logger.warning(\n                msg=f\"Positive values found for {homes_and_trade} \"\n                f\"demand. This happens if the combined electricity demand \"\n                f\"from Industry and Rail nodal energy files is larger than \"\n                f\"the electricity Loads in the network.\\n\"\n                f\"{p[p[homes_and_trade] &gt; 0][homes_and_trade]}\\n\\n\"\n                f\"All values larger than zero will be set to zero. \"\n                f\"(Note that this is different to the Toolbox implementation \"\n                f\"where signs are flipped).\\n\"\n            )\n            # fixme: just a note. There is a bug in the old Toolbox that\n            #  counts the aforementioned amounts as demand (although\n            #  the negative values should probably be clipped.)\n            p[homes_and_trade] = p[homes_and_trade].clip(upper=0)\n\n        # rename electricity base load to avoid mixing it up\n        p = p.rename({\"electricity\": \"industry + hh &amp; services load\"}, axis=1)\n\n        df = insert_index_level(p.stack(), \"low voltage\", DataModel.BUS_CARRIER)\n        df = df.reorder_levels(DataModel.IDX_NAMES)\n\n        df.attrs[\"name\"] = \"Electricity split \"\n        df.attrs[\"unit\"] = \"MWh\"\n\n        return df\n\n    def bev_v2g(self, drop_v2g_withdrawal: bool = True) -&gt; DataFrame:\n        \"\"\"\n        Calculate BEV and V2G energy amounts.\n\n        Parameters\n        ----------\n        drop_v2g_withdrawal\n            Whether to exclude vehicle to grid technologies from the\n            results. This option is included since the Toolbox\n            implementation drops them too.\n\n        Returns\n        -------\n        :\n            A DataFrame containing the calculated BEV and V2G energy\n            amounts.\n        \"\"\"\n        c = Carrier\n        names_supply = {\n            c.bev_charger: c.bev_charger_supply,\n            c.v2g: c.v2g_supply,\n        }\n        names_withdrawal = {\n            c.bev: c.bev_passenger_withdrawal,\n            c.bev_charger: c.bev_charger_draw,\n            c.v2g: c.v2g_withdrawal,\n        }\n        carrier = [Carrier.bev, Carrier.bev_charger, Carrier.v2g]\n        supply = self.supply(\n            comps=\"Link\",\n            groupby=[\"location\", \"carrier\", \"bus_carrier\"],\n            bus_carrier=[BusCarrier.AC, BusCarrier.LI_ION],\n        )\n        supply = filter_by(supply, carrier=carrier)\n\n        withdrawal = self.withdrawal(\n            comps=\"Link\",\n            groupby=[\"location\", \"carrier\", \"bus_carrier\"],\n            bus_carrier=[BusCarrier.AC, BusCarrier.LI_ION],\n        )\n        withdrawal = filter_by(withdrawal, carrier=carrier)\n        withdrawal = withdrawal.mul(-1)  # to keep withdrawal negative\n\n        # rename carrier to avoid name clashes for supply/withdrawal\n        supply = supply.rename(names_supply, level=DataModel.CARRIER)\n        withdrawal = withdrawal.rename(names_withdrawal, level=DataModel.CARRIER)\n\n        # join along index, sum duplicates and pivot carriers to columns\n        p = (\n            pd.concat([withdrawal, supply])\n            .groupby([DataModel.LOCATION, DataModel.CARRIER])\n            .sum()\n            .unstack()\n        )\n\n        ratio = (p[c.bev_charger_draw] / p[c.bev_charger_supply]).abs()\n\n        p[c.bev_charger_losses] = p[c.bev_charger_draw] + p[c.bev_charger_supply]\n        p[c.bev_demand] = ratio * p[c.bev_passenger_withdrawal]\n        p[c.bev_losses] = p[c.bev_demand] - p[c.bev_passenger_withdrawal]\n        p[c.v2g_demand] = ratio * p[c.v2g_withdrawal] if c.v2g_withdrawal in p else 0\n        p[c.v2g_losses] = p[c.v2g_demand] + p[c.v2g_supply] if c.v2g_supply in p else 0\n\n        ser = insert_index_level(p.stack(), BusCarrier.AC, DataModel.BUS_CARRIER, pos=2)\n        ser.attrs[\"name\"] = \"BEV&amp;V2G\"\n        ser.attrs[\"unit\"] = \"MWh\"\n\n        if drop_v2g_withdrawal:\n            ser = ser.drop(c.v2g_withdrawal, level=DataModel.CARRIER, errors=\"ignore\")\n\n        return ser\n\n    def phs_split(\n        self, aggregate_time: str = \"sum\", drop_hydro_cols: bool = True\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Split energy amounts for StorageUnits.\n\n        This is done to properly separate primary energy and energy\n        storage, i.e. to separate the natural inflow (primary energy)\n        from storage dispatch (secondary energy).\n\n        Parameters\n        ----------\n        aggregate_time\n            The aggregation function used to aggregate time steps.\n\n        drop_hydro_cols\n            Whether, or not to drop 'hydro' carriers from the result.\n            This is required to stay consistent with the old Toolbox\n            implementation.\n\n        Returns\n        -------\n        :\n            A DataFrame containing the split energy amounts for\n            PHS and hydro.\n\n        Notes\n        -----\n        Not needed if all PHS are implemeted as closed loops. The method is kept\n        if open loop PHS is available.\n        \"\"\"\n        n = self._n\n\n        idx = n.static(\"StorageUnit\").index\n        phs = pd.DataFrame(index=idx)\n        for time_series in (\"p_dispatch\", \"p_store\", \"spill\", \"inflow\"):\n            p = n.pnl(\"StorageUnit\")[time_series].reindex(columns=idx, fill_value=0)\n            weights = get_weightings(n, \"StorageUnit\")\n            phs[time_series] = n.statistics._aggregate_timeseries(\n                p, weights, agg=aggregate_time\n            )\n\n        # calculate the potential dispatch energy for storages\n        stored_energy = phs[\"p_store\"] * n.static(\"StorageUnit\")[\"efficiency_dispatch\"]\n        share_inflow = phs[\"inflow\"] / (phs[\"inflow\"] + stored_energy)\n\n        phs[\"Dispatched Power from Inflow\"] = phs[\"p_dispatch\"] * share_inflow\n        phs[\"Dispatched Power from Stored\"] = phs[\"p_dispatch\"] * (1 - share_inflow)\n        phs[\"Spill from Inflow\"] = phs[\"spill\"] * share_inflow\n        phs[\"Spill from Stored\"] = phs[\"spill\"] * (1 - share_inflow)\n\n        mapper = {\n            \"p_dispatch\": \"Dispatched Power\",\n            \"p_store\": \"Stored Power\",\n            \"inflow\": \"Inflow\",\n            \"spill\": \"Spill\",\n        }\n        phs = phs.rename(mapper, axis=1)\n\n        ser = phs.stack()\n        ser.index = ser.index.swaplevel(0, 1)\n        ser.index = split_location_carrier(ser.index, names=DataModel.IDX_NAMES)\n\n        # merge 'carrier' with 'bus_carrier' level and keep original\n        # bus_carrier. Needed to stay consistent with the old Toolbox\n        # naming conventions.\n        ser.index = pd.MultiIndex.from_tuples(\n            [(r[1], f\"{r[2]} {r[0]}\", r[2]) for r in ser.index],\n            names=DataModel.IDX_NAMES,\n        )\n        ser = ser.rename(\n            index={\"PHS\": BusCarrier.AC, \"hydro\": BusCarrier.AC},\n            level=DataModel.BUS_CARRIER,\n        )\n\n        ser.attrs[\"name\"] = \"PHS&amp;Hydro\"\n        ser.attrs[\"unit\"] = \"MWh\"\n\n        if drop_hydro_cols:\n            cols = [\n                \"hydro Dispatched Power from Inflow\",\n                \"hydro Dispatched Power from Stored\",\n                \"hydro Spill from Inflow\",\n                \"hydro Spill from Stored\",\n            ]\n            ser = ser.drop(cols, level=DataModel.CARRIER)\n\n        return ser.sort_index()\n\n    def phs_hydro_operation(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate Hydro- and Pumped Hydro Storage unit statistics.\n\n        Returns\n        -------\n        :\n            Cumulated or constant time series for storage units.\n        \"\"\"\n        n = self._n\n        ts_efficiency_name_agg = [\n            (\"p_dispatch\", \"efficiency_dispatch\", Group.turbine_cum, \"cumsum\"),\n            (\"p_store\", \"efficiency_store\", Group.pumping_cum, \"cumsum\"),\n            (\"spill\", None, Group.spill_cum, \"cumsum\"),\n            (\"inflow\", None, Group.inflow_cum, \"cumsum\"),\n            (\"state_of_charge\", None, Group.soc, None),\n        ]\n\n        weights = get_weightings(n, \"StorageUnit\")\n\n        su = n.static(\"StorageUnit\").query(\"carrier in ['PHS', 'hydro']\")\n\n        results = []\n        for time_series, efficiency, index_name, agg in ts_efficiency_name_agg:\n            df = n.pnl(\"StorageUnit\")[time_series].filter(su.index, axis=1)\n            if agg:\n                df = df.mul(weights, axis=0).agg(agg)\n            if efficiency == \"efficiency_dispatch\":\n                df = df / su[efficiency]\n            elif efficiency == \"efficiency_store\":\n                df = df * su[efficiency]\n            # The actual bus carrier is \"AC\" for both, PHS and hydro.\n            # Since only PHS and hydro are considered, we can use the\n            # bus_carrier level to track groups.\n            result = insert_index_level(df, index_name, DataModel.BUS_CARRIER, axis=1)\n            results.append(result.T)\n\n        # broadcast storage volume to time series (not quite the\n        # same as utils.scalar_to_time_series, because it's a series)\n        volume = su[\"p_nom_opt\"] * su[\"max_hours\"]\n        volume_ts = pd.concat([volume] * len(n.snapshots), axis=1)\n        volume_ts.columns = n.snapshots\n        volume_ts = insert_index_level(volume_ts, Group.soc_max, DataModel.BUS_CARRIER)\n        results.append(volume_ts)\n\n        statistic = pd.concat(results)\n        statistic.index = split_location_carrier(\n            statistic.index,\n            names=[DataModel.BUS_CARRIER, DataModel.LOCATION, DataModel.CARRIER],\n        )\n        statistic = statistic.reorder_levels(DataModel.IDX_NAMES)\n\n        statistic.columns.names = [DataModel.SNAPSHOTS]\n        statistic.attrs[\"name\"] = \"StorageUnit Operation\"\n        statistic.attrs[\"unit\"] = \"MWh\"\n\n        return statistic\n\n    def trade_energy(\n        self,\n        scope: str | tuple,\n        direction: str = \"saldo\",\n        bus_carrier: str = None,\n        aggregate_time: str = \"sum\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate energy amounts exchanged between locations.\n\n        Returns positive values for 'import' (supply) and negative\n        values for 'export' (withdrawal).\n\n        Parameters\n        ----------\n        scope\n            The scope of energy exchange. Must be one of \"foreign\",\n            \"domestic\", or \"local\".\n\n        direction\n            The direction of the trade. Can be one of \"saldo\", \"export\",\n            or \"import\".\n\n        bus_carrier\n            The bus carrier for which to calculate the energy exchange.\n            Defaults to using all bus carrier.\n\n        aggregate_time\n            The method of aggregating the energy exchange over time.\n            Can be one of \"sum\", \"mean\", \"max\", \"min\".\n\n        Returns\n        -------\n        :\n            A DataFrame containing the calculated energy exchange\n            between locations.\n        \"\"\"\n        n = self._n\n        results_comp = []\n\n        buses = n.static(\"Bus\").reset_index()\n        if bus_carrier:\n            _bc = [bus_carrier] if isinstance(bus_carrier, str) else bus_carrier\n            buses = buses.query(\"carrier in @_bc\")\n\n        carrier = get_transmission_carriers(n, bus_carrier).unique(\"carrier\")  # Noqa: F841\n        comps = get_transmission_carriers(n, bus_carrier).unique(\"component\")\n\n        for port, c in product((0, 1), comps):\n            mask = trade_mask(n.static(c), scope).to_numpy()\n            comp = n.static(c)[mask].reset_index()\n\n            p = buses.merge(\n                comp.query(\"carrier.isin(@carrier)\"),\n                left_on=\"Bus\",\n                right_on=f\"bus{port}\",\n                suffixes=(\"_bus\", \"\"),\n            ).merge(n.pnl(c).get(f\"p{port}\").T, on=c)\n\n            _location = (\n                DataModel.LOCATION + \"_bus\"\n                if \"location\" in comp\n                else DataModel.LOCATION\n            )\n            p = p.set_index([_location, DataModel.CARRIER, \"carrier_bus\", \"unit\"])\n            p.index.names = DataModel.IDX_NAMES + [\"unit\"]\n            # branch components have reversed sign\n            p = p.filter(n.snapshots, axis=1).mul(-1)\n            if direction == \"export\":\n                p = p.clip(upper=0)  # keep negative values (withdrawal)\n            elif direction == \"import\":\n                p = p.clip(lower=0)  # keep positive values (supply)\n            elif direction != \"saldo\":\n                raise ValueError(f\"Direction '{direction}' not supported.\")\n\n            results_comp.append(insert_index_level(p, c, \"component\"))\n\n        result = pd.concat(results_comp)\n\n        if aggregate_time:\n            # assuming Link and Line have the same weights\n            weights = get_weightings(n, \"Link\")\n            result = result.multiply(weights, axis=1)\n            result = result.agg(aggregate_time, axis=1)\n\n        name = \" &amp; \".join(scope) if isinstance(scope, tuple) else scope\n        result.attrs[\"name\"] = f\"{name} {direction}\"\n        result.attrs[\"unit\"] = \"MWh\"\n\n        return result.sort_index()\n\n    def trade_capacity(\n        self,\n        scope: str,\n        bus_carrier: str = \"\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate exchange capacity between locations.\n\n        Parameters\n        ----------\n        scope\n            The scope of energy exchange. Must be one of\n            constants.TRADE_TYPES.\n        bus_carrier\n            The bus carrier for which to calculate the energy exchange.\n            Defaults to using all bus carrier.\n\n        Returns\n        -------\n        :\n            Energy exchange capacity between locations.\n        \"\"\"\n        n = self._n\n\n        capacity = self.optimal_capacity(\n            comps=n.branch_components,\n            bus_carrier=bus_carrier,\n            groupby=[\"bus0\", \"bus1\", \"carrier\", \"bus_carrier\"],\n            nice_names=False,\n        ).to_frame()\n        trade_type = capacity.apply(\n            lambda row: get_trade_type(row.name[1], row.name[2]), axis=1\n        )\n\n        trade_capacity = capacity[trade_type == scope]\n\n        # duplicate capacities to list them for source and destination\n        # locations. For example, the trade capacity for AT -&gt; DE gas\n        # pipeline will be shown in location AT and in location DE.\n        df_list = []\n        for bus in (\"bus0\", \"bus1\"):\n            df = trade_capacity.droplevel(bus)\n            df.index.names = [DataModel.COMPONENT] + DataModel.IDX_NAMES\n            df_list.append(df)\n\n        trade_capacity = pd.concat(df_list).drop_duplicates()\n\n        return trade_capacity.squeeze()\n\n    def grid_capacity(\n        self,\n        comps: list = None,\n        bus_carrier: list = None,\n        carrier: list = None,\n        append_grid: bool = True,\n        align_edges: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Return transmission grid capacities.\n\n        Parameters\n        ----------\n        comps\n            The network components to consider, defaults to all\n            pypsa.Networks.branch_components.\n        bus_carrier\n            The bus carrier to consider.\n        carrier\n            The carrier to consider, defaults to all\n            transmission carriers in the network.\n        append_grid\n            Whether to add the grid lines to the result.\n        align_edges\n            Whether to adjust edges between the same nodes but in\n            reversed direction. For example, AC and DC grids have\n            edges between IT0 0 and FR0 0 as IT-&gt;FR and FR-&gt;IT,\n            respectively. If enabled, both will have the same bus0 and\n            bus1.\n\n        Returns\n        -------\n        :\n            The optimal capacity for transmission technologies between\n            nodes.\n\n        Notes\n        -----\n        The \"pypsa.statistics.transmission\" statistic does not work here\n        because it returns energy amounts whereas this statistic returns\n        the optimal capacity.\n        \"\"\"\n        n = self._n\n        carrier = carrier or get_transmission_carriers(n, bus_carrier).unique(\"carrier\")\n        capacities = n.statistics.optimal_capacity(\n            comps=comps or n.branch_components,\n            bus_carrier=bus_carrier,\n            groupby=[\"bus0\", \"bus1\", \"carrier\", \"bus_carrier\"],\n        )\n        result = filter_by(capacities, carrier=carrier)\n\n        result.attrs[\"name\"] = \"Capacity\"\n        result.attrs[\"unit\"] = \"MW\"\n        result.name = f\"{result.attrs['name']} ({result.attrs['unit']})\"\n\n        if align_edges:\n            result = align_edge_directions(result)\n\n        if append_grid:\n            result = add_grid_lines(n.static(\"Bus\"), result)\n\n        return result.sort_index()\n\n    def grid_flow(\n        self,\n        comps: list = None,\n        bus_carrier: list = None,\n        carrier: list = None,\n        aggregate_time: str = \"sum\",\n        append_grid: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Return the transmission grid energy flow.\n\n        Parameters\n        ----------\n        comps\n            The network components to consider, defaults to all\n            pypsa.Networks.branch_components.\n        bus_carrier\n            The bus carrier to consider.\n        carrier\n            The carrier to consider, defaults to all\n            transmission carrier in the network.\n        aggregate_time\n            The aggregation function aggregate by.\n        append_grid\n            Whether to add the grid lines to the result.\n\n        Returns\n        -------\n        :\n            The amount of energy transfer for transmission technologies\n            between nodes.\n        \"\"\"\n        n = self._n\n        carrier = carrier or get_transmission_carriers(n, bus_carrier).unique(\"carrier\")\n        comps = comps or n.branch_components\n\n        energy_transmission = n.statistics.transmission(\n            comps=comps,\n            groupby=[\"bus0\", \"bus1\", \"carrier\", \"bus_carrier\"],\n            bus_carrier=bus_carrier,\n            aggregate_time=False,\n        )\n        energy_transmission = filter_by(energy_transmission, carrier=carrier)\n\n        # split directions:\n        # positive values are from bus0 to bus1, i.e. bus1 supply\n        bus0_to_bus1 = energy_transmission.clip(lower=0)\n\n        # negative values are from bus1 to bus0, i.e. bus0 supply\n        idx_names = list(energy_transmission.index.names)\n        bus1_to_bus0 = energy_transmission.clip(upper=0).mul(-1)\n        # reverse the node index levels to show positive values and\n        # have a consistent way of interpreting the energy flow\n        bus1_to_bus0 = bus1_to_bus0.swaplevel(\"bus0\", \"bus1\")\n        pos0, pos_1 = idx_names.index(\"bus0\"), idx_names.index(\"bus1\")\n        idx_names[pos_1], idx_names[pos0] = idx_names[pos0], idx_names[pos_1]\n        bus1_to_bus0.index.names = idx_names\n\n        result = pd.concat([bus0_to_bus1, bus1_to_bus0])\n        result = result.groupby(idx_names).sum()\n\n        assert aggregate_time, \"Time Series is not supported.\"\n        unit = \"MW\"\n        if aggregate_time in (\"max\", \"min\"):\n            result = result.agg(aggregate_time, axis=1)\n        elif aggregate_time:  # mean, median, etc.\n            weights = get_weightings(n, comps)\n            result = result.mul(weights, axis=1).agg(aggregate_time, axis=1)\n            unit = \"MWh\"\n\n        result.attrs[\"name\"] = \"Energy\"\n        result.attrs[\"unit\"] = unit\n        result.name = f\"{result.attrs['name']} ({result.attrs['unit']})\"\n\n        if append_grid:\n            result = add_grid_lines(n.static(\"Bus\"), result)\n\n        return result.sort_index()\n</code></pre>"},{"location":"reference/evals/statistic/#evals.statistic.ESMStatistics.ac_load_split","title":"<code>ac_load_split()</code>","text":"<p>Split energy amounts for electricity Loads.</p> <p>The following AC loads can be distinguished:   - industry,   - rail, and   - households and services.</p> <p>Industry and rail data are read from CSV files. HH &amp; services data is the remainder of total electricity minus rail and industry parts.</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The data series with split AC loads.</p> Notes <p>Currently broken: Energy demands only exist for historical years and industry and rail demands probably are not substracted from the correct series.</p> Source code in <code>evals/statistic.py</code> <pre><code>def ac_load_split(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Split energy amounts for electricity Loads.\n\n    The following AC loads can be distinguished:\n      - industry,\n      - rail, and\n      - households and services.\n\n    Industry and rail data are read from CSV files.\n    HH &amp; services data is the remainder of total\n    electricity minus rail and industry parts.\n\n    Returns\n    -------\n    :\n        The data series with split AC loads.\n\n    Notes\n    -----\n    Currently broken: Energy demands only exist for historical years and\n    industry and rail demands probably are not substracted from the correct\n    series.\n    \"\"\"\n    year = self._n.meta[\"wildcards\"][\"planning_horizons\"]\n    clusters = self._n.meta[\"wildcards\"][\"clusters\"]\n    run = self._n.meta[\"run\"]\n    res = get_resources_directory(self._n)(run[\"prefix\"])\n\n    indu = read_csv_files(\n        res,\n        glob=f\"industrial_energy_demand_base_s_{clusters}_*.csv\",\n        sub_directory=run[\"name\"][0],\n    )\n    indu = indu.loc[year, \"current electricity\"] * UNITS[\"TW\"]  # to MWH\n\n    rail = (\n        read_csv_files(\n            res,\n            glob=\"pop_weighted_energy_totals_s_adm.csv\",\n            sub_directory=run[\"name\"][0],\n        )[\"electricity rail\"]\n        * UNITS[\"TW\"]\n    )  # fixme: data for base year only\n\n    p = (\n        self.energy_balance(\n            comps=\"Load\",\n            groupby=[\"location\", \"carrier\", \"bus_carrier\"],\n            bus_carrier=\"low voltage\",\n        )\n        .droplevel(DataModel.BUS_CARRIER)\n        .unstack()\n    )\n\n    # load p is negative, because it is demand (withdrawal), but csv\n    # data (industry, transport) has positive values only. Must\n    # reverse the sign for industry and rail demands.\n    homes_and_trade = Carrier.domestic_homes_and_trade\n    p[Carrier.industry] = indu.mul(-1)\n    p[Carrier.electricity_rail] = rail.mul(-1)\n    p[homes_and_trade] = p[\"electricity\"] + indu + rail\n\n    if any(p[homes_and_trade] &gt; 0):\n        logger.warning(\n            msg=f\"Positive values found for {homes_and_trade} \"\n            f\"demand. This happens if the combined electricity demand \"\n            f\"from Industry and Rail nodal energy files is larger than \"\n            f\"the electricity Loads in the network.\\n\"\n            f\"{p[p[homes_and_trade] &gt; 0][homes_and_trade]}\\n\\n\"\n            f\"All values larger than zero will be set to zero. \"\n            f\"(Note that this is different to the Toolbox implementation \"\n            f\"where signs are flipped).\\n\"\n        )\n        # fixme: just a note. There is a bug in the old Toolbox that\n        #  counts the aforementioned amounts as demand (although\n        #  the negative values should probably be clipped.)\n        p[homes_and_trade] = p[homes_and_trade].clip(upper=0)\n\n    # rename electricity base load to avoid mixing it up\n    p = p.rename({\"electricity\": \"industry + hh &amp; services load\"}, axis=1)\n\n    df = insert_index_level(p.stack(), \"low voltage\", DataModel.BUS_CARRIER)\n    df = df.reorder_levels(DataModel.IDX_NAMES)\n\n    df.attrs[\"name\"] = \"Electricity split \"\n    df.attrs[\"unit\"] = \"MWh\"\n\n    return df\n</code></pre>"},{"location":"reference/evals/statistic/#evals.statistic.ESMStatistics.bev_v2g","title":"<code>bev_v2g(drop_v2g_withdrawal=True)</code>","text":"<p>Calculate BEV and V2G energy amounts.</p> <p>Parameters:</p> Name Type Description Default <code>drop_v2g_withdrawal</code> <code>bool</code> <p>Whether to exclude vehicle to grid technologies from the results. This option is included since the Toolbox implementation drops them too.</p> <code>True</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>A DataFrame containing the calculated BEV and V2G energy amounts.</p> Source code in <code>evals/statistic.py</code> <pre><code>def bev_v2g(self, drop_v2g_withdrawal: bool = True) -&gt; DataFrame:\n    \"\"\"\n    Calculate BEV and V2G energy amounts.\n\n    Parameters\n    ----------\n    drop_v2g_withdrawal\n        Whether to exclude vehicle to grid technologies from the\n        results. This option is included since the Toolbox\n        implementation drops them too.\n\n    Returns\n    -------\n    :\n        A DataFrame containing the calculated BEV and V2G energy\n        amounts.\n    \"\"\"\n    c = Carrier\n    names_supply = {\n        c.bev_charger: c.bev_charger_supply,\n        c.v2g: c.v2g_supply,\n    }\n    names_withdrawal = {\n        c.bev: c.bev_passenger_withdrawal,\n        c.bev_charger: c.bev_charger_draw,\n        c.v2g: c.v2g_withdrawal,\n    }\n    carrier = [Carrier.bev, Carrier.bev_charger, Carrier.v2g]\n    supply = self.supply(\n        comps=\"Link\",\n        groupby=[\"location\", \"carrier\", \"bus_carrier\"],\n        bus_carrier=[BusCarrier.AC, BusCarrier.LI_ION],\n    )\n    supply = filter_by(supply, carrier=carrier)\n\n    withdrawal = self.withdrawal(\n        comps=\"Link\",\n        groupby=[\"location\", \"carrier\", \"bus_carrier\"],\n        bus_carrier=[BusCarrier.AC, BusCarrier.LI_ION],\n    )\n    withdrawal = filter_by(withdrawal, carrier=carrier)\n    withdrawal = withdrawal.mul(-1)  # to keep withdrawal negative\n\n    # rename carrier to avoid name clashes for supply/withdrawal\n    supply = supply.rename(names_supply, level=DataModel.CARRIER)\n    withdrawal = withdrawal.rename(names_withdrawal, level=DataModel.CARRIER)\n\n    # join along index, sum duplicates and pivot carriers to columns\n    p = (\n        pd.concat([withdrawal, supply])\n        .groupby([DataModel.LOCATION, DataModel.CARRIER])\n        .sum()\n        .unstack()\n    )\n\n    ratio = (p[c.bev_charger_draw] / p[c.bev_charger_supply]).abs()\n\n    p[c.bev_charger_losses] = p[c.bev_charger_draw] + p[c.bev_charger_supply]\n    p[c.bev_demand] = ratio * p[c.bev_passenger_withdrawal]\n    p[c.bev_losses] = p[c.bev_demand] - p[c.bev_passenger_withdrawal]\n    p[c.v2g_demand] = ratio * p[c.v2g_withdrawal] if c.v2g_withdrawal in p else 0\n    p[c.v2g_losses] = p[c.v2g_demand] + p[c.v2g_supply] if c.v2g_supply in p else 0\n\n    ser = insert_index_level(p.stack(), BusCarrier.AC, DataModel.BUS_CARRIER, pos=2)\n    ser.attrs[\"name\"] = \"BEV&amp;V2G\"\n    ser.attrs[\"unit\"] = \"MWh\"\n\n    if drop_v2g_withdrawal:\n        ser = ser.drop(c.v2g_withdrawal, level=DataModel.CARRIER, errors=\"ignore\")\n\n    return ser\n</code></pre>"},{"location":"reference/evals/statistic/#evals.statistic.ESMStatistics.grid_capacity","title":"<code>grid_capacity(comps=None, bus_carrier=None, carrier=None, append_grid=True, align_edges=True)</code>","text":"<p>Return transmission grid capacities.</p> <p>Parameters:</p> Name Type Description Default <code>comps</code> <code>list</code> <p>The network components to consider, defaults to all pypsa.Networks.branch_components.</p> <code>None</code> <code>bus_carrier</code> <code>list</code> <p>The bus carrier to consider.</p> <code>None</code> <code>carrier</code> <code>list</code> <p>The carrier to consider, defaults to all transmission carriers in the network.</p> <code>None</code> <code>append_grid</code> <code>bool</code> <p>Whether to add the grid lines to the result.</p> <code>True</code> <code>align_edges</code> <code>bool</code> <p>Whether to adjust edges between the same nodes but in reversed direction. For example, AC and DC grids have edges between IT0 0 and FR0 0 as IT-&gt;FR and FR-&gt;IT, respectively. If enabled, both will have the same bus0 and bus1.</p> <code>True</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The optimal capacity for transmission technologies between nodes.</p> Notes <p>The \"pypsa.statistics.transmission\" statistic does not work here because it returns energy amounts whereas this statistic returns the optimal capacity.</p> Source code in <code>evals/statistic.py</code> <pre><code>def grid_capacity(\n    self,\n    comps: list = None,\n    bus_carrier: list = None,\n    carrier: list = None,\n    append_grid: bool = True,\n    align_edges: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return transmission grid capacities.\n\n    Parameters\n    ----------\n    comps\n        The network components to consider, defaults to all\n        pypsa.Networks.branch_components.\n    bus_carrier\n        The bus carrier to consider.\n    carrier\n        The carrier to consider, defaults to all\n        transmission carriers in the network.\n    append_grid\n        Whether to add the grid lines to the result.\n    align_edges\n        Whether to adjust edges between the same nodes but in\n        reversed direction. For example, AC and DC grids have\n        edges between IT0 0 and FR0 0 as IT-&gt;FR and FR-&gt;IT,\n        respectively. If enabled, both will have the same bus0 and\n        bus1.\n\n    Returns\n    -------\n    :\n        The optimal capacity for transmission technologies between\n        nodes.\n\n    Notes\n    -----\n    The \"pypsa.statistics.transmission\" statistic does not work here\n    because it returns energy amounts whereas this statistic returns\n    the optimal capacity.\n    \"\"\"\n    n = self._n\n    carrier = carrier or get_transmission_carriers(n, bus_carrier).unique(\"carrier\")\n    capacities = n.statistics.optimal_capacity(\n        comps=comps or n.branch_components,\n        bus_carrier=bus_carrier,\n        groupby=[\"bus0\", \"bus1\", \"carrier\", \"bus_carrier\"],\n    )\n    result = filter_by(capacities, carrier=carrier)\n\n    result.attrs[\"name\"] = \"Capacity\"\n    result.attrs[\"unit\"] = \"MW\"\n    result.name = f\"{result.attrs['name']} ({result.attrs['unit']})\"\n\n    if align_edges:\n        result = align_edge_directions(result)\n\n    if append_grid:\n        result = add_grid_lines(n.static(\"Bus\"), result)\n\n    return result.sort_index()\n</code></pre>"},{"location":"reference/evals/statistic/#evals.statistic.ESMStatistics.grid_flow","title":"<code>grid_flow(comps=None, bus_carrier=None, carrier=None, aggregate_time='sum', append_grid=True)</code>","text":"<p>Return the transmission grid energy flow.</p> <p>Parameters:</p> Name Type Description Default <code>comps</code> <code>list</code> <p>The network components to consider, defaults to all pypsa.Networks.branch_components.</p> <code>None</code> <code>bus_carrier</code> <code>list</code> <p>The bus carrier to consider.</p> <code>None</code> <code>carrier</code> <code>list</code> <p>The carrier to consider, defaults to all transmission carrier in the network.</p> <code>None</code> <code>aggregate_time</code> <code>str</code> <p>The aggregation function aggregate by.</p> <code>'sum'</code> <code>append_grid</code> <code>bool</code> <p>Whether to add the grid lines to the result.</p> <code>True</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The amount of energy transfer for transmission technologies between nodes.</p> Source code in <code>evals/statistic.py</code> <pre><code>def grid_flow(\n    self,\n    comps: list = None,\n    bus_carrier: list = None,\n    carrier: list = None,\n    aggregate_time: str = \"sum\",\n    append_grid: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return the transmission grid energy flow.\n\n    Parameters\n    ----------\n    comps\n        The network components to consider, defaults to all\n        pypsa.Networks.branch_components.\n    bus_carrier\n        The bus carrier to consider.\n    carrier\n        The carrier to consider, defaults to all\n        transmission carrier in the network.\n    aggregate_time\n        The aggregation function aggregate by.\n    append_grid\n        Whether to add the grid lines to the result.\n\n    Returns\n    -------\n    :\n        The amount of energy transfer for transmission technologies\n        between nodes.\n    \"\"\"\n    n = self._n\n    carrier = carrier or get_transmission_carriers(n, bus_carrier).unique(\"carrier\")\n    comps = comps or n.branch_components\n\n    energy_transmission = n.statistics.transmission(\n        comps=comps,\n        groupby=[\"bus0\", \"bus1\", \"carrier\", \"bus_carrier\"],\n        bus_carrier=bus_carrier,\n        aggregate_time=False,\n    )\n    energy_transmission = filter_by(energy_transmission, carrier=carrier)\n\n    # split directions:\n    # positive values are from bus0 to bus1, i.e. bus1 supply\n    bus0_to_bus1 = energy_transmission.clip(lower=0)\n\n    # negative values are from bus1 to bus0, i.e. bus0 supply\n    idx_names = list(energy_transmission.index.names)\n    bus1_to_bus0 = energy_transmission.clip(upper=0).mul(-1)\n    # reverse the node index levels to show positive values and\n    # have a consistent way of interpreting the energy flow\n    bus1_to_bus0 = bus1_to_bus0.swaplevel(\"bus0\", \"bus1\")\n    pos0, pos_1 = idx_names.index(\"bus0\"), idx_names.index(\"bus1\")\n    idx_names[pos_1], idx_names[pos0] = idx_names[pos0], idx_names[pos_1]\n    bus1_to_bus0.index.names = idx_names\n\n    result = pd.concat([bus0_to_bus1, bus1_to_bus0])\n    result = result.groupby(idx_names).sum()\n\n    assert aggregate_time, \"Time Series is not supported.\"\n    unit = \"MW\"\n    if aggregate_time in (\"max\", \"min\"):\n        result = result.agg(aggregate_time, axis=1)\n    elif aggregate_time:  # mean, median, etc.\n        weights = get_weightings(n, comps)\n        result = result.mul(weights, axis=1).agg(aggregate_time, axis=1)\n        unit = \"MWh\"\n\n    result.attrs[\"name\"] = \"Energy\"\n    result.attrs[\"unit\"] = unit\n    result.name = f\"{result.attrs['name']} ({result.attrs['unit']})\"\n\n    if append_grid:\n        result = add_grid_lines(n.static(\"Bus\"), result)\n\n    return result.sort_index()\n</code></pre>"},{"location":"reference/evals/statistic/#evals.statistic.ESMStatistics.phs_hydro_operation","title":"<code>phs_hydro_operation()</code>","text":"<p>Calculate Hydro- and Pumped Hydro Storage unit statistics.</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>Cumulated or constant time series for storage units.</p> Source code in <code>evals/statistic.py</code> <pre><code>def phs_hydro_operation(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate Hydro- and Pumped Hydro Storage unit statistics.\n\n    Returns\n    -------\n    :\n        Cumulated or constant time series for storage units.\n    \"\"\"\n    n = self._n\n    ts_efficiency_name_agg = [\n        (\"p_dispatch\", \"efficiency_dispatch\", Group.turbine_cum, \"cumsum\"),\n        (\"p_store\", \"efficiency_store\", Group.pumping_cum, \"cumsum\"),\n        (\"spill\", None, Group.spill_cum, \"cumsum\"),\n        (\"inflow\", None, Group.inflow_cum, \"cumsum\"),\n        (\"state_of_charge\", None, Group.soc, None),\n    ]\n\n    weights = get_weightings(n, \"StorageUnit\")\n\n    su = n.static(\"StorageUnit\").query(\"carrier in ['PHS', 'hydro']\")\n\n    results = []\n    for time_series, efficiency, index_name, agg in ts_efficiency_name_agg:\n        df = n.pnl(\"StorageUnit\")[time_series].filter(su.index, axis=1)\n        if agg:\n            df = df.mul(weights, axis=0).agg(agg)\n        if efficiency == \"efficiency_dispatch\":\n            df = df / su[efficiency]\n        elif efficiency == \"efficiency_store\":\n            df = df * su[efficiency]\n        # The actual bus carrier is \"AC\" for both, PHS and hydro.\n        # Since only PHS and hydro are considered, we can use the\n        # bus_carrier level to track groups.\n        result = insert_index_level(df, index_name, DataModel.BUS_CARRIER, axis=1)\n        results.append(result.T)\n\n    # broadcast storage volume to time series (not quite the\n    # same as utils.scalar_to_time_series, because it's a series)\n    volume = su[\"p_nom_opt\"] * su[\"max_hours\"]\n    volume_ts = pd.concat([volume] * len(n.snapshots), axis=1)\n    volume_ts.columns = n.snapshots\n    volume_ts = insert_index_level(volume_ts, Group.soc_max, DataModel.BUS_CARRIER)\n    results.append(volume_ts)\n\n    statistic = pd.concat(results)\n    statistic.index = split_location_carrier(\n        statistic.index,\n        names=[DataModel.BUS_CARRIER, DataModel.LOCATION, DataModel.CARRIER],\n    )\n    statistic = statistic.reorder_levels(DataModel.IDX_NAMES)\n\n    statistic.columns.names = [DataModel.SNAPSHOTS]\n    statistic.attrs[\"name\"] = \"StorageUnit Operation\"\n    statistic.attrs[\"unit\"] = \"MWh\"\n\n    return statistic\n</code></pre>"},{"location":"reference/evals/statistic/#evals.statistic.ESMStatistics.phs_split","title":"<code>phs_split(aggregate_time='sum', drop_hydro_cols=True)</code>","text":"<p>Split energy amounts for StorageUnits.</p> <p>This is done to properly separate primary energy and energy storage, i.e. to separate the natural inflow (primary energy) from storage dispatch (secondary energy).</p> <p>Parameters:</p> Name Type Description Default <code>aggregate_time</code> <code>str</code> <p>The aggregation function used to aggregate time steps.</p> <code>'sum'</code> <code>drop_hydro_cols</code> <code>bool</code> <p>Whether, or not to drop 'hydro' carriers from the result. This is required to stay consistent with the old Toolbox implementation.</p> <code>True</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>A DataFrame containing the split energy amounts for PHS and hydro.</p> Notes <p>Not needed if all PHS are implemeted as closed loops. The method is kept if open loop PHS is available.</p> Source code in <code>evals/statistic.py</code> <pre><code>def phs_split(\n    self, aggregate_time: str = \"sum\", drop_hydro_cols: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Split energy amounts for StorageUnits.\n\n    This is done to properly separate primary energy and energy\n    storage, i.e. to separate the natural inflow (primary energy)\n    from storage dispatch (secondary energy).\n\n    Parameters\n    ----------\n    aggregate_time\n        The aggregation function used to aggregate time steps.\n\n    drop_hydro_cols\n        Whether, or not to drop 'hydro' carriers from the result.\n        This is required to stay consistent with the old Toolbox\n        implementation.\n\n    Returns\n    -------\n    :\n        A DataFrame containing the split energy amounts for\n        PHS and hydro.\n\n    Notes\n    -----\n    Not needed if all PHS are implemeted as closed loops. The method is kept\n    if open loop PHS is available.\n    \"\"\"\n    n = self._n\n\n    idx = n.static(\"StorageUnit\").index\n    phs = pd.DataFrame(index=idx)\n    for time_series in (\"p_dispatch\", \"p_store\", \"spill\", \"inflow\"):\n        p = n.pnl(\"StorageUnit\")[time_series].reindex(columns=idx, fill_value=0)\n        weights = get_weightings(n, \"StorageUnit\")\n        phs[time_series] = n.statistics._aggregate_timeseries(\n            p, weights, agg=aggregate_time\n        )\n\n    # calculate the potential dispatch energy for storages\n    stored_energy = phs[\"p_store\"] * n.static(\"StorageUnit\")[\"efficiency_dispatch\"]\n    share_inflow = phs[\"inflow\"] / (phs[\"inflow\"] + stored_energy)\n\n    phs[\"Dispatched Power from Inflow\"] = phs[\"p_dispatch\"] * share_inflow\n    phs[\"Dispatched Power from Stored\"] = phs[\"p_dispatch\"] * (1 - share_inflow)\n    phs[\"Spill from Inflow\"] = phs[\"spill\"] * share_inflow\n    phs[\"Spill from Stored\"] = phs[\"spill\"] * (1 - share_inflow)\n\n    mapper = {\n        \"p_dispatch\": \"Dispatched Power\",\n        \"p_store\": \"Stored Power\",\n        \"inflow\": \"Inflow\",\n        \"spill\": \"Spill\",\n    }\n    phs = phs.rename(mapper, axis=1)\n\n    ser = phs.stack()\n    ser.index = ser.index.swaplevel(0, 1)\n    ser.index = split_location_carrier(ser.index, names=DataModel.IDX_NAMES)\n\n    # merge 'carrier' with 'bus_carrier' level and keep original\n    # bus_carrier. Needed to stay consistent with the old Toolbox\n    # naming conventions.\n    ser.index = pd.MultiIndex.from_tuples(\n        [(r[1], f\"{r[2]} {r[0]}\", r[2]) for r in ser.index],\n        names=DataModel.IDX_NAMES,\n    )\n    ser = ser.rename(\n        index={\"PHS\": BusCarrier.AC, \"hydro\": BusCarrier.AC},\n        level=DataModel.BUS_CARRIER,\n    )\n\n    ser.attrs[\"name\"] = \"PHS&amp;Hydro\"\n    ser.attrs[\"unit\"] = \"MWh\"\n\n    if drop_hydro_cols:\n        cols = [\n            \"hydro Dispatched Power from Inflow\",\n            \"hydro Dispatched Power from Stored\",\n            \"hydro Spill from Inflow\",\n            \"hydro Spill from Stored\",\n        ]\n        ser = ser.drop(cols, level=DataModel.CARRIER)\n\n    return ser.sort_index()\n</code></pre>"},{"location":"reference/evals/statistic/#evals.statistic.ESMStatistics.trade_capacity","title":"<code>trade_capacity(scope, bus_carrier='')</code>","text":"<p>Calculate exchange capacity between locations.</p> <p>Parameters:</p> Name Type Description Default <code>scope</code> <code>str</code> <p>The scope of energy exchange. Must be one of constants.TRADE_TYPES.</p> required <code>bus_carrier</code> <code>str</code> <p>The bus carrier for which to calculate the energy exchange. Defaults to using all bus carrier.</p> <code>''</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>Energy exchange capacity between locations.</p> Source code in <code>evals/statistic.py</code> <pre><code>def trade_capacity(\n    self,\n    scope: str,\n    bus_carrier: str = \"\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate exchange capacity between locations.\n\n    Parameters\n    ----------\n    scope\n        The scope of energy exchange. Must be one of\n        constants.TRADE_TYPES.\n    bus_carrier\n        The bus carrier for which to calculate the energy exchange.\n        Defaults to using all bus carrier.\n\n    Returns\n    -------\n    :\n        Energy exchange capacity between locations.\n    \"\"\"\n    n = self._n\n\n    capacity = self.optimal_capacity(\n        comps=n.branch_components,\n        bus_carrier=bus_carrier,\n        groupby=[\"bus0\", \"bus1\", \"carrier\", \"bus_carrier\"],\n        nice_names=False,\n    ).to_frame()\n    trade_type = capacity.apply(\n        lambda row: get_trade_type(row.name[1], row.name[2]), axis=1\n    )\n\n    trade_capacity = capacity[trade_type == scope]\n\n    # duplicate capacities to list them for source and destination\n    # locations. For example, the trade capacity for AT -&gt; DE gas\n    # pipeline will be shown in location AT and in location DE.\n    df_list = []\n    for bus in (\"bus0\", \"bus1\"):\n        df = trade_capacity.droplevel(bus)\n        df.index.names = [DataModel.COMPONENT] + DataModel.IDX_NAMES\n        df_list.append(df)\n\n    trade_capacity = pd.concat(df_list).drop_duplicates()\n\n    return trade_capacity.squeeze()\n</code></pre>"},{"location":"reference/evals/statistic/#evals.statistic.ESMStatistics.trade_energy","title":"<code>trade_energy(scope, direction='saldo', bus_carrier=None, aggregate_time='sum')</code>","text":"<p>Calculate energy amounts exchanged between locations.</p> <p>Returns positive values for 'import' (supply) and negative values for 'export' (withdrawal).</p> <p>Parameters:</p> Name Type Description Default <code>scope</code> <code>str | tuple</code> <p>The scope of energy exchange. Must be one of \"foreign\", \"domestic\", or \"local\".</p> required <code>direction</code> <code>str</code> <p>The direction of the trade. Can be one of \"saldo\", \"export\", or \"import\".</p> <code>'saldo'</code> <code>bus_carrier</code> <code>str</code> <p>The bus carrier for which to calculate the energy exchange. Defaults to using all bus carrier.</p> <code>None</code> <code>aggregate_time</code> <code>str</code> <p>The method of aggregating the energy exchange over time. Can be one of \"sum\", \"mean\", \"max\", \"min\".</p> <code>'sum'</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>A DataFrame containing the calculated energy exchange between locations.</p> Source code in <code>evals/statistic.py</code> <pre><code>def trade_energy(\n    self,\n    scope: str | tuple,\n    direction: str = \"saldo\",\n    bus_carrier: str = None,\n    aggregate_time: str = \"sum\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate energy amounts exchanged between locations.\n\n    Returns positive values for 'import' (supply) and negative\n    values for 'export' (withdrawal).\n\n    Parameters\n    ----------\n    scope\n        The scope of energy exchange. Must be one of \"foreign\",\n        \"domestic\", or \"local\".\n\n    direction\n        The direction of the trade. Can be one of \"saldo\", \"export\",\n        or \"import\".\n\n    bus_carrier\n        The bus carrier for which to calculate the energy exchange.\n        Defaults to using all bus carrier.\n\n    aggregate_time\n        The method of aggregating the energy exchange over time.\n        Can be one of \"sum\", \"mean\", \"max\", \"min\".\n\n    Returns\n    -------\n    :\n        A DataFrame containing the calculated energy exchange\n        between locations.\n    \"\"\"\n    n = self._n\n    results_comp = []\n\n    buses = n.static(\"Bus\").reset_index()\n    if bus_carrier:\n        _bc = [bus_carrier] if isinstance(bus_carrier, str) else bus_carrier\n        buses = buses.query(\"carrier in @_bc\")\n\n    carrier = get_transmission_carriers(n, bus_carrier).unique(\"carrier\")  # Noqa: F841\n    comps = get_transmission_carriers(n, bus_carrier).unique(\"component\")\n\n    for port, c in product((0, 1), comps):\n        mask = trade_mask(n.static(c), scope).to_numpy()\n        comp = n.static(c)[mask].reset_index()\n\n        p = buses.merge(\n            comp.query(\"carrier.isin(@carrier)\"),\n            left_on=\"Bus\",\n            right_on=f\"bus{port}\",\n            suffixes=(\"_bus\", \"\"),\n        ).merge(n.pnl(c).get(f\"p{port}\").T, on=c)\n\n        _location = (\n            DataModel.LOCATION + \"_bus\"\n            if \"location\" in comp\n            else DataModel.LOCATION\n        )\n        p = p.set_index([_location, DataModel.CARRIER, \"carrier_bus\", \"unit\"])\n        p.index.names = DataModel.IDX_NAMES + [\"unit\"]\n        # branch components have reversed sign\n        p = p.filter(n.snapshots, axis=1).mul(-1)\n        if direction == \"export\":\n            p = p.clip(upper=0)  # keep negative values (withdrawal)\n        elif direction == \"import\":\n            p = p.clip(lower=0)  # keep positive values (supply)\n        elif direction != \"saldo\":\n            raise ValueError(f\"Direction '{direction}' not supported.\")\n\n        results_comp.append(insert_index_level(p, c, \"component\"))\n\n    result = pd.concat(results_comp)\n\n    if aggregate_time:\n        # assuming Link and Line have the same weights\n        weights = get_weightings(n, \"Link\")\n        result = result.multiply(weights, axis=1)\n        result = result.agg(aggregate_time, axis=1)\n\n    name = \" &amp; \".join(scope) if isinstance(scope, tuple) else scope\n    result.attrs[\"name\"] = f\"{name} {direction}\"\n    result.attrs[\"unit\"] = \"MWh\"\n\n    return result.sort_index()\n</code></pre>"},{"location":"reference/evals/statistic/#evals.statistic.collect_myopic_statistics","title":"<code>collect_myopic_statistics(networks, statistic, aggregate_components='sum', drop_zeros=True, drop_unit=True, **kwargs)</code>","text":"<p>Build a myopic statistic from loaded networks.</p> <p>This method calls ESMStatisticsAccessor methods. It calls the statistics method for every year and optionally aggregates components, e.g. Links and Lines often should become summed up.</p> <p>Parameters:</p> Name Type Description Default <code>networks</code> <code>dict</code> <p>The loaded networks in a dictionary with the year as keys.</p> required <code>statistic</code> <code>str</code> <p>The name of the metric to build.</p> required <code>aggregate_components</code> <code>str | None</code> <p>The aggregation function to combine components by.</p> <code>'sum'</code> <code>drop_zeros</code> <code>bool</code> <p>Whether to drop rows from the returned statistic that have only zeros as values.</p> <code>True</code> <code>drop_unit</code> <code>bool</code> <p>Whether to drop the unit index level from the returned statistic.</p> <code>True</code> <code>**kwargs</code> <code>object</code> <p>Any key word argument accepted by the statistics function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>pandas.DataFrame | pandas.Series</code> <p>The built statistic with the year as the outermost index level.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>In case a non-existent statistics function was requested.</p> Source code in <code>evals/statistic.py</code> <pre><code>def collect_myopic_statistics(\n    networks: dict,\n    statistic: str,\n    aggregate_components: str | None = \"sum\",\n    drop_zeros: bool = True,\n    drop_unit: bool = True,\n    **kwargs: object,\n) -&gt; pd.DataFrame | pd.Series:\n    \"\"\"\n    Build a myopic statistic from loaded networks.\n\n    This method calls ESMStatisticsAccessor methods. It calls the\n    statistics method for every year and optionally aggregates\n    components, e.g. Links and Lines often should become summed up.\n\n    Parameters\n    ----------\n    networks\n        The loaded networks in a dictionary with the year as keys.\n    statistic\n        The name of the metric to build.\n    aggregate_components\n        The aggregation function to combine components by.\n    drop_zeros\n        Whether to drop rows from the returned statistic that have\n        only zeros as values.\n    drop_unit\n        Whether to drop the unit index level from the returned statistic.\n    **kwargs\n        Any key word argument accepted by the statistics function.\n\n    Returns\n    -------\n    :\n        The built statistic with the year as the outermost index level.\n\n    Raises\n    ------\n    ValueError\n        In case a non-existent statistics function was requested.\n    \"\"\"\n    kwargs = kwargs or {}\n\n    pypsa_statistics = [m[0] for m in getmembers(pypsa.statistics.StatisticsAccessor)]\n\n    if statistic in pypsa_statistics:  # register a default to reduce verbosity\n        kwargs.setdefault(\"groupby\", [\"location\", \"carrier\", \"bus_carrier\", \"unit\"])\n\n    year_statistics = []\n    for year, n in networks.items():\n        func = getattr(n.statistics, statistic)\n        assert func, (\n            f\"Statistic '{statistic}' not found. \"\n            f\"Available statistics are: \"\n            f\"'{[m[0] for m in getmembers(n.statistics)]}'.\"\n        )\n        year_statistic = func(**kwargs)\n        year_statistic = insert_index_level(year_statistic, year, DataModel.YEAR)\n        year_statistics.append(year_statistic)\n\n    statistic = pd.concat(year_statistics, axis=0, sort=True)\n    if DataModel.LOCATION in statistic.index.names:\n        if \"EU\" in statistic.index.unique(DataModel.LOCATION):\n            logger.debug(\n                f\"EU node found in statistic:\\n\"\n                f\"{filter_by(statistic, location='EU')}\"\n                f\"\\n\\nPlease check if this is intentional!\"\n            )\n\n    if aggregate_components and \"component\" in statistic.index.names:\n        _names = statistic.index.droplevel(\"component\").names\n        statistic = statistic.groupby(_names).agg(aggregate_components)\n\n    if kwargs.get(\"aggregate_time\") is False:\n        statistic.columns.name = DataModel.SNAPSHOTS\n\n    if drop_zeros:\n        if isinstance(statistic, pd.Series):\n            statistic = statistic.loc[statistic != 0]\n        elif isinstance(statistic, pd.DataFrame):\n            statistic = statistic.loc[(statistic != 0).any(axis=1)]\n        else:\n            raise TypeError(f\"Unknown statistic type '{type(statistic)}'\")\n\n    # assign the correct unit the statistic if possible\n    if \"unit\" in statistic.index.names and drop_unit:\n        if not statistic.empty:\n            try:\n                statistic.attrs[\"unit\"] = statistic.index.unique(\"unit\").item()\n            except ValueError:\n                logger.warning(\n                    f\"Mixed units detected in statistic: {statistic.index.unique('unit')}.\"\n                )\n        statistic = statistic.droplevel(\"unit\")\n\n    return statistic.sort_index()\n</code></pre>"},{"location":"reference/evals/statistic/#evals.statistic.get_location","title":"<code>get_location(n, c, port='', location_port='', avoid_eu_locations=True)</code>","text":"<p>Return the grouper series for the location of a component.</p> <p>The additional location port argument will swap the bus location to the specified bus port locations. The default location is the location from buses at the \"port\" argument. But be careful, the location override will happen for all ports of the component.</p> <p>Note, that the bus_carrier will still be the bus_carrier from the \"port\" argument, i.e. only the location is swapped.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>pypsa.Network</code> <p>The network to evaluate.</p> required <code>c</code> <code>str</code> <p>The component name, e.g. 'Load', 'Generator', 'Link', etc.</p> required <code>port</code> <code>str</code> <p>Limit results to this branch port.</p> <code>''</code> <code>location_port</code> <code>str</code> <p>Use the specified port bus for the location, defaults to using the location of the 'port' bus.</p> <code>''</code> <code>avoid_eu_locations</code> <code>bool</code> <p>Look into the port 0 and port 1 location in branch components and prefer locations that are not 'EU'.</p> <code>True</code> <p>Returns:</p> Type Description <code>pandas.Series</code> <p>A list of series to group statistics by.</p> Source code in <code>evals/statistic.py</code> <pre><code>def get_location(\n    n: pypsa.Network,\n    c: str,\n    port: str = \"\",\n    location_port: str = \"\",\n    avoid_eu_locations: bool = True,\n) -&gt; pd.Series:\n    \"\"\"\n    Return the grouper series for the location of a component.\n\n    The additional location port argument will swap the bus\n    location to the specified bus port locations. The default\n    location is the location from buses at the \"port\" argument.\n    But be careful, the location override will happen for all\n    ports of the component.\n\n    Note, that the bus_carrier will still be the bus_carrier\n    from the \"port\" argument, i.e. only the location is swapped.\n\n    Parameters\n    ----------\n    n\n        The network to evaluate.\n    c\n        The component name, e.g. 'Load', 'Generator', 'Link', etc.\n    port\n        Limit results to this branch port.\n    location_port\n        Use the specified port bus for the location, defaults to\n        using the location of the 'port' bus.\n    avoid_eu_locations\n        Look into the port 0 and port 1 location in branch components\n        and prefer locations that are not 'EU'.\n\n    Returns\n    -------\n    :\n        A list of series to group statistics by.\n    \"\"\"\n    if avoid_eu_locations and c in n.branch_components:\n        bus0 = n.static(c)[\"bus0\"].map(n.static(\"Bus\").location).rename(\"loc0\")\n        bus1 = n.static(c)[\"bus1\"].map(n.static(\"Bus\").location).rename(\"loc1\")\n        buses = pd.concat([bus0, bus1], axis=1)\n\n        def _select_location(row) -&gt; str:\n            if row.loc0 != \"EU\" or pd.isna(row.loc1):\n                return row.loc0\n            return row.loc1\n\n        return buses.apply(_select_location, axis=1).rename(\"location\")\n\n        # selection order: country code &gt; EU &gt; NaN\n\n    # todo: probably obsolete?\n    if location_port and c in n.branch_components:\n        buses = n.static(c)[f\"bus{location_port}\"]\n        return buses.map(n.static(\"Bus\").location).rename(DataModel.LOCATION)\n\n    return n.static(c)[f\"bus{port}\"].map(n.buses.location).rename(\"location\")\n</code></pre>"},{"location":"reference/evals/statistic/#evals.statistic.get_location_from_name_at_port","title":"<code>get_location_from_name_at_port(n, c, location_port='')</code>","text":"<p>Return the location from the component name.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>pypsa.Network</code> <p>The network to evaluate.</p> required <code>c</code> <code>str</code> <p>The component name, e.g. 'Load', 'Generator', 'Link', etc.</p> required <code>location_port</code> <code>str</code> <p>Limit results to this branch port.</p> <code>''</code> <p>Returns:</p> Type Description <code>pandas.Series</code> Source code in <code>evals/statistic.py</code> <pre><code>def get_location_from_name_at_port(\n    n: pypsa.Network, c: str, location_port: str = \"\"\n) -&gt; pd.Series:\n    \"\"\"\n    Return the location from the component name.\n\n    Parameters\n    ----------\n    n\n        The network to evaluate.\n    c\n        The component name, e.g. 'Load', 'Generator', 'Link', etc.\n    location_port\n        Limit results to this branch port.\n\n    Returns\n    -------\n    :\n\n    \"\"\"\n    group = f\"({Regex.region.pattern})\"\n    return (\n        n.static(c)[f\"bus{location_port}\"]\n        .str.extract(group, expand=False)\n        .str.strip()  # some white spaces still go through regex\n        .rename(f\"bus{location_port}\")\n    )\n</code></pre>"},{"location":"reference/evals/utils/","title":"utils.py","text":"<p>Collect package helper functions.</p>"},{"location":"reference/evals/utils/#evals.utils.add_grid_lines","title":"<code>add_grid_lines(buses, statistic)</code>","text":"<p>Add a column with gridlines to a statistic.</p> <p>Parameters:</p> Name Type Description Default <code>buses</code> <code>pandas.DataFrame</code> <p>The Bus component data frame from a pypsa network.</p> required <code>statistic</code> <code>pandas.Series</code> <p>A pandas object with a multiindex. There must be a \"bus0\" and a \"bus1\" multiindex level, that hold the node names.</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>A data frame with an additional \"line\" column that holds x/y coordinate pairs between the respective bus0 and bus1 locations.</p> Source code in <code>evals/utils.py</code> <pre><code>def add_grid_lines(buses: pd.DataFrame, statistic: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"\n    Add a column with gridlines to a statistic.\n\n    Parameters\n    ----------\n    buses\n        The Bus component data frame from a pypsa network.\n\n    statistic\n        A pandas object with a multiindex. There must be a \"bus0\" and\n        a \"bus1\" multiindex level, that hold the node names.\n\n    Returns\n    -------\n    :\n        A data frame with an additional \"line\" column that holds x/y\n        coordinate pairs between the respective bus0 and bus1 locations.\n    \"\"\"\n    if isinstance(statistic, pd.Series):\n        statistic = statistic.to_frame()\n\n    bus0 = statistic.index.get_level_values(\"bus0\").str.strip()\n    bus1 = statistic.index.get_level_values(\"bus1\").str.strip()\n    ac_buses = filter_by(buses, carrier=\"AC\")[[\"x\", \"y\"]]\n\n    def _get_bus_lines(_nodes: tuple[str]) -&gt; np.ndarray:\n        \"\"\"\n        Draw a line between buses using AC bus coordinates.\n\n        Note, that only AC buses have coordinates assigned.\n\n        Parameters\n        ----------\n        _nodes\n            The start node name and the end node name in a tuple.\n\n        Returns\n        -------\n        :\n            A one dimensional array with lists of coordinate pairs,\n            i.e. grid lines.\n        \"\"\"\n        return ac_buses.loc[[*_nodes]][[\"y\", \"x\"]].values.tolist()\n\n    # generate lines [(x0, y0), (x1,y1)] between buses for every\n    # row in grid and store it in a new column\n    statistic[\"line\"] = [*map(_get_bus_lines, zip(bus0, bus1, strict=True))]\n\n    return statistic\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.aggregate_eu","title":"<code>aggregate_eu(df, agg='sum')</code>","text":"<p>Calculate the EU region as the sum of all country regions.</p> <p>The carrier 'import net', 'export net', 'Import European' and ' Export European' need to be removed from the EU data set. The total import and export over all countries evens out and is not required for EU location. The non-EU imports are named differently, e.g. 'global import'.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The data frame with one MultiIndex level named 'location'.</p> required <code>agg</code> <code>str</code> <p>The aggregation function.</p> <code>'sum'</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>Summed metric with one location named 'EU'.</p> Source code in <code>evals/utils.py</code> <pre><code>def aggregate_eu(df: pd.DataFrame, agg: str = \"sum\") -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the EU region as the sum of all country regions.\n\n    The carrier 'import net', 'export net', 'Import European' and '\n    Export European' need to be removed from the EU data set.\n    The total import and export over all countries evens out and\n    is not required for EU location. The non-EU imports\n    are named differently, e.g. 'global import'.\n\n    Parameters\n    ----------\n    df\n        The data frame with one MultiIndex level named 'location'.\n    agg\n        The aggregation function.\n\n    Returns\n    -------\n    :\n        Summed metric with one location named 'EU'.\n    \"\"\"\n    df = df.query(f\"{DataModel.LOCATION} not in ['EU', '']\")  # valid countries only\n    totals = rename_aggregate(df, \"EU\", level=DataModel.LOCATION, agg=agg)\n    excluded = [\n        Group.import_net,  # required for CH4 and H2!\n        Group.export_net,\n        Group.import_foreign,\n        Group.export_foreign,\n        # exclude domestic trade for EU region\n        Group.import_domestic,\n        Group.export_domestic,\n        Carrier.import_domestic,\n        Carrier.export_domestic,\n    ]\n    return totals.drop(excluded, level=DataModel.CARRIER, errors=\"ignore\")\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.aggregate_locations","title":"<code>aggregate_locations(df, keep_regions=('AT',), nice_names=True)</code>","text":"<p>Aggregate to countries, including EU and keeping certain regions.</p> <p>The input data frame is expected to contain locations as regions, e.g. \"AT0 1\", \"FR0 0\", etc.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The input data frame with a locations index level.</p> required <code>keep_regions</code> <code>tuple</code> <p>A tuple of regions, that should be preserved in the output, i.e. they are added to the result as before the aggregation.</p> <code>('AT',)</code> <code>nice_names</code> <code>bool</code> <p>Whether, or not to use the nice country names instead of the country codes.</p> <code>True</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>A data frame with aggregated countries, plus any region in 'keep_regions' and Europe/EU.</p> Source code in <code>evals/utils.py</code> <pre><code>def aggregate_locations(\n    df: pd.DataFrame,\n    keep_regions: tuple = (\"AT\",),\n    nice_names: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregate to countries, including EU and keeping certain regions.\n\n    The input data frame is expected to contain locations as regions,\n    e.g. \"AT0 1\", \"FR0 0\", etc.\n\n    Parameters\n    ----------\n    df\n        The input data frame with a locations index level.\n    keep_regions\n        A tuple of regions, that should be preserved in the output,\n        i.e. they are added to the result as before the aggregation.\n    nice_names\n        Whether, or not to use the nice country names instead of the\n        country codes.\n\n    Returns\n    -------\n    :\n        A data frame with aggregated countries, plus any region in\n        'keep_regions' and Europe/EU.\n    \"\"\"\n    country_code_map = {loc: loc[:2] for loc in df.index.unique(DataModel.LOCATION)}\n    if \"EU\" in country_code_map.values():\n        logger = logging.getLogger(__name__)\n        logger.warning(\n            \"Values for 'EU' node found in input data frame. \"\n            \"This can lead to value doubling during location aggregation.\",\n        )\n    countries = rename_aggregate(df, country_code_map, level=DataModel.LOCATION)\n    # domestic trade only makes sense between regions. Aggregated\n    # countries could have domestic trade, but import and export nets\n    # to zero.\n    countries = countries.drop(\n        [\n            Carrier.export_domestic,\n            Carrier.import_domestic,\n            Group.import_domestic,\n            Group.export_domestic,\n        ],\n        level=DataModel.CARRIER,\n        errors=\"ignore\",\n    )\n    europe = aggregate_eu(df)\n    mask = df.index.get_level_values(DataModel.LOCATION).str.startswith(keep_regions)\n    regions = df.loc[mask, :]\n    result = pd.concat([countries, regions, europe]).sort_index(axis=0)\n    if nice_names:\n        result = result.rename(index=ALIAS_LOCATION, level=DataModel.LOCATION)\n    return result\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.align_edge_directions","title":"<code>align_edge_directions(df, lvl0='bus0', lvl1='bus1')</code>","text":"<p>Align the directionality of edges between two nodes.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The input data frame with a multiindex.</p> required <code>lvl0</code> <code>str</code> <p>The first MultiIndex level name to swap values.</p> <code>'bus0'</code> <code>lvl1</code> <code>str</code> <p>The second MultiIndex level name to swap values.</p> <code>'bus1'</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The input data frame with aligned edge directions between the nodes in lvl1 and lvl0.</p> Source code in <code>evals/utils.py</code> <pre><code>def align_edge_directions(\n    df: pd.DataFrame, lvl0: str = \"bus0\", lvl1: str = \"bus1\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Align the directionality of edges between two nodes.\n\n    Parameters\n    ----------\n    df\n        The input data frame with a multiindex.\n    lvl0\n        The first MultiIndex level name to swap values.\n    lvl1\n        The second MultiIndex level name to swap values.\n\n    Returns\n    -------\n    :\n        The input data frame with aligned edge directions between the\n        nodes in lvl1 and lvl0.\n    \"\"\"\n    seen = []\n\n    def _reverse_values_if_seen(df_slice: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Reverse index levels if they have a duplicated permutation.\n\n        Parameters\n        ----------\n        df_slice\n            A slice of a data frame with the bus0 and bus1 index level.\n\n        Returns\n        -------\n        :\n            The slice with exchanged level values if the combination of\n            lvl1 and lvl2 is not unique and the original slice\n            otherwise.\n        \"\"\"\n        buses = {df_slice.index.unique(lvl0)[0], df_slice.index.unique(lvl1)[0]}\n        if buses in seen:\n            reversed_slice = df_slice.swaplevel(lvl0, lvl1)\n            # keep original names since we only want to swap values\n            reversed_slice.index.names = df_slice.index.names\n            return reversed_slice\n        else:\n            seen.append(buses)\n            return df_slice\n\n    return df.groupby([lvl0, lvl1], group_keys=False).apply(\n        _reverse_values_if_seen,\n    )\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.apply_cutoff","title":"<code>apply_cutoff(df, limit, drop=True)</code>","text":"<p>Replace small absolute values with NaN.</p> <p>The limit boundary is not inclusive, i.e. the limit value itself will not be replaced by NaN.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The data frame to remove values from.</p> required <code>limit</code> <code>float</code> <p>Absolute values smaller than the limit will be dropped.</p> required <code>drop</code> <code>bool</code> <p>Whether to drop all NaN rows from the returned data frame.</p> <code>True</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>A data frame without values that are smaller than the limit.</p> Source code in <code>evals/utils.py</code> <pre><code>def apply_cutoff(df: pd.DataFrame, limit: float, drop: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Replace small absolute values with NaN.\n\n    The limit boundary is not inclusive, i.e. the limit value itself\n    will not be replaced by NaN.\n\n    Parameters\n    ----------\n    df\n        The data frame to remove values from.\n    limit\n        Absolute values smaller than the limit will be dropped.\n    drop\n        Whether to drop all NaN rows from the returned data frame.\n\n    Returns\n    -------\n    :\n        A data frame without values that are smaller than the limit.\n    \"\"\"\n    result = df.mask(cond=df.abs() &lt; abs(limit), other=pd.NA)\n    if drop:\n        result = result.dropna(how=\"all\", axis=0)\n    return result\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.calculate_cost_annuity","title":"<code>calculate_cost_annuity(n, r=0.07)</code>","text":"<p>Calculate the annuity factor for an asset.</p> <p>Calculate the annuity factor for an asset with lifetime n years and discount rate of r, e.g. annuity(20,0.05)*20 = 1.6</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>float</code> <p>The lifetime of the asset in years.</p> required <code>r</code> <code>float | pandas.Series</code> <p>The discount rate of the asset.</p> <code>0.07</code> <p>Returns:</p> Type Description <code>float | pandas.Series</code> <p>The calculated annuity factors.</p> Notes <p>This function was adopted from the abandoned package \"vresutils\".</p> Source code in <code>evals/utils.py</code> <pre><code>def calculate_cost_annuity(n: float, r: float | pd.Series = 0.07) -&gt; float | pd.Series:\n    \"\"\"\n    Calculate the annuity factor for an asset.\n\n    Calculate the annuity factor for an asset with lifetime n years and\n    discount rate of r, e.g. annuity(20,0.05)*20 = 1.6\n\n    Parameters\n    ----------\n    n\n        The lifetime of the asset in years.\n    r\n        The discount rate of the asset.\n\n    Returns\n    -------\n    :\n        The calculated annuity factors.\n\n    Notes\n    -----\n    This function was adopted from the abandoned package \"vresutils\".\n    \"\"\"\n    if isinstance(r, pd.Series):\n        ser = pd.Series(1 / n, index=r.index)\n        return ser.where(r == 0, r / (1.0 - 1.0 / (1.0 + r) ** n))\n    elif r &gt; 0:\n        return r / (1.0 - 1.0 / (1.0 + r) ** n)\n    else:\n        return 1 / n\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.calculate_input_share","title":"<code>calculate_input_share(df, bus_carrier)</code>","text":"<p>Calculate the withdrawal necessary to supply energy for requested bus_carrier.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame | pandas.Series</code> <p>The input DataFrame or Series with a MultiIndex.</p> required <code>bus_carrier</code> <code>str | list</code> <p>Calculates the input energy for this bus_carrier.</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame | pandas.Series</code> <p>The withdrawal amounts necessary to produce energy of <code>bus_carrier</code>.</p> Source code in <code>evals/utils.py</code> <pre><code>def calculate_input_share(\n    df: pd.DataFrame | pd.Series,\n    bus_carrier: str | list,\n) -&gt; pd.DataFrame | pd.Series:\n    \"\"\"\n    Calculate the withdrawal necessary to supply energy for requested bus_carrier.\n\n    Parameters\n    ----------\n    df\n        The input DataFrame or Series with a MultiIndex.\n    bus_carrier\n        Calculates the input energy for this bus_carrier.\n\n    Returns\n    -------\n    :\n        The withdrawal amounts necessary to produce energy of `bus_carrier`.\n    \"\"\"\n\n    def _input_share(_df):\n        withdrawal = _df[_df.lt(0)]\n        supply = _df[_df.ge(0)]\n        bus_carrier_supply = filter_by(supply, bus_carrier=bus_carrier).sum()\n        # scaling takes into account that Link inputs and outputs are not equally large\n        scaling = abs(supply.sum() / withdrawal.sum())\n        # share takes multiple outputs into account\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):  # silently divide by zero\n            share = bus_carrier_supply / supply.sum()\n        if scaling &gt; 1.0:\n            _carrier = _df.index.unique(DataModel.CARRIER).item()\n            _bus_carrier = \"ambient heat\" if \"heat pump\" in _carrier else \"latent heat\"\n            surplus = rename_aggregate(\n                withdrawal * (scaling - 1), _bus_carrier, level=DataModel.BUS_CARRIER\n            )\n            return pd.concat([withdrawal, surplus]) * share\n        else:\n            return withdrawal * scaling * share\n\n    wo_bus_carrier = [s for s in df.index.names if s != \"bus_carrier\"]\n    return df.groupby(wo_bus_carrier, group_keys=False).apply(_input_share).mul(-1)\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.combine_statistics","title":"<code>combine_statistics(statistics, metric_name, is_unit, to_unit, keep_regions=('AT', 'GB', 'ES', 'FR', 'DE', 'IT'), region_nice_names=True)</code>","text":"<p>Build the metric data frame from statistics.</p> <p>Parameters:</p> Name Type Description Default <code>statistics</code> <code>list</code> <p>The statistics to combine.</p> required <code>metric_name</code> <code>str</code> <p>The metric name used in plot titles and column labels.</p> required <code>is_unit</code> <code>str</code> <p>The common unit of input statistics.</p> required <code>to_unit</code> <code>str</code> <p>The desired unit of the output metric.</p> required <code>keep_regions</code> <code>tuple</code> <p>A collection of country codes for which original input cluster codes will be included in the metric locations.</p> <code>('AT', 'GB', 'ES', 'FR', 'DE', 'IT')</code> <code>region_nice_names</code> <code>bool</code> <p>Whether to replace location country codes with country/region names.</p> <code>True</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The formatted metric in the desired unit and locations.</p> Source code in <code>evals/utils.py</code> <pre><code>def combine_statistics(\n    statistics: list,\n    metric_name: str,\n    is_unit: str,\n    to_unit: str,\n    keep_regions: tuple = (\"AT\", \"GB\", \"ES\", \"FR\", \"DE\", \"IT\"),\n    region_nice_names: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Build the metric data frame from statistics.\n\n    Parameters\n    ----------\n    statistics\n        The statistics to combine.\n    metric_name\n        The metric name used in plot titles and column labels.\n    is_unit\n        The common unit of input statistics.\n    to_unit\n        The desired unit of the output metric.\n    keep_regions\n        A collection of country codes for which original input\n        cluster codes will be included in the metric locations.\n    region_nice_names\n        Whether to replace location country codes with country/region\n        names.\n\n    Returns\n    -------\n    :\n        The formatted metric in the desired unit and locations.\n    \"\"\"\n    df = pd.concat(statistics)\n\n    if was_series := isinstance(df, pd.Series):\n        df = df.to_frame(f\"{metric_name} ({is_unit})\")\n\n    df = aggregate_locations(df, keep_regions, region_nice_names)\n\n    df.attrs[\"name\"] = metric_name\n    df.attrs[\"unit\"] = to_unit\n\n    df.columns.name = DataModel.METRIC if was_series else DataModel.SNAPSHOTS\n    if df.columns.name == DataModel.SNAPSHOTS:\n        df.columns = pd.to_datetime(df.columns, errors=\"raise\")\n\n    if to_unit and (is_unit != to_unit):\n        df = scale(df, to_unit=to_unit)\n\n    df = _split_trade_saldo_to_netted_import_export(df)\n\n    verify_metric_format(df)\n\n    return df\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.drop_from_multtindex_by_regex","title":"<code>drop_from_multtindex_by_regex(df, pattern, level=DataModel.CARRIER)</code>","text":"<p>Drop all rows that match the regex in the index level.</p> <p>This function is needed, because pandas.DataFrame.filter cannot be applied to MultiIndexes.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The input data frame with a multi index.</p> required <code>pattern</code> <code>str</code> <p>The regular expression pattern as a raw string.</p> required <code>level</code> <code>str</code> <p>The multi index level to match the regex to.</p> <code>evals.constants.DataModel.CARRIER</code> <p>Returns:</p> Type Description <code>pandas.DataFrame | pandas.Series</code> <p>The input data where the regular expression does not match.</p> Source code in <code>evals/utils.py</code> <pre><code>def drop_from_multtindex_by_regex(\n    df: pd.DataFrame, pattern: str, level: str = DataModel.CARRIER\n) -&gt; pd.DataFrame | pd.Series:\n    \"\"\"\n    Drop all rows that match the regex in the index level.\n\n    This function is needed, because pandas.DataFrame.filter cannot\n    be applied to MultiIndexes.\n\n    Parameters\n    ----------\n    df\n        The input data frame with a multi index.\n    pattern\n        The regular expression pattern as a raw string.\n    level\n        The multi index level to match the regex to.\n\n    Returns\n    -------\n    :\n        The input data where the regular expression does not match.\n    \"\"\"\n    mask = df.index.get_level_values(level).str.contains(pattern, regex=True)\n    return df[~mask]\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.expand_to_time_series","title":"<code>expand_to_time_series(df, snapshots, nhours=8760)</code>","text":"<p>Convert time aggregated value to a time series.</p> <p>Any column label will be dropped and replaced by the given snapshots. It is assumed, that the metric holds yearly values, as produced by time aggregation methods. The data frame index and attrs are preserved. Time series value will become the yearly value divided by the number hours per year, i.e. the hourly values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame | pandas.Series</code> <p>A data frame input data frame with one column.</p> required <code>snapshots</code> <code>pandas.Index</code> <p>The columns labels to use in the result (snapshot time stamps).</p> required <code>nhours</code> <code>int</code> <p>Divide values in the input by this number..</p> <code>8760</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The time series data frame with values average values representing the time interval between snapshots.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If a data frame with more than one column is passed.</p> Source code in <code>evals/utils.py</code> <pre><code>def expand_to_time_series(\n    df: pd.DataFrame | pd.Series, snapshots: pd.Index, nhours: int = 8760\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert time aggregated value to a time series.\n\n    Any column label will be dropped and replaced by the given\n    snapshots. It is assumed, that the metric holds yearly values, as\n    produced by time aggregation methods. The data frame index and\n    attrs are preserved. Time series value will become the yearly value\n    divided by the number hours per year, i.e. the hourly values.\n\n    Parameters\n    ----------\n    df\n        A data frame input data frame with one column.\n    snapshots\n        The columns labels to use in the result (snapshot time stamps).\n    nhours\n        Divide values in the input by this number..\n\n    Returns\n    -------\n    :\n        The time series data frame with values average values\n        representing the time interval between snapshots.\n\n    Raises\n    ------\n    NotImplementedError\n        If a data frame with more than one column is passed.\n    \"\"\"\n    if isinstance(df, pd.DataFrame):\n        if df.shape[1] &gt; 1:\n            raise NotImplementedError(\n                f\"Broadcasting multiple columns is not supported. \"\n                f\"Only single column data frames may be passed as \"\n                f\"input, but found {df.shape[1]} columns.\"\n            )\n        df = df.squeeze(axis=1)\n\n    hourly = df / nhours\n    values = np.tile(hourly.to_numpy(), (len(snapshots), 1)).T\n    result = pd.DataFrame(index=df.index, columns=snapshots, data=values)\n    result.attrs = df.attrs\n    return result\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.filter_by","title":"<code>filter_by(df, exclude=False, **kwargs)</code>","text":"<p>Filter a data frame by key value pairs.</p> <p>Constructs a pandas query using the pandas.Index.isin() method. Since the pandas query API is only available for data frames, any passed pandas Series is converted to frame and reset to series.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame | pandas.Series</code> <p>The data frame or Series to filter.</p> required <code>exclude</code> <code>bool</code> <p>Set to True to exclude the filter result from the original data set, and return the difference.</p> <code>False</code> <code>**kwargs</code> <code>object</code> <p>Key=value pairs, used in the filter expression. Valid keys are index level names or column labels.</p> <code>{}</code> <p>Returns:</p> Type Description <code>pandas.DataFrame | pandas.Series</code> <p>The filtered data frame in the same format as the input dataframe.</p> Source code in <code>evals/utils.py</code> <pre><code>def filter_by(\n    df: pd.DataFrame | pd.Series, exclude: bool = False, **kwargs: object\n) -&gt; pd.DataFrame | pd.Series:\n    \"\"\"\n    Filter a data frame by key value pairs.\n\n    Constructs a pandas query using the pandas.Index.isin() method.\n    Since the pandas query API is only available for data frames,\n    any passed pandas Series is converted to frame and reset to\n    series.\n\n    Parameters\n    ----------\n    df\n        The data frame or Series to filter.\n    exclude\n        Set to True to exclude the filter result from the original\n        data set, and return the difference.\n    **kwargs\n        Key=value pairs, used in the filter expression. Valid keys are\n        index level names or column labels.\n\n    Returns\n    -------\n    :\n        The filtered data frame in the same format as the input\n        dataframe.\n    \"\"\"\n    if was_series := isinstance(df, pd.Series):\n        df = df.to_frame()\n\n    where_clauses = []\n    for key, vals in kwargs.items():\n        vals = [vals] if np.isscalar(vals) else vals\n        where_clauses.append(f\"{key} in {vals}\")\n\n    expression = \" &amp; \".join(where_clauses)\n    result = df.query(expression)\n\n    if exclude:\n        result = df.drop(result.index)\n\n    # squeeze(axis=1) to preserve index even for single rows\n    return result.squeeze(axis=1) if was_series else result\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.filter_for_carrier_connected_to","title":"<code>filter_for_carrier_connected_to(df, bus_carrier)</code>","text":"<p>Return a subset with technologies connected to a bus carrier.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The input DataFrame or Series with a MultiIndex.</p> required <code>bus_carrier</code> <code>str | list</code> <p>The bus carrier to filter for.</p> required <p>Returns:</p> Type Description <p>A subset of the input data that contains all location + carrier combinations that have at least one connection to the requested bus_carrier.</p> Source code in <code>evals/utils.py</code> <pre><code>def filter_for_carrier_connected_to(df: pd.DataFrame, bus_carrier: str | list):\n    \"\"\"\n    Return a subset with technologies connected to a bus carrier.\n\n    Parameters\n    ----------\n    df\n        The input DataFrame or Series with a MultiIndex.\n    bus_carrier\n        The bus carrier to filter for.\n\n    Returns\n    -------\n    :\n        A subset of the input data that contains all location + carrier\n        combinations that have at least one connection to the requested\n        bus_carrier.\n    \"\"\"\n    carrier_connected_to_bus_carrier = []\n    locations_connected_to_bus_carrier = []\n    for (loc, carrier), data in df.groupby([DataModel.LOCATION, DataModel.CARRIER]):\n        if filter_by(data, bus_carrier=bus_carrier).any():\n            carrier_connected_to_bus_carrier.append(carrier)\n            locations_connected_to_bus_carrier.append(loc)\n\n    return filter_by(\n        df,\n        carrier=carrier_connected_to_bus_carrier,\n        location=locations_connected_to_bus_carrier,\n    )\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.get_heat_loss_factor","title":"<code>get_heat_loss_factor(networks)</code>","text":"<p>Return the heat loss factor for district heating from the config.</p> <p>Parameters:</p> Name Type Description Default <code>networks</code> <code>dict</code> <p>The loaded networks.</p> required <p>Returns:</p> Type Description <code>The heat loss factor for district heating networks.</code> Source code in <code>evals/utils.py</code> <pre><code>def get_heat_loss_factor(networks: dict) -&gt; int:\n    \"\"\"\n    Return the heat loss factor for district heating from the config.\n\n    Parameters\n    ----------\n    networks\n        The loaded networks.\n\n    Returns\n    -------\n    The heat loss factor for district heating networks.\n    \"\"\"\n    heat_loss_factors = {\n        n.meta[\"sector\"][\"district_heating\"][\"district_heating_loss\"]\n        for n in networks.values()\n    }\n    assert len(heat_loss_factors) == 1, \"Varying loss factors are not supported.\"\n    return heat_loss_factors.pop()\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.get_storage_carriers","title":"<code>get_storage_carriers(networks)</code>","text":"<p>Get the storage carriers from the networks.</p> <p>Parameters:</p> Name Type Description Default <code>networks</code> <code>dict</code> <p>The loaded networks.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of storage carrier names.</p> Source code in <code>evals/utils.py</code> <pre><code>def get_storage_carriers(networks: dict) -&gt; list[str]:\n    \"\"\"\n    Get the storage carriers from the networks.\n\n    Parameters\n    ----------\n    networks\n        The loaded networks.\n\n    Returns\n    -------\n    :\n        A list of storage carrier names.\n    \"\"\"\n    storage_carriers = set()\n    for n in networks.values():\n        for c in (\"Store\", \"StorageUnit\"):\n            storage_carriers = storage_carriers.union(n.static(c)[\"carrier\"].unique())\n\n    return sorted(storage_carriers)\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.get_trade_type","title":"<code>get_trade_type(bus_a, bus_b)</code>","text":"<p>Determine the trade type between two buses.</p> <p>Parameters:</p> Name Type Description Default <code>bus_a</code> <code>str</code> <p>1st string that should start with a region substring.</p> required <code>bus_b</code> <code>str</code> <p>2nd string that should start with a region substring.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The trade type. One of constants.TRADE_TYPES.</p> Source code in <code>evals/utils.py</code> <pre><code>def get_trade_type(bus_a: str, bus_b: str) -&gt; str:\n    \"\"\"\n    Determine the trade type between two buses.\n\n    Parameters\n    ----------\n    bus_a\n        1st string that should start with a region substring.\n    bus_b\n        2nd string that should start with a region substring.\n\n    Returns\n    -------\n    :\n        The trade type. One of constants.TRADE_TYPES.\n    \"\"\"\n    loc_a = re.findall(Regex.region, bus_a)[:1]\n    loc_b = re.findall(Regex.region, bus_b)[:1]\n    if not loc_a or not loc_b:  # no region(s) found\n        return \"\"\n    elif loc_a[0] == loc_b[0]:\n        # transformation link in same region, e.g. heat\n        return TradeTypes.LOCAL\n    elif loc_a[0][:2] == loc_b[0][:2]:  # country codes match\n        return TradeTypes.DOMESTIC\n    else:\n        return TradeTypes.FOREIGN\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.get_transmission_techs","title":"<code>get_transmission_techs(networks, bus_carrier=None)</code>","text":"<p>Get the transmission technologies from the networks.</p> <p>Parameters:</p> Name Type Description Default <code>networks</code> <code>dict</code> <p>The loaded networks.</p> required <code>bus_carrier</code> <code>str | list</code> <p>The bus carrier to filter for.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of transmission technology names.</p> Source code in <code>evals/utils.py</code> <pre><code>def get_transmission_techs(networks: dict, bus_carrier: str | list = None) -&gt; list[str]:\n    \"\"\"\n    Get the transmission technologies from the networks.\n\n    Parameters\n    ----------\n    networks\n        The loaded networks.\n    bus_carrier\n        The bus carrier to filter for.\n\n    Returns\n    -------\n    :\n        A list of transmission technology names.\n    \"\"\"\n    transmission_techs = set()\n    for n in networks.values():\n        transmission_techs = transmission_techs.union(\n            get_transmission_carriers(n, bus_carrier)\n        )\n\n    return sorted(transmission_techs)\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.get_unit","title":"<code>get_unit(s, ignore_suffix=True)</code>","text":"<p>Parse the unit from a string.</p> <p>The unit must be inside round parentheses. If multiple parenthesis are found in the input string, returns the last one.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input string that should contain a unit.</p> required <code>ignore_suffix</code> <code>bool</code> <p>Whether to strip the suffix, e.g. <code>_th</code>, <code>_el</code>, <code>_LHV</code>, ...</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>All characters inside the last pair of parenthesis without the enclosing parenthesis, or an empty string.</p> Source code in <code>evals/utils.py</code> <pre><code>def get_unit(s: str, ignore_suffix: bool = True) -&gt; str:\n    \"\"\"\n    Parse the unit from a string.\n\n    The unit must be inside round parentheses. If multiple\n    parenthesis are found in the input string, returns the last one.\n\n    Parameters\n    ----------\n    s\n        The input string that should contain a unit.\n    ignore_suffix\n        Whether to strip the suffix, e.g. `_th`, `_el`, `_LHV`, ...\n\n    Returns\n    -------\n    :\n        All characters inside the last pair of parenthesis without\n        the enclosing parenthesis, or an empty string.\n    \"\"\"\n    if matches := re.findall(Regex.unit, s):\n        unit = matches[-1].strip(\"()\")\n        if ignore_suffix and \"_\" in unit:\n            return \"_\".join(unit.split(\"_\")[:-1])\n        else:\n            return matches[-1].strip(\"()\")\n    return \"\"\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.insert_index_level","title":"<code>insert_index_level(df, value, index_name, axis=0, pos=0)</code>","text":"<p>Add an index level to the data frame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame | pandas.Series</code> <p>The data frame that will receive the new outer level index.</p> required <code>value</code> <code>str</code> <p>The new index values.</p> required <code>index_name</code> <code>str</code> <p>The new index level name.</p> required <code>axis</code> <code>optional</code> <p>The index axis. Pass 0 for row index and 1 for column index.</p> <code>0</code> <code>pos</code> <code>optional</code> <p>Move the new index name to this position. 0 is outer left, 1 is the second, and so on.</p> <code>0</code> <p>Returns:</p> Type Description <code>pandas.DataFrame | pandas.Series</code> <p>The data frame with the new index level.</p> Source code in <code>evals/utils.py</code> <pre><code>def insert_index_level(\n    df: pd.DataFrame | pd.Series,\n    value: str,\n    index_name: str,\n    axis: int = 0,\n    pos: int = 0,\n) -&gt; pd.DataFrame | pd.Series:\n    \"\"\"\n    Add an index level to the data frame.\n\n    Parameters\n    ----------\n    df\n        The data frame that will receive the new outer level index.\n    value\n        The new index values.\n    index_name\n        The new index level name.\n    axis : optional\n        The index axis. Pass 0 for row index and 1 for column index.\n    pos : optional\n        Move the new index name to this position. 0 is outer left,\n        1 is the second, and so on.\n\n    Returns\n    -------\n    :\n        The data frame with the new index level.\n    \"\"\"\n    result = pd.concat({value: df}, names=[index_name], axis=axis)\n    if pos == 0:  # no need to reorder levels. We are done inserting.\n        return result\n    idx = df.index if axis == 0 else df.columns\n    idx_names = list(idx.names)\n    idx_names.insert(pos, index_name)\n    if isinstance(result, pd.DataFrame):\n        return result.reorder_levels(idx_names, axis=axis)\n    return result.reorder_levels(idx_names)\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.operations_override","title":"<code>operations_override(networks, component, operation)</code>","text":"<p>Patch the used operations time series.</p> <p>Useful if a code block should use a different productive component series. For example, <code>p_set</code> instead of <code>p</code>.</p> <p>Parameters:</p> Name Type Description Default <code>networks</code> <code>dict</code> <p>The PyPSA network dictionary.</p> required <code>component</code> <code>str</code> <p>The component to patch, e.g. Link, Store, etc.</p> required <code>operation</code> <code>str</code> <p>The desired operations time series to use instead of 'p' or 'e'.</p> required <p>Yields:</p> Type Description <code>None</code> <p>Passes to the with statement block.</p> Source code in <code>evals/utils.py</code> <pre><code>@contextmanager\ndef operations_override(networks: dict, component: str, operation: str) -&gt; None:\n    \"\"\"\n    Patch the used operations time series.\n\n    Useful if a code block should use a different productive\n    component series. For example, `p_set` instead of `p`.\n\n    Parameters\n    ----------\n    networks\n        The PyPSA network dictionary.\n    component\n        The component to patch, e.g. Link, Store, etc.\n    operation\n        The desired operations time series to use instead of 'p' or 'e'.\n\n    Yields\n    ------\n    :\n        Passes to the with statement block.\n    \"\"\"\n    _temp_key = \"_tmp\"\n\n    for n in networks.values():\n        c = n.pnl(component)\n        c[_temp_key] = c[\"p\"]  # save a copy\n        c[\"p\"] = c[operation]  # overwrite\n\n    yield  # run anything in the with statement\n\n    for n in networks.values():\n        c = n.pnl(component)\n        c[\"p\"] = c.pop(_temp_key)  # restore original\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.prettify_number","title":"<code>prettify_number(x)</code>","text":"<p>Format a float for display on trace hover actions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The imprecise value to format.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted number as a string with 1 or 0 decimal places, depending on the magnitude of the input value.</p> Source code in <code>evals/utils.py</code> <pre><code>def prettify_number(x: float) -&gt; str:\n    \"\"\"\n    Format a float for display on trace hover actions.\n\n    Parameters\n    ----------\n    x\n        The imprecise value to format.\n\n    Returns\n    -------\n    :\n        The formatted number as a string with 1 or 0 decimal places,\n        depending on the magnitude of the input value.\n    \"\"\"\n    # if abs(round(x, 0)) &gt;= 10:\n    #     with localcontext():\n    #         return str(round(round(Decimal(x), 1), 0))\n    # else:\n    #     with localcontext() as ctx:\n    #         ctx.rounding = ROUND_HALF_UP\n    #         return str(round(round(Decimal(x), 2), 1))\n    #\n    if abs(x) &gt;= 10:\n        return f\"{int(round(x, 0)):d}\"\n    else:\n        return f\"{round(x, 1):.1f}\"\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.print_link_bus_efficiencies","title":"<code>print_link_bus_efficiencies(networks, year, like)</code>","text":"<p>Debugging utility function to review Link branches.</p> <p>Parameters:</p> Name Type Description Default <code>networks</code> <p>The loaded networks.</p> required <code>year</code> <p>The year to print the Link branches for.</p> required <code>like</code> <p>A regex to filter the Link index.</p> required <p>Returns:</p> Type Description <code>pandas.Series</code> <p>A pandas Series with the first Link filter result.</p> Source code in <code>evals/utils.py</code> <pre><code>def print_link_bus_efficiencies(networks, year, like) -&gt; pd.Series:\n    \"\"\"\n    Debugging utility function to review Link branches.\n\n    Parameters\n    ----------\n    networks\n        The loaded networks.\n    year\n        The year to print the Link branches for.\n    like\n        A regex to filter the Link index.\n\n    Returns\n    -------\n    :\n        A pandas Series with the first Link filter result.\n    \"\"\"\n    return (\n        networks[year]\n        .static(\"Link\")\n        .filter(like=like, axis=0)\n        .filter(regex=\"bus|eff\")\n        .iloc[0, :]\n        .T.sort_index()\n    )\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.rename_aggregate","title":"<code>rename_aggregate(df, mapper, level=DataModel.CARRIER, agg='sum')</code>","text":"<p>Rename index values and aggregate duplicates.</p> <p>In case the supplied mapper is a string, all values in the supplied level are replaced by this string.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame | pandas.Series</code> <p>The input data frame.</p> required <code>mapper</code> <code>dict | str</code> <p>A Dictionary with key-value pairs to rename index values, or a string used to replace all values in the given level.</p> required <code>level</code> <code>str</code> <p>The index level name.</p> <code>evals.constants.DataModel.CARRIER</code> <code>agg</code> <code>str</code> <p>The aggregation method for duplicated index values after renaming.</p> <code>'sum'</code> <p>Returns:</p> Type Description <code>pandas.Series | pandas.DataFrame</code> <p>A data frame with renamed index values and aggregated values.</p> Notes <p>Support for column axis mapping was removed, because the groupby operation along axis=1 removes column level names and does not work correctly.</p> Source code in <code>evals/utils.py</code> <pre><code>def rename_aggregate(\n    df: pd.DataFrame | pd.Series,\n    mapper: dict | str,\n    level: str = DataModel.CARRIER,\n    agg: str = \"sum\",\n) -&gt; pd.Series | pd.DataFrame:\n    \"\"\"\n    Rename index values and aggregate duplicates.\n\n    In case the supplied mapper is a string, all values in the\n    supplied level are replaced by this string.\n\n    Parameters\n    ----------\n    df\n        The input data frame.\n    mapper\n        A Dictionary with key-value pairs to rename index values, or\n        a string used to replace all values in the given level.\n    level\n        The index level name.\n    agg\n        The aggregation method for duplicated index values after\n        renaming.\n\n    Returns\n    -------\n    :\n        A data frame with renamed index values and aggregated values.\n\n    Notes\n    -----\n    Support for column axis mapping was removed, because the groupby\n    operation along axis=1 removes column level names and does not\n    work correctly.\n    \"\"\"\n    if isinstance(mapper, str):\n        mapper = dict.fromkeys(df.index.unique(level=level), mapper)\n    renamed = df.rename(mapper, level=level)\n    return renamed.groupby(df.index.names).agg(agg)\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.scale","title":"<code>scale(df, to_unit)</code>","text":"<p>Scale metric values to the specified target unit.</p> <p>Multiplies all columns in the metric by a scaling factor. The scaling factor is calculated from the unit in the data frame columns and the given target unit. Also updates the unit names encoded in the data frame columns for time aggregated metrics.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The input data frame with valid units in the column labels.</p> required <code>to_unit</code> <code>str</code> <p>The target unit. See constants.UNITS for possible units.</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The scaled data frame with replaced units in column labels.</p> <p>Raises:</p> Type Description <code>raises KeyError</code> <p>If the 'to_unit' is not found in UNITS, or if the attrs dictionary has no unit field.</p> <code>raises ValueError</code> <p>If input units are inconsistent, i.e. mixed power and energy columns.</p> Source code in <code>evals/utils.py</code> <pre><code>def scale(df: pd.DataFrame, to_unit: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Scale metric values to the specified target unit.\n\n    Multiplies all columns in the metric by a scaling factor.\n    The scaling factor is calculated from the unit in the data frame\n    columns and the given target unit. Also updates the unit\n    names encoded in the data frame columns for time aggregated\n    metrics.\n\n    Parameters\n    ----------\n    df\n        The input data frame with valid units in the column labels.\n    to_unit\n        The target unit. See constants.UNITS for possible\n        units.\n\n    Returns\n    -------\n    :\n        The scaled data frame with replaced units in column labels.\n\n    Raises\n    ------\n    raises KeyError\n        If the 'to_unit' is not found in UNITS, or if the attrs\n        dictionary has no unit field.\n    raises ValueError\n        If input units are inconsistent, i.e. mixed power and energy\n        columns.\n    \"\"\"\n    suffix = \"\"\n    if to_unit.endswith((\"_LHV\", \"_th\", \"_el\")):\n        to_unit, suffix = to_unit.split(\"_\")\n\n    if df.columns.name == DataModel.SNAPSHOTS:\n        is_unit = df.attrs[\"unit\"]\n        scaling_factor = is_unit / to_unit\n        result = df.mul(scaling_factor)\n    else:\n        scale_to = to_unit if isinstance(to_unit, float) else UNITS[to_unit]\n        units_in = list(map(get_unit, df.columns))\n        if to_unit.endswith(\"h\") and not all(u.endswith(\"h\") for u in units_in):\n            raise ValueError(\"Denying to convert units from power to energy.\")\n        if to_unit.endswith(\"W\") and not all(u.endswith(\"W\") for u in units_in):\n            raise ValueError(\"Denying to convert unit from energy to power.\")\n        scale_in = [UNITS[s] for s in units_in]\n        scaling_factors = [x / scale_to for x in scale_in]\n\n        result = df.mul(scaling_factors, axis=1)\n        result.columns = result.columns.str.replace(\n            \"|\".join(units_in), to_unit, regex=True\n        )\n\n    if suffix:\n        result.attrs[\"unit\"] = f\"{to_unit}_{suffix}\"\n    else:\n        result.attrs[\"unit\"] = to_unit\n\n    return result\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.split_location_carrier","title":"<code>split_location_carrier(index, names)</code>","text":"<p>Split location and carrier in the index.</p> <p>The location must be encoded in the string and match the regex '^[A-Z]{2}\\d\\s\\d'. Subsequent characters become the carrier name. The location defaults to an emtpy string if the regex does not match.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>pandas.MultiIndex</code> <p>A pandas Multiindex with the innermost level to split.</p> required <code>names</code> <code>list</code> <p>The list of output Multiindex names.</p> required <p>Returns:</p> Type Description <code>pandas.MultiIndex</code> <p>The resulting Multiindex with one additional level due to the splitting.</p> Source code in <code>evals/utils.py</code> <pre><code>def split_location_carrier(index: pd.MultiIndex, names: list) -&gt; pd.MultiIndex:\n    r\"\"\"\n    Split location and carrier in the index.\n\n    The location must be encoded in the string and match the regex\n    '^[A-Z]{2}\\\\d\\\\s\\\\d'. Subsequent characters become the carrier\n    name. The location defaults to an emtpy string if the regex\n    does not match.\n\n    Parameters\n    ----------\n    index\n        A pandas Multiindex with the innermost level to split.\n    names\n        The list of output Multiindex names.\n\n    Returns\n    -------\n    :\n        The resulting Multiindex with one additional\n        level due to the splitting.\n    \"\"\"\n    idx_split = []\n    for *prefixes, loc_category in index:\n        matches = re.match(Regex.region, loc_category)\n        location = matches.group().strip() if matches else \"\"\n        technology = loc_category.removeprefix(location).strip()\n        idx_split.append((*prefixes, location, technology))\n\n    return pd.MultiIndex.from_tuples(idx_split, names=names)\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.split_urban_central_heat_losses_and_consumption","title":"<code>split_urban_central_heat_losses_and_consumption(df, heat_loss)</code>","text":"<p>Split urban heat amounts by a heat loss factor.</p> <p>Amounts for urban central heat contain distribution losses. However, the evaluation shows final demands in the results. Therefore, heat network distribution losses need to be separated from the total amounts because grid distribution losses do not arrive at the metering endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame | pandas.Series</code> <p>The input data frame with values for urban central heat technologies.</p> required <code>heat_loss</code> <code>int</code> <p>The heat loss factor from the configuration file.</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The data frame with split heat amounts for end user demand (urban dentral heat), distribution grid losses (urban dentral heat losses) and anything else from the input data frame (not urban central heat).</p> Source code in <code>evals/utils.py</code> <pre><code>def split_urban_central_heat_losses_and_consumption(\n    df: pd.DataFrame | pd.Series, heat_loss: int\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Split urban heat amounts by a heat loss factor.\n\n    Amounts for urban central heat contain distribution losses.\n    However, the evaluation shows final demands\n    in the results. Therefore, heat network distribution losses need\n    to be separated from the total amounts because grid distribution\n    losses do not arrive at the metering endpoint.\n\n    Parameters\n    ----------\n    df\n        The input data frame with values for urban central heat\n        technologies.\n    heat_loss\n        The heat loss factor from the configuration file.\n\n    Returns\n    -------\n    :\n        The data frame with split heat amounts for end user demand\n        (urban dentral heat), distribution grid losses (urban dentral\n        heat losses) and anything else from the input data frame\n        (not urban central heat).\n    \"\"\"\n    loss_factor = heat_loss / (1 + heat_loss)\n    urban_heat_bus_carrier = [BusCarrier.HEAT_URBAN_CENTRAL]\n\n    urban_heat = filter_by(df, bus_carrier=urban_heat_bus_carrier)\n    rest = filter_by(df, bus_carrier=urban_heat_bus_carrier, exclude=True)\n    consumption = urban_heat.mul(1 - loss_factor)\n    losses = urban_heat.mul(loss_factor)\n    losses_mapper = dict.fromkeys(urban_heat_bus_carrier, Carrier.grid_losses)\n    losses = losses.rename(losses_mapper, level=DataModel.CARRIER)\n\n    return pd.concat([rest, consumption, losses]).sort_index()\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.trade_mask","title":"<code>trade_mask(comp, scopes, buses=('bus0', 'bus1'))</code>","text":"<p>Get the mask for a given trade type.</p> <p>The logic only compares bus0 and bus1 in a given component.</p> <p>Parameters:</p> Name Type Description Default <code>comp</code> <code>pandas.DataFrame</code> <p>The component data frame. Should be one a branch_component, i.e. 'Line', 'Link', or 'Transformer'.</p> required <code>scopes</code> <code>str | tuple</code> <p>The trade scope(s) to match. One or multiple of 'local', 'domestic', 'foreign'.</p> required <code>buses</code> <code>tuple</code> <p>Two buses to determine the trade type from. The trade type will be 'local', 'domestic', or 'foreign', for same location, same country code, or different country code, respectively.</p> <code>('bus0', 'bus1')</code> <p>Returns:</p> Type Description <code>pandas.Series</code> <p>A pandas Series with the same index as component index and 1 or 0 as values for match or differ, respectively.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>In case the passed trade type is not supported and to prevent unintended string matches.</p> Source code in <code>evals/utils.py</code> <pre><code>def trade_mask(\n    comp: pd.DataFrame, scopes: str | tuple, buses: tuple = (\"bus0\", \"bus1\")\n) -&gt; pd.Series:\n    \"\"\"\n    Get the mask for a given trade type.\n\n    The logic only compares bus0 and bus1 in a given component.\n\n    Parameters\n    ----------\n    comp\n        The component data frame. Should be one a branch_component,\n        i.e. 'Line', 'Link', or 'Transformer'.\n    scopes\n        The trade scope(s) to match. One or multiple of 'local',\n        'domestic', 'foreign'.\n    buses\n        Two buses to determine the trade type from. The trade type will\n        be 'local', 'domestic', or 'foreign', for same location, same\n        country code, or different country code, respectively.\n\n    Returns\n    -------\n    :\n        A pandas Series with the same index as component index and 1\n        or 0 as values for match or differ, respectively.\n\n    Raises\n    ------\n    ValueError\n        In case the passed trade type is not supported and to prevent\n        unintended string matches.\n    \"\"\"\n    scopes = (scopes,) if isinstance(scopes, str) else scopes\n    if unknown_scopes := set(scopes).difference(\n        {TradeTypes.LOCAL, TradeTypes.DOMESTIC, TradeTypes.FOREIGN}\n    ):\n        raise ValueError(f\"Invalid trade scopes detected: {unknown_scopes}.\")\n    df = comp[[*buses]]\n    trade = df.apply(lambda row: get_trade_type(row[buses[0]], row[buses[1]]), axis=1)\n    return trade.isin(scopes)\n</code></pre>"},{"location":"reference/evals/utils/#evals.utils.verify_metric_format","title":"<code>verify_metric_format(metric)</code>","text":"<p>Ensure correct metric format.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>pandas.DataFrame</code> <p>The metric data frame. This format is supported by export functions.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the metric does not comply with the data model.</p> Source code in <code>evals/utils.py</code> <pre><code>def verify_metric_format(metric: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Ensure correct metric format.\n\n    Parameters\n    ----------\n    metric\n        The metric data frame. This format is supported by export\n        functions.\n\n    Raises\n    ------\n    AssertionError\n        If the metric does not comply with the data model.\n    \"\"\"\n    assert isinstance(metric, pd.DataFrame), (\n        f\"Metric must be a DataFrame, but {type(metric)} was passed.\"\n    )\n    assert set(metric.index.names).issubset(set(DataModel.YEAR_IDX_NAMES)), (\n        f\"Metric index levels must contain {DataModel.YEAR_IDX_NAMES}, \"\n        f\"but {metric.index.names} is set.\"\n    )\n    assert metric.columns.names in ([DataModel.METRIC], [DataModel.SNAPSHOTS]), (\n        f\"Metric column level names must be [{DataModel.METRIC}] or \"\n        f\"[{DataModel.SNAPSHOTS}], but {metric.columns.names} is set.\"\n    )\n\n    assert metric.attrs.get(\"name\"), \"Must set the metric name in 'metric.attrs'.\"\n    assert metric.attrs.get(\"unit\"), \"Must set the metric unit in 'metric.attrs'.\"\n\n    if metric.columns.names == [DataModel.METRIC]:\n        assert all(\"(\" in c and \")\" in c for c in metric.columns), (\n            f\"All columns must have a unit in braces: {metric.columns}\"\n        )\n\n        assert len(metric.columns) == 1, \"Multiple aggregated metrics are not allowed.\"\n\n    elif metric.columns.name == DataModel.SNAPSHOTS:\n        assert isinstance(metric.columns, pd.DatetimeIndex), (\n            \"Snapshot columns must be of type DatetimeIndex.\"\n        )\n</code></pre>"},{"location":"reference/evals/plots/","title":"Index","text":""},{"location":"reference/evals/plots/#evals.plots.ESMBarChart","title":"<code>ESMBarChart</code>","text":"<p>               Bases: <code>evals.plots._base.ESMChart</code></p> <p>The ESM Bar Chart exports metrics as plotly HTML file.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>object</code> <p>Positional arguments of the base class.</p> <code>()</code> <code>**kwargs</code> <code>object</code> <p>Key word arguments of the base class.</p> <code>{}</code> Source code in <code>evals/plots/barchart.py</code> <pre><code>class ESMBarChart(ESMChart):\n    \"\"\"\n    The ESM Bar Chart exports metrics as plotly HTML file.\n\n    Parameters\n    ----------\n    *args\n        Positional arguments of the base class.\n\n    **kwargs\n        Key word arguments of the base class.\n    \"\"\"\n\n    def __init__(self, *args: object, **kwargs: object) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.fig = go.Figure()\n\n        self.location = self._df.index.unique(DataModel.LOCATION)[0]\n        self.col_values = self._df.columns[0]\n\n    @cached_property\n    def barmode(self) -&gt; str:\n        \"\"\"\n        Determine the barmode for the bar chart.\n\n        Returns\n        -------\n        :\n            The barmode for the bar chart, either \"relative\" if there\n            are both negative and positive values, or \"stack\" if not.\n        \"\"\"\n        has_negatives = self._df.lt(0).to_numpy().any()\n        has_positives = self._df.ge(0).to_numpy().any()\n        return \"relative\" if has_negatives and has_positives else \"stack\"\n\n    @cached_property\n    def df(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Plot data formatted for bar charts.\n\n        Returns\n        -------\n        :\n            The formatted data for creating bar charts.\n        \"\"\"\n        df = apply_cutoff(self._df, limit=self.cfg.cutoff, drop=self.cfg.cutoff_drop)\n\n        df = self.custom_sort(\n            df.reset_index(),\n            by=self.cfg.plot_category,\n            values=self.cfg.category_orders,\n            ascending=True,\n        )\n        df[\"display_value\"] = df[self.col_values].apply(prettify_number)\n\n        return df\n\n    def plot(self) -&gt; None:\n        \"\"\"Create the bar chart.\"\"\"\n        title = self.cfg.title.format(location=self.location, unit=self.unit)\n\n        if self.empty_input or self.df[self.col_values].isna().all():\n            self.fig = empty_figure(title)\n            return\n\n        pattern = {\n            col: self.cfg.pattern.get(col, \"\")\n            for col in self.df[self.cfg.plot_category].unique()\n        }\n        self.fig = px.bar(\n            self.df,\n            color_discrete_map=self.cfg.colors,\n            pattern_shape=self.cfg.plot_category,\n            pattern_shape_map=pattern,\n            barmode=self.barmode,\n            x=self.cfg.plot_xaxis,\n            y=self.col_values,\n            color=self.cfg.plot_category,\n            text=self.col_values,\n            title=title,\n            labels={\n                self.col_values: \"&lt;b&gt;\" + self.unit + \"&lt;/b&gt;\",\n                self.cfg.plot_category: self.cfg.legend_header,\n            },\n            custom_data=[self.cfg.plot_category, \"display_value\"],\n        )\n\n        self._set_base_layout()\n        self._style_bars()\n        self._style_title_and_legend_and_xaxis_label()\n        self._append_footnotes()\n        self.fig.for_each_trace(self._set_legend_rank, selector={\"type\": \"bar\"})\n\n        # add total sum labels at the end of the bar(s)\n        if self.barmode == \"relative\":\n            self.fig.add_hline(y=0)  # visual separator between supply and withdrawal\n            self._add_total_sum_trace(\"Lower Sum\", orientation=\"down\")\n            self._add_total_sum_trace(\"Upper Sum\", orientation=\"up\")\n        else:\n            self._add_total_sum_trace(\"Sum\")\n\n    def _add_total_sum_trace(\n        self,\n        name_trace: str,\n        orientation: str = None,\n    ) -&gt; None:\n        \"\"\"\n        Create a scatter trace for total sum labels.\n\n        The label will show the total sum as text at the end of the bar\n        trace. The label will not be part of the legend.\n\n        Parameters\n        ----------\n        name_trace\n            The name of the trace useful to identify the trace in the\n            JSON representation.\n        orientation : optional, {'up', 'down', None}\n            The orientation of the trace used to choose the\n            text position and the sign of the values.\n        \"\"\"\n        sign = 1\n        if orientation == \"up\":\n            values = self.df[self.df[self.col_values].gt(0)]\n        elif orientation == \"down\":\n            sign = -1\n            values = self.df[self.df[self.col_values].le(0)]\n        else:  # barmode = stacked\n            values = self.df\n\n        totals = values.groupby(self.cfg.plot_xaxis).sum(numeric_only=True)\n        totals[\"display_value\"] = totals[self.col_values].apply(prettify_number)\n        y_offset = totals[self.col_values].abs().max() / 100 * sign\n\n        scatter = go.Scatter(\n            x=totals.index,\n            y=totals[self.col_values] + y_offset,\n            text=totals[\"display_value\"],\n            texttemplate=\"&lt;b&gt; %{text} \" + self.unit + \"&lt;/b&gt;\",\n            mode=\"text\",\n            textposition=f\"{'bottom' if orientation == 'down' else 'top'} center\",\n            showlegend=False,\n            name=name_trace,\n            textfont={\"size\": 18},\n            hoverinfo=\"skip\",\n        )\n\n        self.fig.add_trace(scatter)\n\n    def _style_bars(self) -&gt; None:\n        \"\"\"Update bar trace styles.\"\"\"\n        self.fig.update_traces(\n            selector={\"type\": \"bar\"},\n            width=0.6,\n            textposition=\"inside\",\n            insidetextanchor=\"middle\",\n            texttemplate=\"&lt;b&gt;%{customdata[1]}&lt;/b&gt;\",\n            insidetextfont={\"size\": 16},\n            textangle=0,\n            hovertemplate=\"%{customdata[0]}: %{customdata[1]} \" + self.unit,\n            hoverlabel={\"namelength\": 0},\n        )\n\n    def _set_legend_rank(self, trace: go.Bar) -&gt; go.Bar:\n        \"\"\"\n        Set the legendrank attribute for bar traces.\n\n        Only traces listed in the category_order\n        configuration item are considered.\n\n        Parameters\n        ----------\n        trace\n            The trace object to set the legendrank for.\n\n        Returns\n        -------\n        :\n            The updated bar trace, or the original bar trace.\n        \"\"\"\n        if trace[\"name\"] in self.cfg.category_orders:\n            y = trace[\"y\"]  # need to drop nan and inf or the sum may return NaN\n            trace_sum = y[np.isfinite(y)].sum()\n            sign = -1 if trace_sum &lt; 0 else 1\n            pos = self.cfg.category_orders.index(trace[\"name\"])\n            trace = trace.update(legendrank=1000 + pos * sign)  # 1000 = plotly default\n        return trace\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.ESMBarChart.barmode","title":"<code>barmode</code>  <code>cached</code> <code>property</code>","text":"<p>Determine the barmode for the bar chart.</p> <p>Returns:</p> Type Description <code>str</code> <p>The barmode for the bar chart, either \"relative\" if there are both negative and positive values, or \"stack\" if not.</p>"},{"location":"reference/evals/plots/#evals.plots.ESMBarChart.df","title":"<code>df</code>  <code>cached</code> <code>property</code>","text":"<p>Plot data formatted for bar charts.</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The formatted data for creating bar charts.</p>"},{"location":"reference/evals/plots/#evals.plots.ESMBarChart.plot","title":"<code>plot()</code>","text":"<p>Create the bar chart.</p> Source code in <code>evals/plots/barchart.py</code> <pre><code>def plot(self) -&gt; None:\n    \"\"\"Create the bar chart.\"\"\"\n    title = self.cfg.title.format(location=self.location, unit=self.unit)\n\n    if self.empty_input or self.df[self.col_values].isna().all():\n        self.fig = empty_figure(title)\n        return\n\n    pattern = {\n        col: self.cfg.pattern.get(col, \"\")\n        for col in self.df[self.cfg.plot_category].unique()\n    }\n    self.fig = px.bar(\n        self.df,\n        color_discrete_map=self.cfg.colors,\n        pattern_shape=self.cfg.plot_category,\n        pattern_shape_map=pattern,\n        barmode=self.barmode,\n        x=self.cfg.plot_xaxis,\n        y=self.col_values,\n        color=self.cfg.plot_category,\n        text=self.col_values,\n        title=title,\n        labels={\n            self.col_values: \"&lt;b&gt;\" + self.unit + \"&lt;/b&gt;\",\n            self.cfg.plot_category: self.cfg.legend_header,\n        },\n        custom_data=[self.cfg.plot_category, \"display_value\"],\n    )\n\n    self._set_base_layout()\n    self._style_bars()\n    self._style_title_and_legend_and_xaxis_label()\n    self._append_footnotes()\n    self.fig.for_each_trace(self._set_legend_rank, selector={\"type\": \"bar\"})\n\n    # add total sum labels at the end of the bar(s)\n    if self.barmode == \"relative\":\n        self.fig.add_hline(y=0)  # visual separator between supply and withdrawal\n        self._add_total_sum_trace(\"Lower Sum\", orientation=\"down\")\n        self._add_total_sum_trace(\"Upper Sum\", orientation=\"up\")\n    else:\n        self._add_total_sum_trace(\"Sum\")\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.ESMGroupedBarChart","title":"<code>ESMGroupedBarChart</code>","text":"<p>               Bases: <code>evals.plots._base.ESMChart</code></p> <p>A class that produces multiple bar charts in subplots.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>Positional arguments of the base class.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Key word arguments of the base class.</p> <code>{}</code> Source code in <code>evals/plots/facetbars.py</code> <pre><code>class ESMGroupedBarChart(ESMChart):\n    \"\"\"\n    A class that produces multiple bar charts in subplots.\n\n    Parameters\n    ----------\n    *args\n        Positional arguments of the base class.\n\n    **kwargs\n        Key word arguments of the base class.\n    \"\"\"\n\n    def __init__(self, *args: tuple, **kwargs: dict) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.location = self._df.index.unique(DataModel.LOCATION)[0]\n        self.col_values = self._df.columns[0]\n\n        # self.df is accessed below. location and col_values must be\n        # set before the first access to the df property.\n        ncols = len(self.df[DataModel.BUS_CARRIER].unique())\n        column_widths = [0.85 / ncols] * ncols\n        self.fig = make_subplots(\n            rows=1, cols=ncols, shared_yaxes=True, column_widths=column_widths\n        )\n\n    @cached_property\n    def df(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Plot data formatted for grouped bar charts.\n\n        Returns\n        -------\n        :\n            The formatted data for creating bar charts.\n        \"\"\"\n        df = apply_cutoff(self._df, limit=self.cfg.cutoff, drop=False)\n        df = df.reset_index()\n\n        # need to add missing carrier for every sector to prevent\n        # broken sort order. If a carrier is missing in, lets say the\n        # first subplot, the sort order will be different (compared)\n        # to a sector that has all carriers. To prevent this, we add\n        # missing dummy carriers before sorting.\n        fill_values = product(\n            df[DataModel.YEAR].unique(),\n            df[DataModel.LOCATION].unique(),\n            df[DataModel.CARRIER].unique(),\n            df[DataModel.BUS_CARRIER].unique(),\n        )\n        df_fill = pd.DataFrame(columns=DataModel.YEAR_IDX_NAMES, data=fill_values)\n        df_fill[self.col_values] = np.nan\n        df = pd.concat([df, df_fill], ignore_index=True)\n\n        # sort every sector in alphabetical order and separately to\n        # correctly align sectors and stacked carrier traces in bars\n        df_list = []\n        for _, df_sector in df.groupby(self.cfg.facet_column, sort=True):\n            sorted_sector = self.custom_sort(\n                df_sector,\n                by=self.cfg.plot_category,\n                values=self.cfg.category_orders,\n                ascending=True,\n            )\n            df_list.append(sorted_sector)\n        df = pd.concat(df_list)\n        # df = df.groupby(self.cfg.facet_column, sort=True).apply(\n        #     self.custom_sort,\n        #     by=self.cfg.plot_category,\n        #     values=self.cfg.category_orders,\n        #     ascending=True,\n        #     # include_groups=False,\n        # )\n        # remove NaN categories again after sorting with all categories\n        df = df.dropna(how=\"all\", subset=self.col_values)\n        df[\"display_value\"] = df[self.col_values].apply(prettify_number)\n\n        return df\n\n    def plot(self) -&gt; None:\n        \"\"\"Create the bar chart.\"\"\"\n        title = self.cfg.title.format(location=self.location, unit=self.unit)\n        if self.empty_input or self.df[self.col_values].isna().all():\n            self.fig = empty_figure(title)\n            return\n\n        pattern = {\n            col: self.cfg.pattern.get(col, \"\")\n            for col in self.df[self.cfg.plot_category].unique()\n        }\n\n        self.fig = px.bar(\n            self.df,\n            x=self.cfg.plot_xaxis,\n            y=self.col_values,\n            facet_col=self.cfg.facet_column,\n            facet_col_spacing=0.04,\n            pattern_shape=self.cfg.plot_category,\n            pattern_shape_map=pattern,\n            color=self.cfg.plot_category,\n            color_discrete_map=self.cfg.colors,\n            text=self.cfg.facet_column,  # needed to rename xaxis and dropped afterward\n            title=title,\n            custom_data=[self.cfg.plot_category, \"display_value\"],\n        )\n\n        self.fig.for_each_xaxis(self._rename_xaxis)\n        self.fig.for_each_xaxis(self._add_total_sum_subplot_traces)\n        self.fig.update_annotations(text=\"\")  # remove text labels\n\n        self._set_base_layout()\n        self._style_grouped_bars()\n        self._style_title_and_legend_and_xaxis_label()\n        self._append_footnotes()\n        self.fig.for_each_xaxis(self._style_inner_xaxis_labels)\n\n    def _rename_xaxis(self, xaxis: go.XAxis) -&gt; None:\n        \"\"\"\n        Update the xaxis labels.\n\n        The function iterates over subplot columns and looks for the\n        sector name in the figure data where the xaxis index matches\n        and updates the xaxis label and removes the upper text.\n\n        Parameters\n        ----------\n        xaxis\n            The subplot xaxis (a dictionary).\n\n        Notes\n        -----\n        A better way to set the xaxis labels is desirable. However, I\n        could not find a way to replace the 'year' string using the\n        'label' argument in plotly.express.bar(), because all columns\n        are named the same (='year') and the label argument only\n        maps old to new names in a dictionary.\n        \"\"\"\n        layout = self.fig[\"layout\"]\n        idx = xaxis[\"anchor\"].lstrip(\"y\")\n        for data in self.fig[\"data\"]:\n            if data[\"xaxis\"] == f\"x{idx}\":\n                sector = data[\"text\"][0]\n                layout[f\"xaxis{idx}\"][\"title\"][\"text\"] = f\"&lt;b&gt;{sector}\"\n                break\n\n    def _style_inner_xaxis_labels(self, xaxis: go.XAxis) -&gt; None:\n        \"\"\"\n        Set the font size for the inner xaxis labels.\n\n        Parameters\n        ----------\n        xaxis\n            The subplot xaxis (a dictionary-like object).\n        \"\"\"\n        xaxis.update(\n            tickfont_size=self.cfg.xaxis_font_size, categoryorder=\"category ascending\"\n        )\n\n    def _style_grouped_bars(self) -&gt; None:\n        \"\"\"Style bar traces for grouped bar charts.\"\"\"\n        self.fig.update_traces(\n            selector={\"type\": \"bar\"},\n            width=0.8,\n            textposition=\"inside\",\n            insidetextanchor=\"middle\",\n            texttemplate=\"&lt;b&gt;%{customdata[1]}&lt;/b&gt;\",\n            textangle=0,\n            insidetextfont={\"size\": 16},\n            hovertemplate=\"%{customdata[0]}: %{customdata[1]} \" + self.unit,\n            hoverlabel={\"namelength\": 0},\n        )\n\n    def _add_total_sum_subplot_traces(self, xaxis: go.XAxis) -&gt; None:\n        \"\"\"\n        Add traces for total sum labels in every subplot.\n\n        The Xaxis is needed to parse the subplot position dynamically.\n        The method adds text annotations with the total amount of\n        energy per stacked bar.\n\n        Parameters\n        ----------\n        xaxis\n            The subplot xaxis (a dictionary-like object).\n        \"\"\"\n        idx = xaxis[\"anchor\"].lstrip(\"y\")\n        sector = xaxis[\"title\"][\"text\"].lstrip(\"&lt;b&gt;\")\n        values = self.df.query(f\"{self.cfg.facet_column} == '{sector}'\").copy()\n\n        values[\"pos\"] = values[self.col_values].where(values[self.col_values].gt(0))\n        values[\"neg\"] = values[self.col_values].where(values[self.col_values].le(0))\n\n        totals = values.groupby(self.cfg.plot_xaxis).sum(numeric_only=True)\n        totals[\"pos_display\"] = totals[\"pos\"].apply(prettify_number)\n        totals[\"neg_display\"] = totals[\"neg\"].apply(prettify_number)\n\n        if totals[\"pos\"].sum() &gt; 0:\n            scatter = go.Scatter(\n                x=totals.index,\n                y=totals[\"pos\"] + totals[\"pos\"].abs().max() / 100,\n                text=totals[\"pos_display\"],\n                texttemplate=\"&lt;b&gt;%{text}&lt;/b&gt;\",\n                mode=\"text\",\n                textposition=\"top center\",\n                showlegend=False,\n                name=\"Sum\",\n                textfont={\"size\": 18},\n                hoverinfo=\"skip\",\n            )\n\n            self.fig.add_trace(scatter, col=int(idx) if idx else 1, row=1)\n\n        if totals[\"neg\"].sum() &lt; 0:\n            scatter = go.Scatter(\n                x=totals.index,\n                y=totals[\"neg\"] - totals[\"neg\"].abs().max() / 100,\n                text=totals[\"neg_display\"],\n                texttemplate=\"&lt;b&gt;%{text}&lt;/b&gt;\",\n                mode=\"text\",\n                textposition=\"bottom center\",\n                showlegend=False,\n                name=\"Sum\",\n                textfont={\"size\": 18},\n                hoverinfo=\"skip\",\n            )\n            self.fig.add_trace(scatter, col=int(idx) if idx else 1, row=1)\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.ESMGroupedBarChart.df","title":"<code>df</code>  <code>cached</code> <code>property</code>","text":"<p>Plot data formatted for grouped bar charts.</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The formatted data for creating bar charts.</p>"},{"location":"reference/evals/plots/#evals.plots.ESMGroupedBarChart.plot","title":"<code>plot()</code>","text":"<p>Create the bar chart.</p> Source code in <code>evals/plots/facetbars.py</code> <pre><code>def plot(self) -&gt; None:\n    \"\"\"Create the bar chart.\"\"\"\n    title = self.cfg.title.format(location=self.location, unit=self.unit)\n    if self.empty_input or self.df[self.col_values].isna().all():\n        self.fig = empty_figure(title)\n        return\n\n    pattern = {\n        col: self.cfg.pattern.get(col, \"\")\n        for col in self.df[self.cfg.plot_category].unique()\n    }\n\n    self.fig = px.bar(\n        self.df,\n        x=self.cfg.plot_xaxis,\n        y=self.col_values,\n        facet_col=self.cfg.facet_column,\n        facet_col_spacing=0.04,\n        pattern_shape=self.cfg.plot_category,\n        pattern_shape_map=pattern,\n        color=self.cfg.plot_category,\n        color_discrete_map=self.cfg.colors,\n        text=self.cfg.facet_column,  # needed to rename xaxis and dropped afterward\n        title=title,\n        custom_data=[self.cfg.plot_category, \"display_value\"],\n    )\n\n    self.fig.for_each_xaxis(self._rename_xaxis)\n    self.fig.for_each_xaxis(self._add_total_sum_subplot_traces)\n    self.fig.update_annotations(text=\"\")  # remove text labels\n\n    self._set_base_layout()\n    self._style_grouped_bars()\n    self._style_title_and_legend_and_xaxis_label()\n    self._append_footnotes()\n    self.fig.for_each_xaxis(self._style_inner_xaxis_labels)\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.ESMTimeSeriesChart","title":"<code>ESMTimeSeriesChart</code>","text":"<p>               Bases: <code>evals.plots._base.ESMChart</code></p> <p>A class that produces one time series chart.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>Positional arguments of the base class.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Key word arguments of the base class.</p> <code>{}</code> Source code in <code>evals/plots/timeseries.py</code> <pre><code>class ESMTimeSeriesChart(ESMChart):\n    \"\"\"\n    A class that produces one time series chart.\n\n    Parameters\n    ----------\n    *args\n        Positional arguments of the base class.\n\n    **kwargs\n        Key word arguments of the base class.\n    \"\"\"\n\n    def __init__(self, *args: tuple, **kwargs: dict) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.fig = go.Figure()\n        self.year = self._df.index.unique(\"year\")[0]\n        self.yaxes_showgrid = self.yaxes_visible = True\n        self.location = self._df.index.unique(DataModel.LOCATION)[0]\n\n    @cached_property\n    def df(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Plot data formatted for time series charts.\n\n        Returns\n        -------\n        :\n            The formatted data for creating bar charts.\n        \"\"\"\n        df = apply_cutoff(self._df, limit=self.cfg.cutoff, drop=self.cfg.cutoff_drop)\n        df = self.custom_sort(\n            df, by=self.cfg.plot_category, values=self.cfg.category_orders\n        )\n        df = self.fix_snapshots(df, int(self.year))\n        df = df.droplevel([DataModel.YEAR, DataModel.LOCATION])\n\n        return df.T  # transpose to iterate column wise over categories\n\n    def plot(self) -&gt; None:\n        \"\"\"\n        Plot the data to the chart.\n\n        This function iterates over the data series, adds traces to the\n        figure, styles the inflexible demand, sets the layout, styles\n        the title, legend, x-axis label, time series axes, and appends\n        footnotes.\n        \"\"\"\n        title = self.cfg.title.format(\n            location=self.location, year=self.year, unit=self.unit\n        )\n        if self.empty_input:\n            self.fig = empty_figure(title)\n            return\n\n        stackgroup = None\n        for i, (name, series) in enumerate(self.df.items()):\n            if self.cfg.stacked:\n                stackgroup = \"supply\" if series.sum() &gt;= 0 else \"withdrawal\"\n            legendrank = 1000 + i if stackgroup == \"supply\" else 1000 - i\n            self.fig.add_trace(\n                go.Scatter(\n                    x=series.index,\n                    y=series.values,\n                    hovertemplate=\"%{y:.2f} \" + self.unit,\n                    name=name,\n                    fill=self.cfg.fill.get(name, \"tonexty\"),\n                    fillpattern_shape=self.cfg.pattern.get(name),\n                    line_dash=self.cfg.line_dash.get(name, \"solid\"),\n                    line_width=self.cfg.line_width.get(name, 1),\n                    line_color=self.cfg.colors.get(name),\n                    line_shape=self.cfg.line_shape,\n                    fillcolor=self.cfg.colors.get(name),\n                    stackgroup=stackgroup,\n                    legendrank=legendrank,\n                )\n            )\n\n        self._style_inflexible_demand()\n        self._set_base_layout()\n        self._style_title_and_legend_and_xaxis_label()\n        self._style_time_series_axes_and_layout(title)\n        self._append_footnotes()\n\n    @staticmethod\n    def fix_snapshots(df: pd.DataFrame, year: int) -&gt; pd.DataFrame:\n        \"\"\"\n        Correct the year in snapshot timestamp column labels.\n\n        Parameters\n        ----------\n        df\n            The DataFrame with timestamps to be adjusted.\n        year\n            The correct year to use in the data frame columns.\n\n        Returns\n        -------\n        :\n            The DataFrame with corrected timestamps.\n        \"\"\"\n        if isinstance(df.columns, pd.DatetimeIndex):\n            df.columns = [s.replace(year=year) for s in df.columns]\n        return df\n\n    def _style_inflexible_demand(self) -&gt; None:\n        \"\"\"Set the inflexible demand style if it exists.\"\"\"\n        self.fig.update_traces(\n            selector={\"name\": \"Inflexible Demand\"},\n            fillcolor=None,\n            fill=None,\n            stackgroup=None,\n            legendrank=2000,  # first entry in legend (from top)\n        )\n\n    def _style_time_series_axes_and_layout(self, title) -&gt; None:\n        \"\"\"\n        Update the layout and axes for time series charts.\n\n        Parameters\n        ----------\n        title\n            The figure title to show at the top of the graph.\n        \"\"\"\n        self.fig.update_yaxes(\n            tickprefix=\"&lt;b&gt;\",\n            ticksuffix=\"&lt;/b&gt;\",\n            tickfont_size=15,\n            color=self.cfg.yaxis_color,\n            title_font_size=15,\n            tickformat=\".0f\",  # if \"TW\" in self.unit else \".3f\",\n            gridwidth=1,\n            gridcolor=\"gainsboro\",\n        )\n        self.fig.update_xaxes(ticklabelmode=\"period\")\n        self.fig.update_layout(\n            title=title,\n            yaxis_title=self.unit,\n            hovermode=\"x\",\n        )\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.ESMTimeSeriesChart.df","title":"<code>df</code>  <code>cached</code> <code>property</code>","text":"<p>Plot data formatted for time series charts.</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The formatted data for creating bar charts.</p>"},{"location":"reference/evals/plots/#evals.plots.ESMTimeSeriesChart.fix_snapshots","title":"<code>fix_snapshots(df, year)</code>  <code>staticmethod</code>","text":"<p>Correct the year in snapshot timestamp column labels.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The DataFrame with timestamps to be adjusted.</p> required <code>year</code> <code>int</code> <p>The correct year to use in the data frame columns.</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The DataFrame with corrected timestamps.</p> Source code in <code>evals/plots/timeseries.py</code> <pre><code>@staticmethod\ndef fix_snapshots(df: pd.DataFrame, year: int) -&gt; pd.DataFrame:\n    \"\"\"\n    Correct the year in snapshot timestamp column labels.\n\n    Parameters\n    ----------\n    df\n        The DataFrame with timestamps to be adjusted.\n    year\n        The correct year to use in the data frame columns.\n\n    Returns\n    -------\n    :\n        The DataFrame with corrected timestamps.\n    \"\"\"\n    if isinstance(df.columns, pd.DatetimeIndex):\n        df.columns = [s.replace(year=year) for s in df.columns]\n    return df\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.ESMTimeSeriesChart.plot","title":"<code>plot()</code>","text":"<p>Plot the data to the chart.</p> <p>This function iterates over the data series, adds traces to the figure, styles the inflexible demand, sets the layout, styles the title, legend, x-axis label, time series axes, and appends footnotes.</p> Source code in <code>evals/plots/timeseries.py</code> <pre><code>def plot(self) -&gt; None:\n    \"\"\"\n    Plot the data to the chart.\n\n    This function iterates over the data series, adds traces to the\n    figure, styles the inflexible demand, sets the layout, styles\n    the title, legend, x-axis label, time series axes, and appends\n    footnotes.\n    \"\"\"\n    title = self.cfg.title.format(\n        location=self.location, year=self.year, unit=self.unit\n    )\n    if self.empty_input:\n        self.fig = empty_figure(title)\n        return\n\n    stackgroup = None\n    for i, (name, series) in enumerate(self.df.items()):\n        if self.cfg.stacked:\n            stackgroup = \"supply\" if series.sum() &gt;= 0 else \"withdrawal\"\n        legendrank = 1000 + i if stackgroup == \"supply\" else 1000 - i\n        self.fig.add_trace(\n            go.Scatter(\n                x=series.index,\n                y=series.values,\n                hovertemplate=\"%{y:.2f} \" + self.unit,\n                name=name,\n                fill=self.cfg.fill.get(name, \"tonexty\"),\n                fillpattern_shape=self.cfg.pattern.get(name),\n                line_dash=self.cfg.line_dash.get(name, \"solid\"),\n                line_width=self.cfg.line_width.get(name, 1),\n                line_color=self.cfg.colors.get(name),\n                line_shape=self.cfg.line_shape,\n                fillcolor=self.cfg.colors.get(name),\n                stackgroup=stackgroup,\n                legendrank=legendrank,\n            )\n        )\n\n    self._style_inflexible_demand()\n    self._set_base_layout()\n    self._style_title_and_legend_and_xaxis_label()\n    self._style_time_series_axes_and_layout(title)\n    self._append_footnotes()\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.TransmissionGridMap","title":"<code>TransmissionGridMap</code>","text":"<p>Creates a map with transmission capacities.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>pandas.DataFrame</code> <p>A data frame with the transmission grid capacities.</p> required <code>import_energy</code> <code>pandas.DataFrame</code> <p>A data frame with global import energy import amounts and capacity.</p> required <code>import_capacity</code> <code>pandas.DataFrame</code> <p>A data frame with global import capacities.</p> required <code>buses</code> <code>pandas.DataFrame</code> <p>The buses data frame, used to determine the bus coordinates.</p> required <code>config</code> <code>evals.plots.gridmap.GridMapConfig</code> <p>The GridMapConfig object with configuration options.</p> required Source code in <code>evals/plots/gridmap.py</code> <pre><code>class TransmissionGridMap:\n    \"\"\"\n    Creates a map with transmission capacities.\n\n    Parameters\n    ----------\n    grid\n        A data frame with the transmission grid capacities.\n    import_energy\n        A data frame with global import energy import amounts and\n        capacity.\n    import_capacity\n        A data frame with global import capacities.\n    buses\n        The buses data frame, used to determine the bus coordinates.\n    config\n        The GridMapConfig object with configuration options.\n    \"\"\"\n\n    def __init__(\n        self,\n        grid: pd.DataFrame,\n        import_energy: pd.DataFrame,\n        import_capacity: pd.DataFrame,\n        buses: pd.DataFrame,\n        config: GridMapConfig,\n    ) -&gt; None:\n        self.grid = grid\n        self.import_energy = import_energy\n        self.import_capacity = import_capacity\n        self.buses = buses  # todo: do not uses buses from one year\n        self.cfg = config\n        self.fmap = folium.Map(\n            tiles=None,\n            location=self.cfg.map_center,\n            zoom_start=self.cfg.zoom_start,\n            max_bounds=self.cfg.max_bounds,\n            max_lat=self.cfg.bounds[\"N\"],\n            max_lon=self.cfg.bounds[\"E\"],\n            min_lat=self.cfg.bounds[\"S\"],\n            min_lon=self.cfg.bounds[\"W\"],\n        )\n\n        # feature groups are layers in the map and can be shown or hid\n        self.feature_groups = {}\n        for year in sorted(grid.index.unique(DataModel.YEAR)):\n            fg = folium.FeatureGroup(name=year, show=True)\n            self.feature_groups[year] = fg\n            self.fmap.add_child(fg)  # register the feature group\n\n    def save(\n        self, output_path: pathlib.Path, file_name: str, subdir: str = \"HTML\"\n    ) -&gt; None:\n        \"\"\"\n        Write the map to a html file.\n\n        We want to store the HTML inside the JSON folder by default,\n        because Folium does not support the import of JSON files.\n        Therefore, we dump HTML files and include them as iFrames in\n        the web UI instead of importing JSONs via the plotly library.\n\n        Parameters\n        ----------\n        output_path\n            The path to save the map in.\n        file_name\n            The name of the file to export the map to.\n        subdir\n            An optional subdirectory to store files at. Leave emtpy\n            to skip, or change to html.\n        \"\"\"\n        output_path = self.make_evaluation_result_directories(output_path, subdir)\n        self.fmap.save(output_path / f\"{file_name}.html\")\n\n    def make_evaluation_result_directories(\n        self, result_path: Path, subdir: Path | str\n    ) -&gt; Path:\n        \"\"\"\n        Create all directories needed to store evaluations results.\n\n        Parameters\n        ----------\n        result_path\n            The path of the result folder.\n        subdir\n            A relative path inside the result folder.\n\n        Returns\n        -------\n        :\n            The joined path: result_dir / subdir.\n        \"\"\"\n        output_path = self.make_directory(result_path, subdir)\n        output_path = self.make_directory(output_path, \"HTML\")\n\n        return output_path\n\n    def draw_grid_by_carrier_groups_myopic(self) -&gt; None:\n        \"\"\"Plot carrier groups for all years to one map.\"\"\"\n        self.add_basemap_layers()\n\n        plot_grid = self._calculate_line_weights(self.grid)\n\n        _groups = [DataModel.YEAR, \"bus0\", \"bus1\"]\n        year_edge = plot_grid.groupby(_groups, group_keys=False)\n\n        plot_grid = year_edge.apply(self._calculate_line_offset)\n\n        grid_line = plot_grid.groupby(plot_grid.index.names, group_keys=False)\n        plot_grid = grid_line.apply(self._calculate_line_center)\n\n        plot_grid.groupby([DataModel.YEAR, DataModel.CARRIER]).apply(\n            self._draw_grid_polyline_with_circle_marker\n        )\n\n        self.draw_country_markers()\n        # self.draw_import_locations()\n        self.add_control_widgets()\n\n    def add_control_widgets(self) -&gt; None:\n        \"\"\"Add UI elements to the map.\"\"\"\n        plugins.GroupedLayerControl(\n            groups={\"Year\": list(self.feature_groups.values())},\n            collapsed=False,\n            position=\"topleft\",\n        ).add_to(self.fmap)\n\n        plugins.Fullscreen(\n            position=\"topright\",\n            title=\"Full Screen\",\n            title_cancel=\"Exit Full Screen\",\n            force_separate_button=True,\n        ).add_to(self.fmap)\n\n    def draw_import_locations(self) -&gt; None:\n        \"\"\"\n        Add import location icons and lines to the map.\n\n        Notes\n        -----\n        Available icons: https://fontawesome.com/icons/categories\n        \"\"\"\n        icon_locations = {\n            # node: [y, x]  Lat, Lon\n            \"BE0 0\": [51.21868, 2.86993],\n            \"DE0 4\": [53.92445, 8.67684],\n            \"EE6 0\": [58.78505, 23.15726],\n            \"ES0 0\": [43.41430, -4.27864],\n            \"FI3 0\": [60.89107, 22.68793],\n            \"FR0 0\": [48.18790, -3.68987],\n            \"GB5 0\": [54.63442, -0.70133],\n            \"GR0 0\": [38.67136, 26.65004],\n            \"HU0 0\": [48.20228, 22.60233],\n            \"IT0 0\": [37.16396, 13.49807],\n            \"LT6 0\": [55.71138, 21.07711],\n            \"LV6 0\": [56.99505, 27.72035],\n            \"NL0 0\": [53.03334, 4.96787],\n            \"NO3 0\": [60.05473, 5.00377],\n            \"PL0 0\": [51.99998, 22.13991],\n            \"PT0 0\": [37.98446, -8.88731],\n            \"RO0 0\": [44.51848, 28.89059],\n            \"SK0 0\": [48.78314, 22.35254],\n        }\n\n        _idx = self.import_capacity.index.names\n        row_slices = self.import_capacity.to_frame().groupby(_idx, group_keys=False)\n        import_capacity = row_slices.apply(self._calculate_line_weights)\n\n        for (year, node), capas in import_capacity.groupby(\n            [DataModel.YEAR, DataModel.LOCATION]\n        ):\n            fg = self.feature_groups[year]\n            # need a new instance for every icon\n            # icon = self._get_icon(RIGHT_TO_BRACKET_SOLID)\n            popup_table = filter_by(self.import_energy, year=year, location=node)\n            popup_table = popup_table.droplevel(\n                [DataModel.YEAR, DataModel.CARRIER, DataModel.BUS_CARRIER]\n            )\n            bootstrap5_classes = (\n                \"table table-striped table-hover table-condensed table-responsive\"\n            )\n            popup_html = popup_table.to_frame().to_html(classes=bootstrap5_classes)\n            folium.Marker(\n                location=icon_locations[node],\n                # icon=folium.CustomIcon(icon, icon_size=(10, 10)),\n                popup=folium.Popup(popup_html),\n                tooltip=\"Global Import\",\n            ).add_to(fg)\n\n            # draw line from import icon location to node location\n            node_y = self.buses.loc[node, \"y\"]\n            node_x = self.buses.loc[node, \"x\"]\n            capacity = capas[self.import_capacity.name].iloc[0]\n            label = f\"{capas.attrs['name']}: {capacity:.2f} {capas.attrs['unit']}\"\n            folium.PolyLine(\n                locations=[icon_locations[node], [node_y, node_x]],\n                color=\"black\",\n                weight=capas[\"line_weight\"].iloc[0],\n                tooltip=label,\n                popup=label,\n            ).add_to(fg)\n\n    def add_basemap_layers(self) -&gt; None:\n        \"\"\"Add common background layer to the map.\"\"\"\n        self._add_wms_tiles()\n        self._load_geojson(\n            \"regions_onshore_base_s_adm.geojson\",\n            style={\n                \"weight\": 1,\n                \"color\": \"grey\",\n                \"fillColor\": \"white\",\n                \"opacity\": 0.5,\n            },\n        )\n\n        # self._load_geojson(\n        #     \"neighbors.geojson\",\n        #     style={\n        #         \"weight\": 0.5,\n        #         \"color\": \"black\",\n        #         \"fillColor\": \"black\",\n        #         \"opacity\": 0.2,\n        #     },\n        # )\n\n    def draw_country_markers(self) -&gt; None:\n        \"\"\"\n        Draw markers for countries on the map.\n\n        Retrieves bus information from networks, iterates over unique\n        bus locations, creates CircleMarker and Marker objects for\n        each location with corresponding short and nice names, and\n        adds them to the map.\n        \"\"\"\n        fg_labels = folium.FeatureGroup(\n            name=\"Country Marker\", overlays=True, interactive=False\n        )\n\n        buses0 = self.grid.index.unique(\"bus0\")\n        buses1 = self.grid.index.unique(\"bus1\")\n        icon_css = \"margin-top:1.5px; font-size:10px; font-family:sans-serif\"\n\n        for bus in buses0.union(buses1):\n            # keep region ID for AT and DE, else just the country code\n            short_name = bus[:2] + bus[-1] if bus.startswith((\"AT\", \"DE\")) else bus[:2]\n            nice_name = ALIAS_REGION.get(bus, ALIAS_COUNTRY[bus[:2]])\n            location = self.buses.loc[bus, [\"y\", \"x\"]].to_numpy()\n\n            icon = plugins.BeautifyIcon(\n                icon_shape=\"circle\",\n                border_width=2,\n                border_color=\"black\",\n                # background_color=\"white\",\n                text_color=\"black\",\n                inner_icon_style=icon_css,\n                number=short_name,\n            )\n            marker = folium.Marker(location=location, popup=nice_name, icon=icon)\n            marker.add_to(fg_labels)\n\n        fg_labels.add_to(self.fmap)\n\n    @staticmethod\n    def _get_icon(icon: str) -&gt; str:\n        \"\"\"\n        Encode a raw SVG string to bytes.\n\n        Parameters\n        ----------\n        icon\n            The utf-8 encoded HTML representation of an SVG icon.\n\n        Returns\n        -------\n        :\n            The base64 encoded SVG icon as a string.\n        \"\"\"\n        data = base64.b64encode(icon.strip().encode(\"utf-8\")).decode(\"utf-8\")\n        return f\"data:image/svg+xml;base64,{data}\"\n\n    def _calculate_line_weights(self, df_slice: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate the line weights for a grid.\n\n        Parameters\n        ----------\n        df_slice\n            The grids that will be plotted to the map.\n\n        Returns\n        -------\n        :\n            The grids with an additional column for the line weight in px.\n        \"\"\"\n        # prevent assignment to copies of a data view\n        df_slice = df_slice.copy()\n\n        col = f\"{df_slice.attrs['name']} ({df_slice.attrs['unit']})\"\n\n        _min, _max = df_slice[col].min(), df_slice[col].max()\n        _max_width = self.cfg.line_weight_max  # 20.0\n        _min_width = self.cfg.line_weight_min  # 2.0\n\n        def linear_scale(ser: pd.Series) -&gt; pd.Series:\n            \"\"\"\n            Scale values between lower and upper line weight values.\n\n            Returns the linear equation k * x + d. Where x is the ratio,\n            k is the min-max range and d the lower bound constant.\n\n            Parameters\n            ----------\n            ser\n                The values to be scaled between 2 and 5.\n\n            Returns\n            -------\n            The scaled value used as line width in pixel.\n            \"\"\"\n            min_max_ratio = (ser - _min) / (_max - _min)\n            return min_max_ratio * (_max_width - _min_width) + _min_width\n\n        if _min == _max:\n            df_slice.loc[:, \"line_weight\"] = self.cfg.line_weight_min\n        else:\n            df_slice.loc[:, \"line_weight\"] = df_slice[col].apply(linear_scale)\n\n        return df_slice\n\n    @staticmethod\n    def _calculate_line_offset(df_slice: pd.DataFrame, gap: float = 1) -&gt; pd.DataFrame:\n        \"\"\"\n        Add a column with the offset values for a grid.\n\n        The offset is used to prevent overplotting lines and labels.\n        It is only required if multiple edges exist between the same\n        nodes.\n\n        Parameters\n        ----------\n        df_slice\n            A data frame slice for every unique node connection\n            and for all displayed carriers in a map.\n        gap : float, optional\n            Number of pixels to insert between the edges of adjacent lines\n            (default is 1 px), preventing strokes from touching.\n\n        Returns\n        -------\n        :\n            The input data slice with the offset in pixel in an\n            additional column.\n        \"\"\"\n        # if df_slice.shape[0] == 1:\n        #     df_slice[\"offset\"] = 0\n        # elif df_slice.shape[0] == 2:\n        #     # move lines (down and up) by half their combined line\n        #     # weights plus 1 px for a visible gap\n        #     half_weight = df_slice[\"line_weight\"].sum() / 2 + 1\n        #     df_slice[\"offset\"] = [-0.5 * half_weight, 0.5 * half_weight]\n        # else:\n        #     raise NotImplementedError(f\"Number of rows: {df_slice} not supported.\")\n        #\n        # return df_slice\n        weights = df_slice[\"line_weight\"].astype(float).tolist()\n        n = len(weights)\n\n        # Total envelope width = sum of all stroke widths + (n-1) gaps\n        total_width = sum(weights) + (n - 1) * gap\n\n        # Start at left edge of that envelope\n        current = -total_width / 2\n        offsets = []\n\n        # For each stroke:\n        #   \u2022 move by half its width  \u2192 centerline of this band\n        #   \u2022 record that as the offset\n        #   \u2022 then advance by (half its width + gap) to get to the next band\u2019s start\n        for w in weights:\n            current += w / 2\n            offsets.append(current)\n            current += w / 2 + gap\n\n        # Attach offsets and return\n        df_slice[\"offset\"] = offsets\n        return df_slice\n\n    @staticmethod\n    def _calculate_line_center(df_slice: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate the line center for all lines.\n\n        In case the line has an offset, the center is moved by 10%\n        of the line length and along the line in the direction of the\n        offset.\n\n        Parameters\n        ----------\n        df_slice\n            The data frame with a \"line\" column that contains\n            coordinate pairs and an \"offset\" column that contains\n            a positive or negative float for the line offset.\n\n        Returns\n        -------\n        :\n            The input data frame with additional column containing the\n            line center.\n        \"\"\"\n\n        def compute_center(row):\n            offset = row[\"offset\"]\n            line = row[\"line\"]\n            if offset != 0:\n                x0, x1 = line[0][0], line[1][0]\n                y0, y1 = line[0][1], line[1][1]\n                # Move center by +-10% of line length depending on offset sign.\n                ratio = 0.5 + copysign(0.1, offset)\n                x = x0 + ratio * (x1 - x0)\n                y = y0 + ratio * (y1 - y0)\n                return [x, y]\n            else:\n                # Compute the simple midpoint\n                return [(line[0][i] + line[1][i]) / 2 for i in range(len(line[0]))]\n\n        df_slice[\"line_center\"] = df_slice.apply(compute_center, axis=1)\n\n        return df_slice\n\n    def _draw_grid_polyline_with_circle_marker(self, grid: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Draw grid lines on the map for a specific carrier.\n\n        Retrieves the nice name, color, and dash array configuration\n        for the carrier. Iterates over grid data, creates PolyLine\n        objects for capacities larger than the threshold in the config,\n        sets color, weight, dash array, tooltip, and popup based on the\n        capacity, and adds them to the FeatureGroup.\n\n        Parameters\n        ----------\n        grid\n            The metric dataframe with the capacities and the lines.\n        \"\"\"\n        year = grid.index.unique(DataModel.YEAR)[0]\n        carrier = grid.index.unique(DataModel.CARRIER)[0]\n\n        fg = self.feature_groups[year]\n        style = self.cfg.carrier_style[carrier]\n        nice_name = style[\"nice_name\"]\n        color = style[\"color\"]\n        unit = grid.attrs[\"unit\"]\n\n        col = f\"{grid.attrs['name']} ({grid.attrs['unit']})\"\n        significant_edges = grid[grid[col] &gt;= self.cfg.line_capacity_threshold]\n\n        for _, row in significant_edges.iterrows():\n            capacity = row[col]  # / 1000  # GW\n            tooltip = f\"{nice_name}: {capacity:.2f} {unit}\"\n\n            plugins.PolyLineOffset(\n                locations=row[\"line\"],\n                offset=row[\"offset\"],\n                color=color,\n                weight=row[\"line_weight\"],\n                dash_array=f\"{row['line_weight']}\" if style.get(\"dash_array\") else None,\n                # https://www.w3schools.com/graphics/svg_stroking.asp\n                line_cap=\"butt\",  # or \"round\"\n                tooltip=tooltip,\n                popup=tooltip,\n            ).add_to(fg)\n\n            # https://github.com/masajid390/BeautifyMarker\n            icon_css = \"margin-top:2.5px; font-size:10px; font-family:sans-serif\"\n            icon = plugins.BeautifyIcon(\n                icon_shape=\"circle\",\n                border_width=1,\n                border_color=color,\n                background_color=\"white\",\n                text_color=color,\n                number=prettify_number(capacity),\n                inner_icon_style=icon_css,\n            )\n\n            folium.Marker(\n                location=row[\"line_center\"],\n                popup=folium.Popup(nice_name),\n                icon=icon,\n                tooltip=tooltip,\n            ).add_to(fg)\n\n    def _add_wms_tiles(self) -&gt; None:\n        \"\"\"Add a web map tile service layer to the map.\"\"\"\n        folium.TileLayer(\n            name=\"WMS tiles\",\n            control=False,  # no layer controls\n            tiles=self.cfg.tile_provider,\n            attr=self.cfg.attribution,\n            min_zoom=self.cfg.zoom_min,\n            max_zoom=self.cfg.zoom_max,\n            # additional leaflet.js args: https://leafletjs.com/reference.html#tilelayer\n        ).add_to(self.fmap)\n\n    def _load_geojson(self, file_name: str, style: dict = None) -&gt; None:\n        \"\"\"\n        Add the geojson layer.\n\n        Parameters\n        ----------\n        file_name\n            The name of the geojson file under esmtools/data.\n        style\n            The style dictionary to pass to the geojson layer.\n        \"\"\"\n        # res = resources.files(\"evals\") / \"data\"\n        # gdf = gpd.read_file(res / file_name).to_crs(crs=f\"EPSG:{self.cfg.crs}\")\n        gdf = gpd.read_file(Path(\"resources\") / file_name).to_crs(\n            crs=f\"EPSG:{self.cfg.crs}\"\n        )\n        if style:  # applies the same style to all features\n            gdf[\"style\"] = [style] * gdf.shape[0]\n\n        gj = GeoJson(gdf, control=False, overlay=True)\n        gj.add_to(self.fmap)\n\n    @staticmethod\n    def make_directory(base: Path, subdir: Path | str) -&gt; Path:\n        \"\"\"\n        Create a directory and return its path.\n\n        Parameters\n        ----------\n        base\n            The path to base of the new folder.\n        subdir\n            A relative path inside the base folder.\n\n        Returns\n        -------\n        :\n            The joined path: result_dir / subdir / now.\n        \"\"\"\n        base = Path(base).resolve()\n        assert base.is_dir(), f\"Base path does not exist: {base}.\"\n        directory_path = base / subdir\n        directory_path.mkdir(parents=True, exist_ok=True)\n\n        return directory_path\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.TransmissionGridMap.add_basemap_layers","title":"<code>add_basemap_layers()</code>","text":"<p>Add common background layer to the map.</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>def add_basemap_layers(self) -&gt; None:\n    \"\"\"Add common background layer to the map.\"\"\"\n    self._add_wms_tiles()\n    self._load_geojson(\n        \"regions_onshore_base_s_adm.geojson\",\n        style={\n            \"weight\": 1,\n            \"color\": \"grey\",\n            \"fillColor\": \"white\",\n            \"opacity\": 0.5,\n        },\n    )\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.TransmissionGridMap.add_control_widgets","title":"<code>add_control_widgets()</code>","text":"<p>Add UI elements to the map.</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>def add_control_widgets(self) -&gt; None:\n    \"\"\"Add UI elements to the map.\"\"\"\n    plugins.GroupedLayerControl(\n        groups={\"Year\": list(self.feature_groups.values())},\n        collapsed=False,\n        position=\"topleft\",\n    ).add_to(self.fmap)\n\n    plugins.Fullscreen(\n        position=\"topright\",\n        title=\"Full Screen\",\n        title_cancel=\"Exit Full Screen\",\n        force_separate_button=True,\n    ).add_to(self.fmap)\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.TransmissionGridMap.draw_country_markers","title":"<code>draw_country_markers()</code>","text":"<p>Draw markers for countries on the map.</p> <p>Retrieves bus information from networks, iterates over unique bus locations, creates CircleMarker and Marker objects for each location with corresponding short and nice names, and adds them to the map.</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>def draw_country_markers(self) -&gt; None:\n    \"\"\"\n    Draw markers for countries on the map.\n\n    Retrieves bus information from networks, iterates over unique\n    bus locations, creates CircleMarker and Marker objects for\n    each location with corresponding short and nice names, and\n    adds them to the map.\n    \"\"\"\n    fg_labels = folium.FeatureGroup(\n        name=\"Country Marker\", overlays=True, interactive=False\n    )\n\n    buses0 = self.grid.index.unique(\"bus0\")\n    buses1 = self.grid.index.unique(\"bus1\")\n    icon_css = \"margin-top:1.5px; font-size:10px; font-family:sans-serif\"\n\n    for bus in buses0.union(buses1):\n        # keep region ID for AT and DE, else just the country code\n        short_name = bus[:2] + bus[-1] if bus.startswith((\"AT\", \"DE\")) else bus[:2]\n        nice_name = ALIAS_REGION.get(bus, ALIAS_COUNTRY[bus[:2]])\n        location = self.buses.loc[bus, [\"y\", \"x\"]].to_numpy()\n\n        icon = plugins.BeautifyIcon(\n            icon_shape=\"circle\",\n            border_width=2,\n            border_color=\"black\",\n            # background_color=\"white\",\n            text_color=\"black\",\n            inner_icon_style=icon_css,\n            number=short_name,\n        )\n        marker = folium.Marker(location=location, popup=nice_name, icon=icon)\n        marker.add_to(fg_labels)\n\n    fg_labels.add_to(self.fmap)\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.TransmissionGridMap.draw_grid_by_carrier_groups_myopic","title":"<code>draw_grid_by_carrier_groups_myopic()</code>","text":"<p>Plot carrier groups for all years to one map.</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>def draw_grid_by_carrier_groups_myopic(self) -&gt; None:\n    \"\"\"Plot carrier groups for all years to one map.\"\"\"\n    self.add_basemap_layers()\n\n    plot_grid = self._calculate_line_weights(self.grid)\n\n    _groups = [DataModel.YEAR, \"bus0\", \"bus1\"]\n    year_edge = plot_grid.groupby(_groups, group_keys=False)\n\n    plot_grid = year_edge.apply(self._calculate_line_offset)\n\n    grid_line = plot_grid.groupby(plot_grid.index.names, group_keys=False)\n    plot_grid = grid_line.apply(self._calculate_line_center)\n\n    plot_grid.groupby([DataModel.YEAR, DataModel.CARRIER]).apply(\n        self._draw_grid_polyline_with_circle_marker\n    )\n\n    self.draw_country_markers()\n    # self.draw_import_locations()\n    self.add_control_widgets()\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.TransmissionGridMap.draw_import_locations","title":"<code>draw_import_locations()</code>","text":"<p>Add import location icons and lines to the map.</p> Notes <p>Available icons: https://fontawesome.com/icons/categories</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>def draw_import_locations(self) -&gt; None:\n    \"\"\"\n    Add import location icons and lines to the map.\n\n    Notes\n    -----\n    Available icons: https://fontawesome.com/icons/categories\n    \"\"\"\n    icon_locations = {\n        # node: [y, x]  Lat, Lon\n        \"BE0 0\": [51.21868, 2.86993],\n        \"DE0 4\": [53.92445, 8.67684],\n        \"EE6 0\": [58.78505, 23.15726],\n        \"ES0 0\": [43.41430, -4.27864],\n        \"FI3 0\": [60.89107, 22.68793],\n        \"FR0 0\": [48.18790, -3.68987],\n        \"GB5 0\": [54.63442, -0.70133],\n        \"GR0 0\": [38.67136, 26.65004],\n        \"HU0 0\": [48.20228, 22.60233],\n        \"IT0 0\": [37.16396, 13.49807],\n        \"LT6 0\": [55.71138, 21.07711],\n        \"LV6 0\": [56.99505, 27.72035],\n        \"NL0 0\": [53.03334, 4.96787],\n        \"NO3 0\": [60.05473, 5.00377],\n        \"PL0 0\": [51.99998, 22.13991],\n        \"PT0 0\": [37.98446, -8.88731],\n        \"RO0 0\": [44.51848, 28.89059],\n        \"SK0 0\": [48.78314, 22.35254],\n    }\n\n    _idx = self.import_capacity.index.names\n    row_slices = self.import_capacity.to_frame().groupby(_idx, group_keys=False)\n    import_capacity = row_slices.apply(self._calculate_line_weights)\n\n    for (year, node), capas in import_capacity.groupby(\n        [DataModel.YEAR, DataModel.LOCATION]\n    ):\n        fg = self.feature_groups[year]\n        # need a new instance for every icon\n        # icon = self._get_icon(RIGHT_TO_BRACKET_SOLID)\n        popup_table = filter_by(self.import_energy, year=year, location=node)\n        popup_table = popup_table.droplevel(\n            [DataModel.YEAR, DataModel.CARRIER, DataModel.BUS_CARRIER]\n        )\n        bootstrap5_classes = (\n            \"table table-striped table-hover table-condensed table-responsive\"\n        )\n        popup_html = popup_table.to_frame().to_html(classes=bootstrap5_classes)\n        folium.Marker(\n            location=icon_locations[node],\n            # icon=folium.CustomIcon(icon, icon_size=(10, 10)),\n            popup=folium.Popup(popup_html),\n            tooltip=\"Global Import\",\n        ).add_to(fg)\n\n        # draw line from import icon location to node location\n        node_y = self.buses.loc[node, \"y\"]\n        node_x = self.buses.loc[node, \"x\"]\n        capacity = capas[self.import_capacity.name].iloc[0]\n        label = f\"{capas.attrs['name']}: {capacity:.2f} {capas.attrs['unit']}\"\n        folium.PolyLine(\n            locations=[icon_locations[node], [node_y, node_x]],\n            color=\"black\",\n            weight=capas[\"line_weight\"].iloc[0],\n            tooltip=label,\n            popup=label,\n        ).add_to(fg)\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.TransmissionGridMap.make_directory","title":"<code>make_directory(base, subdir)</code>  <code>staticmethod</code>","text":"<p>Create a directory and return its path.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>pathlib.Path</code> <p>The path to base of the new folder.</p> required <code>subdir</code> <code>pathlib.Path | str</code> <p>A relative path inside the base folder.</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>The joined path: result_dir / subdir / now.</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>@staticmethod\ndef make_directory(base: Path, subdir: Path | str) -&gt; Path:\n    \"\"\"\n    Create a directory and return its path.\n\n    Parameters\n    ----------\n    base\n        The path to base of the new folder.\n    subdir\n        A relative path inside the base folder.\n\n    Returns\n    -------\n    :\n        The joined path: result_dir / subdir / now.\n    \"\"\"\n    base = Path(base).resolve()\n    assert base.is_dir(), f\"Base path does not exist: {base}.\"\n    directory_path = base / subdir\n    directory_path.mkdir(parents=True, exist_ok=True)\n\n    return directory_path\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.TransmissionGridMap.make_evaluation_result_directories","title":"<code>make_evaluation_result_directories(result_path, subdir)</code>","text":"<p>Create all directories needed to store evaluations results.</p> <p>Parameters:</p> Name Type Description Default <code>result_path</code> <code>pathlib.Path</code> <p>The path of the result folder.</p> required <code>subdir</code> <code>pathlib.Path | str</code> <p>A relative path inside the result folder.</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>The joined path: result_dir / subdir.</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>def make_evaluation_result_directories(\n    self, result_path: Path, subdir: Path | str\n) -&gt; Path:\n    \"\"\"\n    Create all directories needed to store evaluations results.\n\n    Parameters\n    ----------\n    result_path\n        The path of the result folder.\n    subdir\n        A relative path inside the result folder.\n\n    Returns\n    -------\n    :\n        The joined path: result_dir / subdir.\n    \"\"\"\n    output_path = self.make_directory(result_path, subdir)\n    output_path = self.make_directory(output_path, \"HTML\")\n\n    return output_path\n</code></pre>"},{"location":"reference/evals/plots/#evals.plots.TransmissionGridMap.save","title":"<code>save(output_path, file_name, subdir='HTML')</code>","text":"<p>Write the map to a html file.</p> <p>We want to store the HTML inside the JSON folder by default, because Folium does not support the import of JSON files. Therefore, we dump HTML files and include them as iFrames in the web UI instead of importing JSONs via the plotly library.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>pathlib.Path</code> <p>The path to save the map in.</p> required <code>file_name</code> <code>str</code> <p>The name of the file to export the map to.</p> required <code>subdir</code> <code>str</code> <p>An optional subdirectory to store files at. Leave emtpy to skip, or change to html.</p> <code>'HTML'</code> Source code in <code>evals/plots/gridmap.py</code> <pre><code>def save(\n    self, output_path: pathlib.Path, file_name: str, subdir: str = \"HTML\"\n) -&gt; None:\n    \"\"\"\n    Write the map to a html file.\n\n    We want to store the HTML inside the JSON folder by default,\n    because Folium does not support the import of JSON files.\n    Therefore, we dump HTML files and include them as iFrames in\n    the web UI instead of importing JSONs via the plotly library.\n\n    Parameters\n    ----------\n    output_path\n        The path to save the map in.\n    file_name\n        The name of the file to export the map to.\n    subdir\n        An optional subdirectory to store files at. Leave emtpy\n        to skip, or change to html.\n    \"\"\"\n    output_path = self.make_evaluation_result_directories(output_path, subdir)\n    self.fmap.save(output_path / f\"{file_name}.html\")\n</code></pre>"},{"location":"reference/evals/plots/_base/","title":"_base.py","text":"<p>Common graph bases and emtpy figures.</p>"},{"location":"reference/evals/plots/_base/#evals.plots._base.ESMChart","title":"<code>ESMChart</code>","text":"<p>A base class for Energy System Modeling graphs using Plotly.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The data frame with the plot data. The class expects a data frame that complies with the metric data model, i.e. has the expected column and index labels.</p> required <code>cfg</code> <code>evals.configs.PlotConfig</code> <p>The plotly configuration.</p> required Source code in <code>evals/plots/_base.py</code> <pre><code>class ESMChart:\n    \"\"\"\n    A base class for Energy System Modeling graphs using Plotly.\n\n    Parameters\n    ----------\n    df\n        The data frame with the plot data. The class expects a data\n        frame that complies with the metric data model, i.e. has\n        the expected column and index labels.\n\n    cfg\n        The plotly configuration.\n    \"\"\"\n\n    # todo: avoid inheritance and use MixIns instead\n\n    def __init__(self, df: pd.DataFrame, cfg: PlotConfig) -&gt; None:\n        self._df = df\n        self.cfg = cfg\n        self.fig = go.Figure()\n        self.unit = self.cfg.unit or df.attrs[\"unit\"]\n        self.metric_name = df.attrs[\"name\"]\n        self.location = \"\"\n        self.col_values = \"\"\n\n    @property\n    def empty_input(self) -&gt; bool:\n        \"\"\"\n        Determine if the input DataFrame is empty or all NaN.\n\n        Returns\n        -------\n        :\n            True if the input DataFrame is empty or contains only NaN\n            values, False otherwise.\n        \"\"\"\n        return self._df.empty or self._df.isna().all().all()\n\n    def to_html(\n        self, output_path: pathlib.Path, groupby: list[str], idx: typing.Hashable\n    ) -&gt; pathlib.Path:\n        \"\"\"\n        Serialize the Plotly figure to an HTML file.\n\n        Parameters\n        ----------\n        output_path\n            The folder to save the HTML file under.\n        groupby\n            List of groupby keys needed to fill the file name template.\n        idx\n            The data frame index from the gropuby clause needed to fill\n            the file name template.\n\n        Returns\n        -------\n        :\n            The path of the file written.\n        \"\"\"\n        file_name = f\"{self.construct_file_name(groupby, idx)}.html\"\n        file_path = output_path / \"HTML\" / file_name\n        template_html = \"\"\"\\\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta charset=\"utf-8\" /&gt;\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /&gt;\n&lt;meta name=\"repo_name\" content=\"{{ repo_name }}\" /&gt;\n&lt;meta name=\"repo_branch\" content=\"{{ repo_branch }}\" /&gt;\n&lt;meta name=\"repo_hash\" content=\"{{ repo_hash }}\" /&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    {{ fig }}\n&lt;/body&gt;\n&lt;/html&gt;\"\"\"\n\n        div = self.fig.to_html(include_plotlyjs=\"directory\", full_html=False)\n        with file_path.open(\"w\", encoding=\"utf-8\") as fh:\n            fh.write(Template(template_html).render(fig=div, **RUN_META_DATA))\n\n        # need to write the plotly.js too, because to_html does not\n        bundle_path = file_path.parent / \"plotly.min.js\"\n        if not bundle_path.exists():\n            bundle_path.write_text(get_plotlyjs(), encoding=\"utf-8\")\n\n        return file_path\n\n    def to_json(\n        self, output_path: pathlib.Path, groupby: list[str], idx: typing.Hashable\n    ) -&gt; pathlib.Path:\n        \"\"\"\n        Serialize the Plotly figure to a JSON file.\n\n        Parameters\n        ----------\n        output_path\n            The folder to save the JSON file under.\n        groupby\n            List of groupby keys needed to fill the file name template.\n        idx\n            The data frame index from the gropuby clause needed to fill\n            the file name template.\n\n        Returns\n        -------\n        :\n            The path of the file written.\n        \"\"\"\n        file_name = f\"{self.construct_file_name(groupby, idx)}.json\"\n        file_path = output_path / \"JSON\" / file_name\n        self.fig.write_json(file_path, engine=\"auto\")\n\n        return file_path\n\n    def construct_file_name(self, groupby: list[str], idx: typing.Hashable) -&gt; str:\n        \"\"\"\n        Construct the file name based on the provided template.\n\n        Parameters\n        ----------\n        groupby\n            List of groupby values.\n        idx\n            The index used for constructing the file name.\n\n        Returns\n        -------\n        :\n            The constructed file name based on the template and\n            provided values.\n        \"\"\"\n        idx = [idx] if isinstance(idx, str) else idx\n        parts = {\"metric\": self.metric_name} | {\n            g: ALIAS_LOCATION_REV.get(i, i) for g, i in zip(groupby, idx, strict=True)\n        }\n        return self.cfg.file_name_template.format(**parts)\n\n    @staticmethod\n    def custom_sort(\n        df: pd.DataFrame, by: str, values: tuple, ascending: bool = False\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Sort a data frame by first appearance in values.\n\n        Sort a data frame by the given column and first appearance\n        in a given iterable.\n\n        Parameters\n        ----------\n        df\n            The dataframe to sort.\n        by\n            The column name to find values in.\n        values\n            The values to sort by. The order in this collection defines\n            the sort result.\n        ascending\n            Whether, or not to reverse the result (Plotly inserts\n            legend items from top down).\n\n        Returns\n        -------\n        :\n            The sorted data frame.\n        \"\"\"\n        if not values:\n            return df\n\n        def _custom_order(ser: pd.Series) -&gt; pd.Series:\n            \"\"\"\n            Sort by first appearance in an iterable.\n\n            First, construct a dictionary from the input values with the\n            series value as key and the position as value.\n\n            Second, use the dictionary in an anonymous function to get\n            the position, or 1000 (to put it last) if a value is not\n            found in the data series.\n\n            Parameters\n            ----------\n            ser\n                The pandas Series that should become sorted.\n\n            Returns\n            -------\n            :\n                The sorted pandas Series.\n            \"\"\"\n            order = {s: i for i, s in enumerate(values)}\n            return ser.apply(lambda x: order.get(x, 1000))\n\n        return df.sort_values(by=by, key=_custom_order, ascending=ascending)\n\n    def _set_base_layout(self) -&gt; None:\n        \"\"\"Set various figure properties.\"\"\"\n        self.fig.update_layout(\n            height=800,\n            font_family=\"Calibri\",\n            plot_bgcolor=\"#ffffff\",\n            legend_title_text=self.cfg.legend_header,\n        )\n        # update axes\n        self.fig.update_yaxes(\n            showgrid=self.cfg.yaxes_showgrid, visible=self.cfg.yaxes_visible\n        )\n        # draw zero xaxis line\n        self.fig.add_hline(y=0.0)\n\n        self.fig.update_xaxes(\n            showgrid=False,\n            tickprefix=\"&lt;b&gt;\",\n            ticksuffix=\"&lt;/b&gt;\",\n            tickfont_size=20,\n            title_font={\"size\": 20},\n        )\n        self.fig.update_layout(\n            xaxis={\"categoryorder\": \"category ascending\"},\n            hovermode=\"x\",  # all categories are shown by mouse-over\n        )\n        # trace order always needs to be reversed to show correct order\n        # of legend entries for relative bar charts\n        self.fig.update_layout(legend={\"traceorder\": \"reversed\"})\n\n        # export the metadata directly in the Layout property for JSON\n        self.fig.update_layout(meta=[RUN_META_DATA])\n\n    def _append_footnotes(self) -&gt; None:\n        \"\"\"Append the footnote(s) at the bottom of the figure.\"\"\"\n        self._append_footnote(self.cfg.footnotes[0], align=\"left\")\n        self._append_footnote(self.cfg.footnotes[1], y=-0.2)\n        if lines := self._count_footnote_lines():\n            self.fig.update_layout(margin={\"b\": 125 + 50 * lines})\n\n    def _append_footnote(\n        self, footnote_text: str, y: float = -0.17, align: str = None\n    ) -&gt; None:\n        \"\"\"\n        Append a footnote at the bottom of the Figure.\n\n        Parameters\n        ----------\n        footnote_text\n            The text displayed at the bottom of figures.\n        y\n            The vertical position of the footnote. Negative values\n            move the footnote down.\n        align\n            The text alignment mode.\n        \"\"\"\n        if footnote_text:\n            self.fig.add_annotation(\n                text=footnote_text,\n                xref=\"paper\",\n                yref=\"paper\",\n                xanchor=\"left\",\n                yanchor=\"top\",\n                x=0,\n                y=y,\n                showarrow=False,\n                font={\"size\": 15},\n                align=align,\n            )\n\n    def _style_title_and_legend_and_xaxis_label(self) -&gt; None:\n        \"\"\"Update figure title and legend.\"\"\"\n        self.fig.update_layout(\n            title_font_size=self.cfg.title_font_size,\n            font_size=self.cfg.font_size,\n            legend={\n                \"x\": 1,\n                \"y\": 1,\n                \"font\": {\"size\": self.cfg.legend_font_size},\n            },\n        )\n        if self.cfg.xaxis_title:  # allow skipping via empty string\n            self.fig.update_layout(xaxis_title=self.cfg.xaxis_title)\n\n    def _count_footnote_lines(self) -&gt; int:\n        \"\"\"\n        Count the number of lines in footnote texts.\n\n        Returns\n        -------\n        :\n            The number of text lines required to write the\n            footnote text.\n        \"\"\"\n        return \"\".join(self.cfg.footnotes).count(\"&lt;br&gt;\")\n</code></pre>"},{"location":"reference/evals/plots/_base/#evals.plots._base.ESMChart.empty_input","title":"<code>empty_input</code>  <code>property</code>","text":"<p>Determine if the input DataFrame is empty or all NaN.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the input DataFrame is empty or contains only NaN values, False otherwise.</p>"},{"location":"reference/evals/plots/_base/#evals.plots._base.ESMChart.construct_file_name","title":"<code>construct_file_name(groupby, idx)</code>","text":"<p>Construct the file name based on the provided template.</p> <p>Parameters:</p> Name Type Description Default <code>groupby</code> <code>list[str]</code> <p>List of groupby values.</p> required <code>idx</code> <code>typing.Hashable</code> <p>The index used for constructing the file name.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The constructed file name based on the template and provided values.</p> Source code in <code>evals/plots/_base.py</code> <pre><code>def construct_file_name(self, groupby: list[str], idx: typing.Hashable) -&gt; str:\n    \"\"\"\n    Construct the file name based on the provided template.\n\n    Parameters\n    ----------\n    groupby\n        List of groupby values.\n    idx\n        The index used for constructing the file name.\n\n    Returns\n    -------\n    :\n        The constructed file name based on the template and\n        provided values.\n    \"\"\"\n    idx = [idx] if isinstance(idx, str) else idx\n    parts = {\"metric\": self.metric_name} | {\n        g: ALIAS_LOCATION_REV.get(i, i) for g, i in zip(groupby, idx, strict=True)\n    }\n    return self.cfg.file_name_template.format(**parts)\n</code></pre>"},{"location":"reference/evals/plots/_base/#evals.plots._base.ESMChart.custom_sort","title":"<code>custom_sort(df, by, values, ascending=False)</code>  <code>staticmethod</code>","text":"<p>Sort a data frame by first appearance in values.</p> <p>Sort a data frame by the given column and first appearance in a given iterable.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The dataframe to sort.</p> required <code>by</code> <code>str</code> <p>The column name to find values in.</p> required <code>values</code> <code>tuple</code> <p>The values to sort by. The order in this collection defines the sort result.</p> required <code>ascending</code> <code>bool</code> <p>Whether, or not to reverse the result (Plotly inserts legend items from top down).</p> <code>False</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The sorted data frame.</p> Source code in <code>evals/plots/_base.py</code> <pre><code>@staticmethod\ndef custom_sort(\n    df: pd.DataFrame, by: str, values: tuple, ascending: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Sort a data frame by first appearance in values.\n\n    Sort a data frame by the given column and first appearance\n    in a given iterable.\n\n    Parameters\n    ----------\n    df\n        The dataframe to sort.\n    by\n        The column name to find values in.\n    values\n        The values to sort by. The order in this collection defines\n        the sort result.\n    ascending\n        Whether, or not to reverse the result (Plotly inserts\n        legend items from top down).\n\n    Returns\n    -------\n    :\n        The sorted data frame.\n    \"\"\"\n    if not values:\n        return df\n\n    def _custom_order(ser: pd.Series) -&gt; pd.Series:\n        \"\"\"\n        Sort by first appearance in an iterable.\n\n        First, construct a dictionary from the input values with the\n        series value as key and the position as value.\n\n        Second, use the dictionary in an anonymous function to get\n        the position, or 1000 (to put it last) if a value is not\n        found in the data series.\n\n        Parameters\n        ----------\n        ser\n            The pandas Series that should become sorted.\n\n        Returns\n        -------\n        :\n            The sorted pandas Series.\n        \"\"\"\n        order = {s: i for i, s in enumerate(values)}\n        return ser.apply(lambda x: order.get(x, 1000))\n\n    return df.sort_values(by=by, key=_custom_order, ascending=ascending)\n</code></pre>"},{"location":"reference/evals/plots/_base/#evals.plots._base.ESMChart.to_html","title":"<code>to_html(output_path, groupby, idx)</code>","text":"<p>Serialize the Plotly figure to an HTML file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>pathlib.Path</code> <p>The folder to save the HTML file under.</p> required <code>groupby</code> <code>list[str]</code> <p>List of groupby keys needed to fill the file name template.</p> required <code>idx</code> <code>typing.Hashable</code> <p>The data frame index from the gropuby clause needed to fill the file name template.</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>The path of the file written.</p> Source code in <code>evals/plots/_base.py</code> <pre><code>    def to_html(\n        self, output_path: pathlib.Path, groupby: list[str], idx: typing.Hashable\n    ) -&gt; pathlib.Path:\n        \"\"\"\n        Serialize the Plotly figure to an HTML file.\n\n        Parameters\n        ----------\n        output_path\n            The folder to save the HTML file under.\n        groupby\n            List of groupby keys needed to fill the file name template.\n        idx\n            The data frame index from the gropuby clause needed to fill\n            the file name template.\n\n        Returns\n        -------\n        :\n            The path of the file written.\n        \"\"\"\n        file_name = f\"{self.construct_file_name(groupby, idx)}.html\"\n        file_path = output_path / \"HTML\" / file_name\n        template_html = \"\"\"\\\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta charset=\"utf-8\" /&gt;\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /&gt;\n&lt;meta name=\"repo_name\" content=\"{{ repo_name }}\" /&gt;\n&lt;meta name=\"repo_branch\" content=\"{{ repo_branch }}\" /&gt;\n&lt;meta name=\"repo_hash\" content=\"{{ repo_hash }}\" /&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    {{ fig }}\n&lt;/body&gt;\n&lt;/html&gt;\"\"\"\n\n        div = self.fig.to_html(include_plotlyjs=\"directory\", full_html=False)\n        with file_path.open(\"w\", encoding=\"utf-8\") as fh:\n            fh.write(Template(template_html).render(fig=div, **RUN_META_DATA))\n\n        # need to write the plotly.js too, because to_html does not\n        bundle_path = file_path.parent / \"plotly.min.js\"\n        if not bundle_path.exists():\n            bundle_path.write_text(get_plotlyjs(), encoding=\"utf-8\")\n\n        return file_path\n</code></pre>"},{"location":"reference/evals/plots/_base/#evals.plots._base.ESMChart.to_json","title":"<code>to_json(output_path, groupby, idx)</code>","text":"<p>Serialize the Plotly figure to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>pathlib.Path</code> <p>The folder to save the JSON file under.</p> required <code>groupby</code> <code>list[str]</code> <p>List of groupby keys needed to fill the file name template.</p> required <code>idx</code> <code>typing.Hashable</code> <p>The data frame index from the gropuby clause needed to fill the file name template.</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>The path of the file written.</p> Source code in <code>evals/plots/_base.py</code> <pre><code>def to_json(\n    self, output_path: pathlib.Path, groupby: list[str], idx: typing.Hashable\n) -&gt; pathlib.Path:\n    \"\"\"\n    Serialize the Plotly figure to a JSON file.\n\n    Parameters\n    ----------\n    output_path\n        The folder to save the JSON file under.\n    groupby\n        List of groupby keys needed to fill the file name template.\n    idx\n        The data frame index from the gropuby clause needed to fill\n        the file name template.\n\n    Returns\n    -------\n    :\n        The path of the file written.\n    \"\"\"\n    file_name = f\"{self.construct_file_name(groupby, idx)}.json\"\n    file_path = output_path / \"JSON\" / file_name\n    self.fig.write_json(file_path, engine=\"auto\")\n\n    return file_path\n</code></pre>"},{"location":"reference/evals/plots/_base/#evals.plots._base.empty_figure","title":"<code>empty_figure(title)</code>","text":"<p>Return an empty graph with explanation text.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>The figure title displayed at the top of the graph.</p> required <p>Returns:</p> Type Description <code>plotly.graph_objects.Figure</code> <p>The plotly figure with a text that explains that there is no data available for this view.</p> Source code in <code>evals/plots/_base.py</code> <pre><code>def empty_figure(title: str) -&gt; go.Figure:\n    \"\"\"\n    Return an empty graph with explanation text.\n\n    Parameters\n    ----------\n    title\n        The figure title displayed at the top of the graph.\n\n    Returns\n    -------\n    :\n        The plotly figure with a text that explains that there is no\n        data available for this view.\n    \"\"\"\n    fig = px.bar(pd.DataFrame(), title=title)\n    fig.add_annotation(\n        text=\"No Values to be displayed\",\n        xref=\"paper\",\n        yref=\"paper\",\n        xanchor=\"center\",\n        yanchor=\"middle\",\n        x=0.5,\n        y=0.5,\n        showarrow=False,\n        font={\"size\": 20},\n    )\n    fig.update_xaxes(showgrid=False, showticklabels=False)\n    fig.update_yaxes(showgrid=False, showticklabels=False)\n    fig.update_layout(xaxis_title=\"\", yaxis_title=\"\", plot_bgcolor=\"white\")\n    fig.update_layout(meta=[RUN_META_DATA])\n\n    return fig\n</code></pre>"},{"location":"reference/evals/plots/barchart/","title":"barchart.py","text":"<p>ESM bar charts.</p>"},{"location":"reference/evals/plots/barchart/#evals.plots.barchart.ESMBarChart","title":"<code>ESMBarChart</code>","text":"<p>               Bases: <code>evals.plots._base.ESMChart</code></p> <p>The ESM Bar Chart exports metrics as plotly HTML file.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>object</code> <p>Positional arguments of the base class.</p> <code>()</code> <code>**kwargs</code> <code>object</code> <p>Key word arguments of the base class.</p> <code>{}</code> Source code in <code>evals/plots/barchart.py</code> <pre><code>class ESMBarChart(ESMChart):\n    \"\"\"\n    The ESM Bar Chart exports metrics as plotly HTML file.\n\n    Parameters\n    ----------\n    *args\n        Positional arguments of the base class.\n\n    **kwargs\n        Key word arguments of the base class.\n    \"\"\"\n\n    def __init__(self, *args: object, **kwargs: object) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.fig = go.Figure()\n\n        self.location = self._df.index.unique(DataModel.LOCATION)[0]\n        self.col_values = self._df.columns[0]\n\n    @cached_property\n    def barmode(self) -&gt; str:\n        \"\"\"\n        Determine the barmode for the bar chart.\n\n        Returns\n        -------\n        :\n            The barmode for the bar chart, either \"relative\" if there\n            are both negative and positive values, or \"stack\" if not.\n        \"\"\"\n        has_negatives = self._df.lt(0).to_numpy().any()\n        has_positives = self._df.ge(0).to_numpy().any()\n        return \"relative\" if has_negatives and has_positives else \"stack\"\n\n    @cached_property\n    def df(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Plot data formatted for bar charts.\n\n        Returns\n        -------\n        :\n            The formatted data for creating bar charts.\n        \"\"\"\n        df = apply_cutoff(self._df, limit=self.cfg.cutoff, drop=self.cfg.cutoff_drop)\n\n        df = self.custom_sort(\n            df.reset_index(),\n            by=self.cfg.plot_category,\n            values=self.cfg.category_orders,\n            ascending=True,\n        )\n        df[\"display_value\"] = df[self.col_values].apply(prettify_number)\n\n        return df\n\n    def plot(self) -&gt; None:\n        \"\"\"Create the bar chart.\"\"\"\n        title = self.cfg.title.format(location=self.location, unit=self.unit)\n\n        if self.empty_input or self.df[self.col_values].isna().all():\n            self.fig = empty_figure(title)\n            return\n\n        pattern = {\n            col: self.cfg.pattern.get(col, \"\")\n            for col in self.df[self.cfg.plot_category].unique()\n        }\n        self.fig = px.bar(\n            self.df,\n            color_discrete_map=self.cfg.colors,\n            pattern_shape=self.cfg.plot_category,\n            pattern_shape_map=pattern,\n            barmode=self.barmode,\n            x=self.cfg.plot_xaxis,\n            y=self.col_values,\n            color=self.cfg.plot_category,\n            text=self.col_values,\n            title=title,\n            labels={\n                self.col_values: \"&lt;b&gt;\" + self.unit + \"&lt;/b&gt;\",\n                self.cfg.plot_category: self.cfg.legend_header,\n            },\n            custom_data=[self.cfg.plot_category, \"display_value\"],\n        )\n\n        self._set_base_layout()\n        self._style_bars()\n        self._style_title_and_legend_and_xaxis_label()\n        self._append_footnotes()\n        self.fig.for_each_trace(self._set_legend_rank, selector={\"type\": \"bar\"})\n\n        # add total sum labels at the end of the bar(s)\n        if self.barmode == \"relative\":\n            self.fig.add_hline(y=0)  # visual separator between supply and withdrawal\n            self._add_total_sum_trace(\"Lower Sum\", orientation=\"down\")\n            self._add_total_sum_trace(\"Upper Sum\", orientation=\"up\")\n        else:\n            self._add_total_sum_trace(\"Sum\")\n\n    def _add_total_sum_trace(\n        self,\n        name_trace: str,\n        orientation: str = None,\n    ) -&gt; None:\n        \"\"\"\n        Create a scatter trace for total sum labels.\n\n        The label will show the total sum as text at the end of the bar\n        trace. The label will not be part of the legend.\n\n        Parameters\n        ----------\n        name_trace\n            The name of the trace useful to identify the trace in the\n            JSON representation.\n        orientation : optional, {'up', 'down', None}\n            The orientation of the trace used to choose the\n            text position and the sign of the values.\n        \"\"\"\n        sign = 1\n        if orientation == \"up\":\n            values = self.df[self.df[self.col_values].gt(0)]\n        elif orientation == \"down\":\n            sign = -1\n            values = self.df[self.df[self.col_values].le(0)]\n        else:  # barmode = stacked\n            values = self.df\n\n        totals = values.groupby(self.cfg.plot_xaxis).sum(numeric_only=True)\n        totals[\"display_value\"] = totals[self.col_values].apply(prettify_number)\n        y_offset = totals[self.col_values].abs().max() / 100 * sign\n\n        scatter = go.Scatter(\n            x=totals.index,\n            y=totals[self.col_values] + y_offset,\n            text=totals[\"display_value\"],\n            texttemplate=\"&lt;b&gt; %{text} \" + self.unit + \"&lt;/b&gt;\",\n            mode=\"text\",\n            textposition=f\"{'bottom' if orientation == 'down' else 'top'} center\",\n            showlegend=False,\n            name=name_trace,\n            textfont={\"size\": 18},\n            hoverinfo=\"skip\",\n        )\n\n        self.fig.add_trace(scatter)\n\n    def _style_bars(self) -&gt; None:\n        \"\"\"Update bar trace styles.\"\"\"\n        self.fig.update_traces(\n            selector={\"type\": \"bar\"},\n            width=0.6,\n            textposition=\"inside\",\n            insidetextanchor=\"middle\",\n            texttemplate=\"&lt;b&gt;%{customdata[1]}&lt;/b&gt;\",\n            insidetextfont={\"size\": 16},\n            textangle=0,\n            hovertemplate=\"%{customdata[0]}: %{customdata[1]} \" + self.unit,\n            hoverlabel={\"namelength\": 0},\n        )\n\n    def _set_legend_rank(self, trace: go.Bar) -&gt; go.Bar:\n        \"\"\"\n        Set the legendrank attribute for bar traces.\n\n        Only traces listed in the category_order\n        configuration item are considered.\n\n        Parameters\n        ----------\n        trace\n            The trace object to set the legendrank for.\n\n        Returns\n        -------\n        :\n            The updated bar trace, or the original bar trace.\n        \"\"\"\n        if trace[\"name\"] in self.cfg.category_orders:\n            y = trace[\"y\"]  # need to drop nan and inf or the sum may return NaN\n            trace_sum = y[np.isfinite(y)].sum()\n            sign = -1 if trace_sum &lt; 0 else 1\n            pos = self.cfg.category_orders.index(trace[\"name\"])\n            trace = trace.update(legendrank=1000 + pos * sign)  # 1000 = plotly default\n        return trace\n</code></pre>"},{"location":"reference/evals/plots/barchart/#evals.plots.barchart.ESMBarChart.barmode","title":"<code>barmode</code>  <code>cached</code> <code>property</code>","text":"<p>Determine the barmode for the bar chart.</p> <p>Returns:</p> Type Description <code>str</code> <p>The barmode for the bar chart, either \"relative\" if there are both negative and positive values, or \"stack\" if not.</p>"},{"location":"reference/evals/plots/barchart/#evals.plots.barchart.ESMBarChart.df","title":"<code>df</code>  <code>cached</code> <code>property</code>","text":"<p>Plot data formatted for bar charts.</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The formatted data for creating bar charts.</p>"},{"location":"reference/evals/plots/barchart/#evals.plots.barchart.ESMBarChart.plot","title":"<code>plot()</code>","text":"<p>Create the bar chart.</p> Source code in <code>evals/plots/barchart.py</code> <pre><code>def plot(self) -&gt; None:\n    \"\"\"Create the bar chart.\"\"\"\n    title = self.cfg.title.format(location=self.location, unit=self.unit)\n\n    if self.empty_input or self.df[self.col_values].isna().all():\n        self.fig = empty_figure(title)\n        return\n\n    pattern = {\n        col: self.cfg.pattern.get(col, \"\")\n        for col in self.df[self.cfg.plot_category].unique()\n    }\n    self.fig = px.bar(\n        self.df,\n        color_discrete_map=self.cfg.colors,\n        pattern_shape=self.cfg.plot_category,\n        pattern_shape_map=pattern,\n        barmode=self.barmode,\n        x=self.cfg.plot_xaxis,\n        y=self.col_values,\n        color=self.cfg.plot_category,\n        text=self.col_values,\n        title=title,\n        labels={\n            self.col_values: \"&lt;b&gt;\" + self.unit + \"&lt;/b&gt;\",\n            self.cfg.plot_category: self.cfg.legend_header,\n        },\n        custom_data=[self.cfg.plot_category, \"display_value\"],\n    )\n\n    self._set_base_layout()\n    self._style_bars()\n    self._style_title_and_legend_and_xaxis_label()\n    self._append_footnotes()\n    self.fig.for_each_trace(self._set_legend_rank, selector={\"type\": \"bar\"})\n\n    # add total sum labels at the end of the bar(s)\n    if self.barmode == \"relative\":\n        self.fig.add_hline(y=0)  # visual separator between supply and withdrawal\n        self._add_total_sum_trace(\"Lower Sum\", orientation=\"down\")\n        self._add_total_sum_trace(\"Upper Sum\", orientation=\"up\")\n    else:\n        self._add_total_sum_trace(\"Sum\")\n</code></pre>"},{"location":"reference/evals/plots/facetbars/","title":"facetbars.py","text":"<p>ESM grouped barcharts.</p>"},{"location":"reference/evals/plots/facetbars/#evals.plots.facetbars.ESMGroupedBarChart","title":"<code>ESMGroupedBarChart</code>","text":"<p>               Bases: <code>evals.plots._base.ESMChart</code></p> <p>A class that produces multiple bar charts in subplots.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>Positional arguments of the base class.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Key word arguments of the base class.</p> <code>{}</code> Source code in <code>evals/plots/facetbars.py</code> <pre><code>class ESMGroupedBarChart(ESMChart):\n    \"\"\"\n    A class that produces multiple bar charts in subplots.\n\n    Parameters\n    ----------\n    *args\n        Positional arguments of the base class.\n\n    **kwargs\n        Key word arguments of the base class.\n    \"\"\"\n\n    def __init__(self, *args: tuple, **kwargs: dict) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.location = self._df.index.unique(DataModel.LOCATION)[0]\n        self.col_values = self._df.columns[0]\n\n        # self.df is accessed below. location and col_values must be\n        # set before the first access to the df property.\n        ncols = len(self.df[DataModel.BUS_CARRIER].unique())\n        column_widths = [0.85 / ncols] * ncols\n        self.fig = make_subplots(\n            rows=1, cols=ncols, shared_yaxes=True, column_widths=column_widths\n        )\n\n    @cached_property\n    def df(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Plot data formatted for grouped bar charts.\n\n        Returns\n        -------\n        :\n            The formatted data for creating bar charts.\n        \"\"\"\n        df = apply_cutoff(self._df, limit=self.cfg.cutoff, drop=False)\n        df = df.reset_index()\n\n        # need to add missing carrier for every sector to prevent\n        # broken sort order. If a carrier is missing in, lets say the\n        # first subplot, the sort order will be different (compared)\n        # to a sector that has all carriers. To prevent this, we add\n        # missing dummy carriers before sorting.\n        fill_values = product(\n            df[DataModel.YEAR].unique(),\n            df[DataModel.LOCATION].unique(),\n            df[DataModel.CARRIER].unique(),\n            df[DataModel.BUS_CARRIER].unique(),\n        )\n        df_fill = pd.DataFrame(columns=DataModel.YEAR_IDX_NAMES, data=fill_values)\n        df_fill[self.col_values] = np.nan\n        df = pd.concat([df, df_fill], ignore_index=True)\n\n        # sort every sector in alphabetical order and separately to\n        # correctly align sectors and stacked carrier traces in bars\n        df_list = []\n        for _, df_sector in df.groupby(self.cfg.facet_column, sort=True):\n            sorted_sector = self.custom_sort(\n                df_sector,\n                by=self.cfg.plot_category,\n                values=self.cfg.category_orders,\n                ascending=True,\n            )\n            df_list.append(sorted_sector)\n        df = pd.concat(df_list)\n        # df = df.groupby(self.cfg.facet_column, sort=True).apply(\n        #     self.custom_sort,\n        #     by=self.cfg.plot_category,\n        #     values=self.cfg.category_orders,\n        #     ascending=True,\n        #     # include_groups=False,\n        # )\n        # remove NaN categories again after sorting with all categories\n        df = df.dropna(how=\"all\", subset=self.col_values)\n        df[\"display_value\"] = df[self.col_values].apply(prettify_number)\n\n        return df\n\n    def plot(self) -&gt; None:\n        \"\"\"Create the bar chart.\"\"\"\n        title = self.cfg.title.format(location=self.location, unit=self.unit)\n        if self.empty_input or self.df[self.col_values].isna().all():\n            self.fig = empty_figure(title)\n            return\n\n        pattern = {\n            col: self.cfg.pattern.get(col, \"\")\n            for col in self.df[self.cfg.plot_category].unique()\n        }\n\n        self.fig = px.bar(\n            self.df,\n            x=self.cfg.plot_xaxis,\n            y=self.col_values,\n            facet_col=self.cfg.facet_column,\n            facet_col_spacing=0.04,\n            pattern_shape=self.cfg.plot_category,\n            pattern_shape_map=pattern,\n            color=self.cfg.plot_category,\n            color_discrete_map=self.cfg.colors,\n            text=self.cfg.facet_column,  # needed to rename xaxis and dropped afterward\n            title=title,\n            custom_data=[self.cfg.plot_category, \"display_value\"],\n        )\n\n        self.fig.for_each_xaxis(self._rename_xaxis)\n        self.fig.for_each_xaxis(self._add_total_sum_subplot_traces)\n        self.fig.update_annotations(text=\"\")  # remove text labels\n\n        self._set_base_layout()\n        self._style_grouped_bars()\n        self._style_title_and_legend_and_xaxis_label()\n        self._append_footnotes()\n        self.fig.for_each_xaxis(self._style_inner_xaxis_labels)\n\n    def _rename_xaxis(self, xaxis: go.XAxis) -&gt; None:\n        \"\"\"\n        Update the xaxis labels.\n\n        The function iterates over subplot columns and looks for the\n        sector name in the figure data where the xaxis index matches\n        and updates the xaxis label and removes the upper text.\n\n        Parameters\n        ----------\n        xaxis\n            The subplot xaxis (a dictionary).\n\n        Notes\n        -----\n        A better way to set the xaxis labels is desirable. However, I\n        could not find a way to replace the 'year' string using the\n        'label' argument in plotly.express.bar(), because all columns\n        are named the same (='year') and the label argument only\n        maps old to new names in a dictionary.\n        \"\"\"\n        layout = self.fig[\"layout\"]\n        idx = xaxis[\"anchor\"].lstrip(\"y\")\n        for data in self.fig[\"data\"]:\n            if data[\"xaxis\"] == f\"x{idx}\":\n                sector = data[\"text\"][0]\n                layout[f\"xaxis{idx}\"][\"title\"][\"text\"] = f\"&lt;b&gt;{sector}\"\n                break\n\n    def _style_inner_xaxis_labels(self, xaxis: go.XAxis) -&gt; None:\n        \"\"\"\n        Set the font size for the inner xaxis labels.\n\n        Parameters\n        ----------\n        xaxis\n            The subplot xaxis (a dictionary-like object).\n        \"\"\"\n        xaxis.update(\n            tickfont_size=self.cfg.xaxis_font_size, categoryorder=\"category ascending\"\n        )\n\n    def _style_grouped_bars(self) -&gt; None:\n        \"\"\"Style bar traces for grouped bar charts.\"\"\"\n        self.fig.update_traces(\n            selector={\"type\": \"bar\"},\n            width=0.8,\n            textposition=\"inside\",\n            insidetextanchor=\"middle\",\n            texttemplate=\"&lt;b&gt;%{customdata[1]}&lt;/b&gt;\",\n            textangle=0,\n            insidetextfont={\"size\": 16},\n            hovertemplate=\"%{customdata[0]}: %{customdata[1]} \" + self.unit,\n            hoverlabel={\"namelength\": 0},\n        )\n\n    def _add_total_sum_subplot_traces(self, xaxis: go.XAxis) -&gt; None:\n        \"\"\"\n        Add traces for total sum labels in every subplot.\n\n        The Xaxis is needed to parse the subplot position dynamically.\n        The method adds text annotations with the total amount of\n        energy per stacked bar.\n\n        Parameters\n        ----------\n        xaxis\n            The subplot xaxis (a dictionary-like object).\n        \"\"\"\n        idx = xaxis[\"anchor\"].lstrip(\"y\")\n        sector = xaxis[\"title\"][\"text\"].lstrip(\"&lt;b&gt;\")\n        values = self.df.query(f\"{self.cfg.facet_column} == '{sector}'\").copy()\n\n        values[\"pos\"] = values[self.col_values].where(values[self.col_values].gt(0))\n        values[\"neg\"] = values[self.col_values].where(values[self.col_values].le(0))\n\n        totals = values.groupby(self.cfg.plot_xaxis).sum(numeric_only=True)\n        totals[\"pos_display\"] = totals[\"pos\"].apply(prettify_number)\n        totals[\"neg_display\"] = totals[\"neg\"].apply(prettify_number)\n\n        if totals[\"pos\"].sum() &gt; 0:\n            scatter = go.Scatter(\n                x=totals.index,\n                y=totals[\"pos\"] + totals[\"pos\"].abs().max() / 100,\n                text=totals[\"pos_display\"],\n                texttemplate=\"&lt;b&gt;%{text}&lt;/b&gt;\",\n                mode=\"text\",\n                textposition=\"top center\",\n                showlegend=False,\n                name=\"Sum\",\n                textfont={\"size\": 18},\n                hoverinfo=\"skip\",\n            )\n\n            self.fig.add_trace(scatter, col=int(idx) if idx else 1, row=1)\n\n        if totals[\"neg\"].sum() &lt; 0:\n            scatter = go.Scatter(\n                x=totals.index,\n                y=totals[\"neg\"] - totals[\"neg\"].abs().max() / 100,\n                text=totals[\"neg_display\"],\n                texttemplate=\"&lt;b&gt;%{text}&lt;/b&gt;\",\n                mode=\"text\",\n                textposition=\"bottom center\",\n                showlegend=False,\n                name=\"Sum\",\n                textfont={\"size\": 18},\n                hoverinfo=\"skip\",\n            )\n            self.fig.add_trace(scatter, col=int(idx) if idx else 1, row=1)\n</code></pre>"},{"location":"reference/evals/plots/facetbars/#evals.plots.facetbars.ESMGroupedBarChart.df","title":"<code>df</code>  <code>cached</code> <code>property</code>","text":"<p>Plot data formatted for grouped bar charts.</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The formatted data for creating bar charts.</p>"},{"location":"reference/evals/plots/facetbars/#evals.plots.facetbars.ESMGroupedBarChart.plot","title":"<code>plot()</code>","text":"<p>Create the bar chart.</p> Source code in <code>evals/plots/facetbars.py</code> <pre><code>def plot(self) -&gt; None:\n    \"\"\"Create the bar chart.\"\"\"\n    title = self.cfg.title.format(location=self.location, unit=self.unit)\n    if self.empty_input or self.df[self.col_values].isna().all():\n        self.fig = empty_figure(title)\n        return\n\n    pattern = {\n        col: self.cfg.pattern.get(col, \"\")\n        for col in self.df[self.cfg.plot_category].unique()\n    }\n\n    self.fig = px.bar(\n        self.df,\n        x=self.cfg.plot_xaxis,\n        y=self.col_values,\n        facet_col=self.cfg.facet_column,\n        facet_col_spacing=0.04,\n        pattern_shape=self.cfg.plot_category,\n        pattern_shape_map=pattern,\n        color=self.cfg.plot_category,\n        color_discrete_map=self.cfg.colors,\n        text=self.cfg.facet_column,  # needed to rename xaxis and dropped afterward\n        title=title,\n        custom_data=[self.cfg.plot_category, \"display_value\"],\n    )\n\n    self.fig.for_each_xaxis(self._rename_xaxis)\n    self.fig.for_each_xaxis(self._add_total_sum_subplot_traces)\n    self.fig.update_annotations(text=\"\")  # remove text labels\n\n    self._set_base_layout()\n    self._style_grouped_bars()\n    self._style_title_and_legend_and_xaxis_label()\n    self._append_footnotes()\n    self.fig.for_each_xaxis(self._style_inner_xaxis_labels)\n</code></pre>"},{"location":"reference/evals/plots/gridmap/","title":"gridmap.py","text":"<p>Barchart organized in subplots (facets).</p>"},{"location":"reference/evals/plots/gridmap/#evals.plots.gridmap.GridMapConfig","title":"<code>GridMapConfig</code>  <code>dataclass</code>","text":"<p>Transmission grip map configuration.</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>@dataclass\nclass GridMapConfig:\n    \"\"\"Transmission grip map configuration.\"\"\"\n\n    # This layer will be visible by default\n    show_year: str = \"2030\"\n\n    crs: int = 4326\n\n    map_center: list = field(default_factory=lambda: [41.9, 15])\n\n    zoom_start: int = 5\n    zoom_min: int = 4\n    zoom_max: int = 8\n\n    bounds: dict = field(default_factory=lambda: {\"N\": 65, \"E\": 30, \"S\": 15, \"W\": -5})\n    max_bounds: bool = True  # cannot move map away from bounds\n\n    tile_provider: str = (\n        \"https://{s}.basemaps.cartocdn.com/light_nolabels/{z}/{x}/{y}.png\"\n    )\n\n    # required for copyright and licensing reasons\n    attribution: str = (\n        '&amp;copy; &lt;a href=\"https://www.openstreetmap.org/copyright\"&gt;OpenStreetMap&lt;/a&gt; '\n        'contributors, &amp;copy; &lt;a href=\"https://cartodb.com/attributions\"&gt;CartoDB&lt;/a&gt;'\n    )\n\n    # grid lines\n    line_capacity_threshold: float = 0.1  # GWh\n    line_weight_min: float = 2.0  # px\n    line_weight_max: float = 20.0  # px\n    # ToDo: Add colors\n    carrier_style: dict = field(\n        default_factory=lambda: {\n            \"AC\": {  # AC Lines\n                \"color\": \"#D90429\",\n                \"nice_name\": \"AC\",\n                \"offset\": -10,\n            },\n            \"DC\": {\n                \"color\": \"#E19990\",\n                \"nice_name\": \"DC\",\n                \"offset\": 10,  # px\n            },\n            \"gas pipeline\": {\n                \"color\": \"#63A452\",\n                \"nice_name\": \"Methane (brownfield)\",\n            },\n            \"gas pipeline new\": {\n                \"color\": \"#8BA352\",\n                \"nice_name\": \"Methane (new)\",\n            },\n            \"H2 pipeline\": {\n                \"color\": \"#258994\",\n                \"nice_name\": \"H2\",\n                \"offset\": -10,\n            },\n            \"H2 pipeline retrofitted\": {\n                \"color\": \"#255194\",\n                \"nice_name\": \"H2 (retrofitted)\",\n                \"dash_array\": \"10\",  # px, equal gaps\n                \"offset\": 10,  # px\n            },\n            \"H2 pipeline (Kernnetz)\": {\n                \"color\": \"#259468\",\n                \"nice_name\": \"H2 (Kernnetz)\",\n                \"dash_array\": \"10\",  # px, equal gaps\n                \"offset\": 10,  # px\n            },\n        }\n    )\n</code></pre>"},{"location":"reference/evals/plots/gridmap/#evals.plots.gridmap.TransmissionGridMap","title":"<code>TransmissionGridMap</code>","text":"<p>Creates a map with transmission capacities.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>pandas.DataFrame</code> <p>A data frame with the transmission grid capacities.</p> required <code>import_energy</code> <code>pandas.DataFrame</code> <p>A data frame with global import energy import amounts and capacity.</p> required <code>import_capacity</code> <code>pandas.DataFrame</code> <p>A data frame with global import capacities.</p> required <code>buses</code> <code>pandas.DataFrame</code> <p>The buses data frame, used to determine the bus coordinates.</p> required <code>config</code> <code>evals.plots.gridmap.GridMapConfig</code> <p>The GridMapConfig object with configuration options.</p> required Source code in <code>evals/plots/gridmap.py</code> <pre><code>class TransmissionGridMap:\n    \"\"\"\n    Creates a map with transmission capacities.\n\n    Parameters\n    ----------\n    grid\n        A data frame with the transmission grid capacities.\n    import_energy\n        A data frame with global import energy import amounts and\n        capacity.\n    import_capacity\n        A data frame with global import capacities.\n    buses\n        The buses data frame, used to determine the bus coordinates.\n    config\n        The GridMapConfig object with configuration options.\n    \"\"\"\n\n    def __init__(\n        self,\n        grid: pd.DataFrame,\n        import_energy: pd.DataFrame,\n        import_capacity: pd.DataFrame,\n        buses: pd.DataFrame,\n        config: GridMapConfig,\n    ) -&gt; None:\n        self.grid = grid\n        self.import_energy = import_energy\n        self.import_capacity = import_capacity\n        self.buses = buses  # todo: do not uses buses from one year\n        self.cfg = config\n        self.fmap = folium.Map(\n            tiles=None,\n            location=self.cfg.map_center,\n            zoom_start=self.cfg.zoom_start,\n            max_bounds=self.cfg.max_bounds,\n            max_lat=self.cfg.bounds[\"N\"],\n            max_lon=self.cfg.bounds[\"E\"],\n            min_lat=self.cfg.bounds[\"S\"],\n            min_lon=self.cfg.bounds[\"W\"],\n        )\n\n        # feature groups are layers in the map and can be shown or hid\n        self.feature_groups = {}\n        for year in sorted(grid.index.unique(DataModel.YEAR)):\n            fg = folium.FeatureGroup(name=year, show=True)\n            self.feature_groups[year] = fg\n            self.fmap.add_child(fg)  # register the feature group\n\n    def save(\n        self, output_path: pathlib.Path, file_name: str, subdir: str = \"HTML\"\n    ) -&gt; None:\n        \"\"\"\n        Write the map to a html file.\n\n        We want to store the HTML inside the JSON folder by default,\n        because Folium does not support the import of JSON files.\n        Therefore, we dump HTML files and include them as iFrames in\n        the web UI instead of importing JSONs via the plotly library.\n\n        Parameters\n        ----------\n        output_path\n            The path to save the map in.\n        file_name\n            The name of the file to export the map to.\n        subdir\n            An optional subdirectory to store files at. Leave emtpy\n            to skip, or change to html.\n        \"\"\"\n        output_path = self.make_evaluation_result_directories(output_path, subdir)\n        self.fmap.save(output_path / f\"{file_name}.html\")\n\n    def make_evaluation_result_directories(\n        self, result_path: Path, subdir: Path | str\n    ) -&gt; Path:\n        \"\"\"\n        Create all directories needed to store evaluations results.\n\n        Parameters\n        ----------\n        result_path\n            The path of the result folder.\n        subdir\n            A relative path inside the result folder.\n\n        Returns\n        -------\n        :\n            The joined path: result_dir / subdir.\n        \"\"\"\n        output_path = self.make_directory(result_path, subdir)\n        output_path = self.make_directory(output_path, \"HTML\")\n\n        return output_path\n\n    def draw_grid_by_carrier_groups_myopic(self) -&gt; None:\n        \"\"\"Plot carrier groups for all years to one map.\"\"\"\n        self.add_basemap_layers()\n\n        plot_grid = self._calculate_line_weights(self.grid)\n\n        _groups = [DataModel.YEAR, \"bus0\", \"bus1\"]\n        year_edge = plot_grid.groupby(_groups, group_keys=False)\n\n        plot_grid = year_edge.apply(self._calculate_line_offset)\n\n        grid_line = plot_grid.groupby(plot_grid.index.names, group_keys=False)\n        plot_grid = grid_line.apply(self._calculate_line_center)\n\n        plot_grid.groupby([DataModel.YEAR, DataModel.CARRIER]).apply(\n            self._draw_grid_polyline_with_circle_marker\n        )\n\n        self.draw_country_markers()\n        # self.draw_import_locations()\n        self.add_control_widgets()\n\n    def add_control_widgets(self) -&gt; None:\n        \"\"\"Add UI elements to the map.\"\"\"\n        plugins.GroupedLayerControl(\n            groups={\"Year\": list(self.feature_groups.values())},\n            collapsed=False,\n            position=\"topleft\",\n        ).add_to(self.fmap)\n\n        plugins.Fullscreen(\n            position=\"topright\",\n            title=\"Full Screen\",\n            title_cancel=\"Exit Full Screen\",\n            force_separate_button=True,\n        ).add_to(self.fmap)\n\n    def draw_import_locations(self) -&gt; None:\n        \"\"\"\n        Add import location icons and lines to the map.\n\n        Notes\n        -----\n        Available icons: https://fontawesome.com/icons/categories\n        \"\"\"\n        icon_locations = {\n            # node: [y, x]  Lat, Lon\n            \"BE0 0\": [51.21868, 2.86993],\n            \"DE0 4\": [53.92445, 8.67684],\n            \"EE6 0\": [58.78505, 23.15726],\n            \"ES0 0\": [43.41430, -4.27864],\n            \"FI3 0\": [60.89107, 22.68793],\n            \"FR0 0\": [48.18790, -3.68987],\n            \"GB5 0\": [54.63442, -0.70133],\n            \"GR0 0\": [38.67136, 26.65004],\n            \"HU0 0\": [48.20228, 22.60233],\n            \"IT0 0\": [37.16396, 13.49807],\n            \"LT6 0\": [55.71138, 21.07711],\n            \"LV6 0\": [56.99505, 27.72035],\n            \"NL0 0\": [53.03334, 4.96787],\n            \"NO3 0\": [60.05473, 5.00377],\n            \"PL0 0\": [51.99998, 22.13991],\n            \"PT0 0\": [37.98446, -8.88731],\n            \"RO0 0\": [44.51848, 28.89059],\n            \"SK0 0\": [48.78314, 22.35254],\n        }\n\n        _idx = self.import_capacity.index.names\n        row_slices = self.import_capacity.to_frame().groupby(_idx, group_keys=False)\n        import_capacity = row_slices.apply(self._calculate_line_weights)\n\n        for (year, node), capas in import_capacity.groupby(\n            [DataModel.YEAR, DataModel.LOCATION]\n        ):\n            fg = self.feature_groups[year]\n            # need a new instance for every icon\n            # icon = self._get_icon(RIGHT_TO_BRACKET_SOLID)\n            popup_table = filter_by(self.import_energy, year=year, location=node)\n            popup_table = popup_table.droplevel(\n                [DataModel.YEAR, DataModel.CARRIER, DataModel.BUS_CARRIER]\n            )\n            bootstrap5_classes = (\n                \"table table-striped table-hover table-condensed table-responsive\"\n            )\n            popup_html = popup_table.to_frame().to_html(classes=bootstrap5_classes)\n            folium.Marker(\n                location=icon_locations[node],\n                # icon=folium.CustomIcon(icon, icon_size=(10, 10)),\n                popup=folium.Popup(popup_html),\n                tooltip=\"Global Import\",\n            ).add_to(fg)\n\n            # draw line from import icon location to node location\n            node_y = self.buses.loc[node, \"y\"]\n            node_x = self.buses.loc[node, \"x\"]\n            capacity = capas[self.import_capacity.name].iloc[0]\n            label = f\"{capas.attrs['name']}: {capacity:.2f} {capas.attrs['unit']}\"\n            folium.PolyLine(\n                locations=[icon_locations[node], [node_y, node_x]],\n                color=\"black\",\n                weight=capas[\"line_weight\"].iloc[0],\n                tooltip=label,\n                popup=label,\n            ).add_to(fg)\n\n    def add_basemap_layers(self) -&gt; None:\n        \"\"\"Add common background layer to the map.\"\"\"\n        self._add_wms_tiles()\n        self._load_geojson(\n            \"regions_onshore_base_s_adm.geojson\",\n            style={\n                \"weight\": 1,\n                \"color\": \"grey\",\n                \"fillColor\": \"white\",\n                \"opacity\": 0.5,\n            },\n        )\n\n        # self._load_geojson(\n        #     \"neighbors.geojson\",\n        #     style={\n        #         \"weight\": 0.5,\n        #         \"color\": \"black\",\n        #         \"fillColor\": \"black\",\n        #         \"opacity\": 0.2,\n        #     },\n        # )\n\n    def draw_country_markers(self) -&gt; None:\n        \"\"\"\n        Draw markers for countries on the map.\n\n        Retrieves bus information from networks, iterates over unique\n        bus locations, creates CircleMarker and Marker objects for\n        each location with corresponding short and nice names, and\n        adds them to the map.\n        \"\"\"\n        fg_labels = folium.FeatureGroup(\n            name=\"Country Marker\", overlays=True, interactive=False\n        )\n\n        buses0 = self.grid.index.unique(\"bus0\")\n        buses1 = self.grid.index.unique(\"bus1\")\n        icon_css = \"margin-top:1.5px; font-size:10px; font-family:sans-serif\"\n\n        for bus in buses0.union(buses1):\n            # keep region ID for AT and DE, else just the country code\n            short_name = bus[:2] + bus[-1] if bus.startswith((\"AT\", \"DE\")) else bus[:2]\n            nice_name = ALIAS_REGION.get(bus, ALIAS_COUNTRY[bus[:2]])\n            location = self.buses.loc[bus, [\"y\", \"x\"]].to_numpy()\n\n            icon = plugins.BeautifyIcon(\n                icon_shape=\"circle\",\n                border_width=2,\n                border_color=\"black\",\n                # background_color=\"white\",\n                text_color=\"black\",\n                inner_icon_style=icon_css,\n                number=short_name,\n            )\n            marker = folium.Marker(location=location, popup=nice_name, icon=icon)\n            marker.add_to(fg_labels)\n\n        fg_labels.add_to(self.fmap)\n\n    @staticmethod\n    def _get_icon(icon: str) -&gt; str:\n        \"\"\"\n        Encode a raw SVG string to bytes.\n\n        Parameters\n        ----------\n        icon\n            The utf-8 encoded HTML representation of an SVG icon.\n\n        Returns\n        -------\n        :\n            The base64 encoded SVG icon as a string.\n        \"\"\"\n        data = base64.b64encode(icon.strip().encode(\"utf-8\")).decode(\"utf-8\")\n        return f\"data:image/svg+xml;base64,{data}\"\n\n    def _calculate_line_weights(self, df_slice: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate the line weights for a grid.\n\n        Parameters\n        ----------\n        df_slice\n            The grids that will be plotted to the map.\n\n        Returns\n        -------\n        :\n            The grids with an additional column for the line weight in px.\n        \"\"\"\n        # prevent assignment to copies of a data view\n        df_slice = df_slice.copy()\n\n        col = f\"{df_slice.attrs['name']} ({df_slice.attrs['unit']})\"\n\n        _min, _max = df_slice[col].min(), df_slice[col].max()\n        _max_width = self.cfg.line_weight_max  # 20.0\n        _min_width = self.cfg.line_weight_min  # 2.0\n\n        def linear_scale(ser: pd.Series) -&gt; pd.Series:\n            \"\"\"\n            Scale values between lower and upper line weight values.\n\n            Returns the linear equation k * x + d. Where x is the ratio,\n            k is the min-max range and d the lower bound constant.\n\n            Parameters\n            ----------\n            ser\n                The values to be scaled between 2 and 5.\n\n            Returns\n            -------\n            The scaled value used as line width in pixel.\n            \"\"\"\n            min_max_ratio = (ser - _min) / (_max - _min)\n            return min_max_ratio * (_max_width - _min_width) + _min_width\n\n        if _min == _max:\n            df_slice.loc[:, \"line_weight\"] = self.cfg.line_weight_min\n        else:\n            df_slice.loc[:, \"line_weight\"] = df_slice[col].apply(linear_scale)\n\n        return df_slice\n\n    @staticmethod\n    def _calculate_line_offset(df_slice: pd.DataFrame, gap: float = 1) -&gt; pd.DataFrame:\n        \"\"\"\n        Add a column with the offset values for a grid.\n\n        The offset is used to prevent overplotting lines and labels.\n        It is only required if multiple edges exist between the same\n        nodes.\n\n        Parameters\n        ----------\n        df_slice\n            A data frame slice for every unique node connection\n            and for all displayed carriers in a map.\n        gap : float, optional\n            Number of pixels to insert between the edges of adjacent lines\n            (default is 1 px), preventing strokes from touching.\n\n        Returns\n        -------\n        :\n            The input data slice with the offset in pixel in an\n            additional column.\n        \"\"\"\n        # if df_slice.shape[0] == 1:\n        #     df_slice[\"offset\"] = 0\n        # elif df_slice.shape[0] == 2:\n        #     # move lines (down and up) by half their combined line\n        #     # weights plus 1 px for a visible gap\n        #     half_weight = df_slice[\"line_weight\"].sum() / 2 + 1\n        #     df_slice[\"offset\"] = [-0.5 * half_weight, 0.5 * half_weight]\n        # else:\n        #     raise NotImplementedError(f\"Number of rows: {df_slice} not supported.\")\n        #\n        # return df_slice\n        weights = df_slice[\"line_weight\"].astype(float).tolist()\n        n = len(weights)\n\n        # Total envelope width = sum of all stroke widths + (n-1) gaps\n        total_width = sum(weights) + (n - 1) * gap\n\n        # Start at left edge of that envelope\n        current = -total_width / 2\n        offsets = []\n\n        # For each stroke:\n        #   \u2022 move by half its width  \u2192 centerline of this band\n        #   \u2022 record that as the offset\n        #   \u2022 then advance by (half its width + gap) to get to the next band\u2019s start\n        for w in weights:\n            current += w / 2\n            offsets.append(current)\n            current += w / 2 + gap\n\n        # Attach offsets and return\n        df_slice[\"offset\"] = offsets\n        return df_slice\n\n    @staticmethod\n    def _calculate_line_center(df_slice: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate the line center for all lines.\n\n        In case the line has an offset, the center is moved by 10%\n        of the line length and along the line in the direction of the\n        offset.\n\n        Parameters\n        ----------\n        df_slice\n            The data frame with a \"line\" column that contains\n            coordinate pairs and an \"offset\" column that contains\n            a positive or negative float for the line offset.\n\n        Returns\n        -------\n        :\n            The input data frame with additional column containing the\n            line center.\n        \"\"\"\n\n        def compute_center(row):\n            offset = row[\"offset\"]\n            line = row[\"line\"]\n            if offset != 0:\n                x0, x1 = line[0][0], line[1][0]\n                y0, y1 = line[0][1], line[1][1]\n                # Move center by +-10% of line length depending on offset sign.\n                ratio = 0.5 + copysign(0.1, offset)\n                x = x0 + ratio * (x1 - x0)\n                y = y0 + ratio * (y1 - y0)\n                return [x, y]\n            else:\n                # Compute the simple midpoint\n                return [(line[0][i] + line[1][i]) / 2 for i in range(len(line[0]))]\n\n        df_slice[\"line_center\"] = df_slice.apply(compute_center, axis=1)\n\n        return df_slice\n\n    def _draw_grid_polyline_with_circle_marker(self, grid: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Draw grid lines on the map for a specific carrier.\n\n        Retrieves the nice name, color, and dash array configuration\n        for the carrier. Iterates over grid data, creates PolyLine\n        objects for capacities larger than the threshold in the config,\n        sets color, weight, dash array, tooltip, and popup based on the\n        capacity, and adds them to the FeatureGroup.\n\n        Parameters\n        ----------\n        grid\n            The metric dataframe with the capacities and the lines.\n        \"\"\"\n        year = grid.index.unique(DataModel.YEAR)[0]\n        carrier = grid.index.unique(DataModel.CARRIER)[0]\n\n        fg = self.feature_groups[year]\n        style = self.cfg.carrier_style[carrier]\n        nice_name = style[\"nice_name\"]\n        color = style[\"color\"]\n        unit = grid.attrs[\"unit\"]\n\n        col = f\"{grid.attrs['name']} ({grid.attrs['unit']})\"\n        significant_edges = grid[grid[col] &gt;= self.cfg.line_capacity_threshold]\n\n        for _, row in significant_edges.iterrows():\n            capacity = row[col]  # / 1000  # GW\n            tooltip = f\"{nice_name}: {capacity:.2f} {unit}\"\n\n            plugins.PolyLineOffset(\n                locations=row[\"line\"],\n                offset=row[\"offset\"],\n                color=color,\n                weight=row[\"line_weight\"],\n                dash_array=f\"{row['line_weight']}\" if style.get(\"dash_array\") else None,\n                # https://www.w3schools.com/graphics/svg_stroking.asp\n                line_cap=\"butt\",  # or \"round\"\n                tooltip=tooltip,\n                popup=tooltip,\n            ).add_to(fg)\n\n            # https://github.com/masajid390/BeautifyMarker\n            icon_css = \"margin-top:2.5px; font-size:10px; font-family:sans-serif\"\n            icon = plugins.BeautifyIcon(\n                icon_shape=\"circle\",\n                border_width=1,\n                border_color=color,\n                background_color=\"white\",\n                text_color=color,\n                number=prettify_number(capacity),\n                inner_icon_style=icon_css,\n            )\n\n            folium.Marker(\n                location=row[\"line_center\"],\n                popup=folium.Popup(nice_name),\n                icon=icon,\n                tooltip=tooltip,\n            ).add_to(fg)\n\n    def _add_wms_tiles(self) -&gt; None:\n        \"\"\"Add a web map tile service layer to the map.\"\"\"\n        folium.TileLayer(\n            name=\"WMS tiles\",\n            control=False,  # no layer controls\n            tiles=self.cfg.tile_provider,\n            attr=self.cfg.attribution,\n            min_zoom=self.cfg.zoom_min,\n            max_zoom=self.cfg.zoom_max,\n            # additional leaflet.js args: https://leafletjs.com/reference.html#tilelayer\n        ).add_to(self.fmap)\n\n    def _load_geojson(self, file_name: str, style: dict = None) -&gt; None:\n        \"\"\"\n        Add the geojson layer.\n\n        Parameters\n        ----------\n        file_name\n            The name of the geojson file under esmtools/data.\n        style\n            The style dictionary to pass to the geojson layer.\n        \"\"\"\n        # res = resources.files(\"evals\") / \"data\"\n        # gdf = gpd.read_file(res / file_name).to_crs(crs=f\"EPSG:{self.cfg.crs}\")\n        gdf = gpd.read_file(Path(\"resources\") / file_name).to_crs(\n            crs=f\"EPSG:{self.cfg.crs}\"\n        )\n        if style:  # applies the same style to all features\n            gdf[\"style\"] = [style] * gdf.shape[0]\n\n        gj = GeoJson(gdf, control=False, overlay=True)\n        gj.add_to(self.fmap)\n\n    @staticmethod\n    def make_directory(base: Path, subdir: Path | str) -&gt; Path:\n        \"\"\"\n        Create a directory and return its path.\n\n        Parameters\n        ----------\n        base\n            The path to base of the new folder.\n        subdir\n            A relative path inside the base folder.\n\n        Returns\n        -------\n        :\n            The joined path: result_dir / subdir / now.\n        \"\"\"\n        base = Path(base).resolve()\n        assert base.is_dir(), f\"Base path does not exist: {base}.\"\n        directory_path = base / subdir\n        directory_path.mkdir(parents=True, exist_ok=True)\n\n        return directory_path\n</code></pre>"},{"location":"reference/evals/plots/gridmap/#evals.plots.gridmap.TransmissionGridMap.add_basemap_layers","title":"<code>add_basemap_layers()</code>","text":"<p>Add common background layer to the map.</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>def add_basemap_layers(self) -&gt; None:\n    \"\"\"Add common background layer to the map.\"\"\"\n    self._add_wms_tiles()\n    self._load_geojson(\n        \"regions_onshore_base_s_adm.geojson\",\n        style={\n            \"weight\": 1,\n            \"color\": \"grey\",\n            \"fillColor\": \"white\",\n            \"opacity\": 0.5,\n        },\n    )\n</code></pre>"},{"location":"reference/evals/plots/gridmap/#evals.plots.gridmap.TransmissionGridMap.add_control_widgets","title":"<code>add_control_widgets()</code>","text":"<p>Add UI elements to the map.</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>def add_control_widgets(self) -&gt; None:\n    \"\"\"Add UI elements to the map.\"\"\"\n    plugins.GroupedLayerControl(\n        groups={\"Year\": list(self.feature_groups.values())},\n        collapsed=False,\n        position=\"topleft\",\n    ).add_to(self.fmap)\n\n    plugins.Fullscreen(\n        position=\"topright\",\n        title=\"Full Screen\",\n        title_cancel=\"Exit Full Screen\",\n        force_separate_button=True,\n    ).add_to(self.fmap)\n</code></pre>"},{"location":"reference/evals/plots/gridmap/#evals.plots.gridmap.TransmissionGridMap.draw_country_markers","title":"<code>draw_country_markers()</code>","text":"<p>Draw markers for countries on the map.</p> <p>Retrieves bus information from networks, iterates over unique bus locations, creates CircleMarker and Marker objects for each location with corresponding short and nice names, and adds them to the map.</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>def draw_country_markers(self) -&gt; None:\n    \"\"\"\n    Draw markers for countries on the map.\n\n    Retrieves bus information from networks, iterates over unique\n    bus locations, creates CircleMarker and Marker objects for\n    each location with corresponding short and nice names, and\n    adds them to the map.\n    \"\"\"\n    fg_labels = folium.FeatureGroup(\n        name=\"Country Marker\", overlays=True, interactive=False\n    )\n\n    buses0 = self.grid.index.unique(\"bus0\")\n    buses1 = self.grid.index.unique(\"bus1\")\n    icon_css = \"margin-top:1.5px; font-size:10px; font-family:sans-serif\"\n\n    for bus in buses0.union(buses1):\n        # keep region ID for AT and DE, else just the country code\n        short_name = bus[:2] + bus[-1] if bus.startswith((\"AT\", \"DE\")) else bus[:2]\n        nice_name = ALIAS_REGION.get(bus, ALIAS_COUNTRY[bus[:2]])\n        location = self.buses.loc[bus, [\"y\", \"x\"]].to_numpy()\n\n        icon = plugins.BeautifyIcon(\n            icon_shape=\"circle\",\n            border_width=2,\n            border_color=\"black\",\n            # background_color=\"white\",\n            text_color=\"black\",\n            inner_icon_style=icon_css,\n            number=short_name,\n        )\n        marker = folium.Marker(location=location, popup=nice_name, icon=icon)\n        marker.add_to(fg_labels)\n\n    fg_labels.add_to(self.fmap)\n</code></pre>"},{"location":"reference/evals/plots/gridmap/#evals.plots.gridmap.TransmissionGridMap.draw_grid_by_carrier_groups_myopic","title":"<code>draw_grid_by_carrier_groups_myopic()</code>","text":"<p>Plot carrier groups for all years to one map.</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>def draw_grid_by_carrier_groups_myopic(self) -&gt; None:\n    \"\"\"Plot carrier groups for all years to one map.\"\"\"\n    self.add_basemap_layers()\n\n    plot_grid = self._calculate_line_weights(self.grid)\n\n    _groups = [DataModel.YEAR, \"bus0\", \"bus1\"]\n    year_edge = plot_grid.groupby(_groups, group_keys=False)\n\n    plot_grid = year_edge.apply(self._calculate_line_offset)\n\n    grid_line = plot_grid.groupby(plot_grid.index.names, group_keys=False)\n    plot_grid = grid_line.apply(self._calculate_line_center)\n\n    plot_grid.groupby([DataModel.YEAR, DataModel.CARRIER]).apply(\n        self._draw_grid_polyline_with_circle_marker\n    )\n\n    self.draw_country_markers()\n    # self.draw_import_locations()\n    self.add_control_widgets()\n</code></pre>"},{"location":"reference/evals/plots/gridmap/#evals.plots.gridmap.TransmissionGridMap.draw_import_locations","title":"<code>draw_import_locations()</code>","text":"<p>Add import location icons and lines to the map.</p> Notes <p>Available icons: https://fontawesome.com/icons/categories</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>def draw_import_locations(self) -&gt; None:\n    \"\"\"\n    Add import location icons and lines to the map.\n\n    Notes\n    -----\n    Available icons: https://fontawesome.com/icons/categories\n    \"\"\"\n    icon_locations = {\n        # node: [y, x]  Lat, Lon\n        \"BE0 0\": [51.21868, 2.86993],\n        \"DE0 4\": [53.92445, 8.67684],\n        \"EE6 0\": [58.78505, 23.15726],\n        \"ES0 0\": [43.41430, -4.27864],\n        \"FI3 0\": [60.89107, 22.68793],\n        \"FR0 0\": [48.18790, -3.68987],\n        \"GB5 0\": [54.63442, -0.70133],\n        \"GR0 0\": [38.67136, 26.65004],\n        \"HU0 0\": [48.20228, 22.60233],\n        \"IT0 0\": [37.16396, 13.49807],\n        \"LT6 0\": [55.71138, 21.07711],\n        \"LV6 0\": [56.99505, 27.72035],\n        \"NL0 0\": [53.03334, 4.96787],\n        \"NO3 0\": [60.05473, 5.00377],\n        \"PL0 0\": [51.99998, 22.13991],\n        \"PT0 0\": [37.98446, -8.88731],\n        \"RO0 0\": [44.51848, 28.89059],\n        \"SK0 0\": [48.78314, 22.35254],\n    }\n\n    _idx = self.import_capacity.index.names\n    row_slices = self.import_capacity.to_frame().groupby(_idx, group_keys=False)\n    import_capacity = row_slices.apply(self._calculate_line_weights)\n\n    for (year, node), capas in import_capacity.groupby(\n        [DataModel.YEAR, DataModel.LOCATION]\n    ):\n        fg = self.feature_groups[year]\n        # need a new instance for every icon\n        # icon = self._get_icon(RIGHT_TO_BRACKET_SOLID)\n        popup_table = filter_by(self.import_energy, year=year, location=node)\n        popup_table = popup_table.droplevel(\n            [DataModel.YEAR, DataModel.CARRIER, DataModel.BUS_CARRIER]\n        )\n        bootstrap5_classes = (\n            \"table table-striped table-hover table-condensed table-responsive\"\n        )\n        popup_html = popup_table.to_frame().to_html(classes=bootstrap5_classes)\n        folium.Marker(\n            location=icon_locations[node],\n            # icon=folium.CustomIcon(icon, icon_size=(10, 10)),\n            popup=folium.Popup(popup_html),\n            tooltip=\"Global Import\",\n        ).add_to(fg)\n\n        # draw line from import icon location to node location\n        node_y = self.buses.loc[node, \"y\"]\n        node_x = self.buses.loc[node, \"x\"]\n        capacity = capas[self.import_capacity.name].iloc[0]\n        label = f\"{capas.attrs['name']}: {capacity:.2f} {capas.attrs['unit']}\"\n        folium.PolyLine(\n            locations=[icon_locations[node], [node_y, node_x]],\n            color=\"black\",\n            weight=capas[\"line_weight\"].iloc[0],\n            tooltip=label,\n            popup=label,\n        ).add_to(fg)\n</code></pre>"},{"location":"reference/evals/plots/gridmap/#evals.plots.gridmap.TransmissionGridMap.make_directory","title":"<code>make_directory(base, subdir)</code>  <code>staticmethod</code>","text":"<p>Create a directory and return its path.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>pathlib.Path</code> <p>The path to base of the new folder.</p> required <code>subdir</code> <code>pathlib.Path | str</code> <p>A relative path inside the base folder.</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>The joined path: result_dir / subdir / now.</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>@staticmethod\ndef make_directory(base: Path, subdir: Path | str) -&gt; Path:\n    \"\"\"\n    Create a directory and return its path.\n\n    Parameters\n    ----------\n    base\n        The path to base of the new folder.\n    subdir\n        A relative path inside the base folder.\n\n    Returns\n    -------\n    :\n        The joined path: result_dir / subdir / now.\n    \"\"\"\n    base = Path(base).resolve()\n    assert base.is_dir(), f\"Base path does not exist: {base}.\"\n    directory_path = base / subdir\n    directory_path.mkdir(parents=True, exist_ok=True)\n\n    return directory_path\n</code></pre>"},{"location":"reference/evals/plots/gridmap/#evals.plots.gridmap.TransmissionGridMap.make_evaluation_result_directories","title":"<code>make_evaluation_result_directories(result_path, subdir)</code>","text":"<p>Create all directories needed to store evaluations results.</p> <p>Parameters:</p> Name Type Description Default <code>result_path</code> <code>pathlib.Path</code> <p>The path of the result folder.</p> required <code>subdir</code> <code>pathlib.Path | str</code> <p>A relative path inside the result folder.</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>The joined path: result_dir / subdir.</p> Source code in <code>evals/plots/gridmap.py</code> <pre><code>def make_evaluation_result_directories(\n    self, result_path: Path, subdir: Path | str\n) -&gt; Path:\n    \"\"\"\n    Create all directories needed to store evaluations results.\n\n    Parameters\n    ----------\n    result_path\n        The path of the result folder.\n    subdir\n        A relative path inside the result folder.\n\n    Returns\n    -------\n    :\n        The joined path: result_dir / subdir.\n    \"\"\"\n    output_path = self.make_directory(result_path, subdir)\n    output_path = self.make_directory(output_path, \"HTML\")\n\n    return output_path\n</code></pre>"},{"location":"reference/evals/plots/gridmap/#evals.plots.gridmap.TransmissionGridMap.save","title":"<code>save(output_path, file_name, subdir='HTML')</code>","text":"<p>Write the map to a html file.</p> <p>We want to store the HTML inside the JSON folder by default, because Folium does not support the import of JSON files. Therefore, we dump HTML files and include them as iFrames in the web UI instead of importing JSONs via the plotly library.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>pathlib.Path</code> <p>The path to save the map in.</p> required <code>file_name</code> <code>str</code> <p>The name of the file to export the map to.</p> required <code>subdir</code> <code>str</code> <p>An optional subdirectory to store files at. Leave emtpy to skip, or change to html.</p> <code>'HTML'</code> Source code in <code>evals/plots/gridmap.py</code> <pre><code>def save(\n    self, output_path: pathlib.Path, file_name: str, subdir: str = \"HTML\"\n) -&gt; None:\n    \"\"\"\n    Write the map to a html file.\n\n    We want to store the HTML inside the JSON folder by default,\n    because Folium does not support the import of JSON files.\n    Therefore, we dump HTML files and include them as iFrames in\n    the web UI instead of importing JSONs via the plotly library.\n\n    Parameters\n    ----------\n    output_path\n        The path to save the map in.\n    file_name\n        The name of the file to export the map to.\n    subdir\n        An optional subdirectory to store files at. Leave emtpy\n        to skip, or change to html.\n    \"\"\"\n    output_path = self.make_evaluation_result_directories(output_path, subdir)\n    self.fmap.save(output_path / f\"{file_name}.html\")\n</code></pre>"},{"location":"reference/evals/plots/sankey/","title":"Sankey","text":"<p>Module for Sankey diagram.</p>"},{"location":"reference/evals/plots/timeseries/","title":"timeseries.py","text":"<p>ESM time series scatter plots.</p>"},{"location":"reference/evals/plots/timeseries/#evals.plots.timeseries.ESMTimeSeriesChart","title":"<code>ESMTimeSeriesChart</code>","text":"<p>               Bases: <code>evals.plots._base.ESMChart</code></p> <p>A class that produces one time series chart.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>Positional arguments of the base class.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Key word arguments of the base class.</p> <code>{}</code> Source code in <code>evals/plots/timeseries.py</code> <pre><code>class ESMTimeSeriesChart(ESMChart):\n    \"\"\"\n    A class that produces one time series chart.\n\n    Parameters\n    ----------\n    *args\n        Positional arguments of the base class.\n\n    **kwargs\n        Key word arguments of the base class.\n    \"\"\"\n\n    def __init__(self, *args: tuple, **kwargs: dict) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.fig = go.Figure()\n        self.year = self._df.index.unique(\"year\")[0]\n        self.yaxes_showgrid = self.yaxes_visible = True\n        self.location = self._df.index.unique(DataModel.LOCATION)[0]\n\n    @cached_property\n    def df(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Plot data formatted for time series charts.\n\n        Returns\n        -------\n        :\n            The formatted data for creating bar charts.\n        \"\"\"\n        df = apply_cutoff(self._df, limit=self.cfg.cutoff, drop=self.cfg.cutoff_drop)\n        df = self.custom_sort(\n            df, by=self.cfg.plot_category, values=self.cfg.category_orders\n        )\n        df = self.fix_snapshots(df, int(self.year))\n        df = df.droplevel([DataModel.YEAR, DataModel.LOCATION])\n\n        return df.T  # transpose to iterate column wise over categories\n\n    def plot(self) -&gt; None:\n        \"\"\"\n        Plot the data to the chart.\n\n        This function iterates over the data series, adds traces to the\n        figure, styles the inflexible demand, sets the layout, styles\n        the title, legend, x-axis label, time series axes, and appends\n        footnotes.\n        \"\"\"\n        title = self.cfg.title.format(\n            location=self.location, year=self.year, unit=self.unit\n        )\n        if self.empty_input:\n            self.fig = empty_figure(title)\n            return\n\n        stackgroup = None\n        for i, (name, series) in enumerate(self.df.items()):\n            if self.cfg.stacked:\n                stackgroup = \"supply\" if series.sum() &gt;= 0 else \"withdrawal\"\n            legendrank = 1000 + i if stackgroup == \"supply\" else 1000 - i\n            self.fig.add_trace(\n                go.Scatter(\n                    x=series.index,\n                    y=series.values,\n                    hovertemplate=\"%{y:.2f} \" + self.unit,\n                    name=name,\n                    fill=self.cfg.fill.get(name, \"tonexty\"),\n                    fillpattern_shape=self.cfg.pattern.get(name),\n                    line_dash=self.cfg.line_dash.get(name, \"solid\"),\n                    line_width=self.cfg.line_width.get(name, 1),\n                    line_color=self.cfg.colors.get(name),\n                    line_shape=self.cfg.line_shape,\n                    fillcolor=self.cfg.colors.get(name),\n                    stackgroup=stackgroup,\n                    legendrank=legendrank,\n                )\n            )\n\n        self._style_inflexible_demand()\n        self._set_base_layout()\n        self._style_title_and_legend_and_xaxis_label()\n        self._style_time_series_axes_and_layout(title)\n        self._append_footnotes()\n\n    @staticmethod\n    def fix_snapshots(df: pd.DataFrame, year: int) -&gt; pd.DataFrame:\n        \"\"\"\n        Correct the year in snapshot timestamp column labels.\n\n        Parameters\n        ----------\n        df\n            The DataFrame with timestamps to be adjusted.\n        year\n            The correct year to use in the data frame columns.\n\n        Returns\n        -------\n        :\n            The DataFrame with corrected timestamps.\n        \"\"\"\n        if isinstance(df.columns, pd.DatetimeIndex):\n            df.columns = [s.replace(year=year) for s in df.columns]\n        return df\n\n    def _style_inflexible_demand(self) -&gt; None:\n        \"\"\"Set the inflexible demand style if it exists.\"\"\"\n        self.fig.update_traces(\n            selector={\"name\": \"Inflexible Demand\"},\n            fillcolor=None,\n            fill=None,\n            stackgroup=None,\n            legendrank=2000,  # first entry in legend (from top)\n        )\n\n    def _style_time_series_axes_and_layout(self, title) -&gt; None:\n        \"\"\"\n        Update the layout and axes for time series charts.\n\n        Parameters\n        ----------\n        title\n            The figure title to show at the top of the graph.\n        \"\"\"\n        self.fig.update_yaxes(\n            tickprefix=\"&lt;b&gt;\",\n            ticksuffix=\"&lt;/b&gt;\",\n            tickfont_size=15,\n            color=self.cfg.yaxis_color,\n            title_font_size=15,\n            tickformat=\".0f\",  # if \"TW\" in self.unit else \".3f\",\n            gridwidth=1,\n            gridcolor=\"gainsboro\",\n        )\n        self.fig.update_xaxes(ticklabelmode=\"period\")\n        self.fig.update_layout(\n            title=title,\n            yaxis_title=self.unit,\n            hovermode=\"x\",\n        )\n</code></pre>"},{"location":"reference/evals/plots/timeseries/#evals.plots.timeseries.ESMTimeSeriesChart.df","title":"<code>df</code>  <code>cached</code> <code>property</code>","text":"<p>Plot data formatted for time series charts.</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The formatted data for creating bar charts.</p>"},{"location":"reference/evals/plots/timeseries/#evals.plots.timeseries.ESMTimeSeriesChart.fix_snapshots","title":"<code>fix_snapshots(df, year)</code>  <code>staticmethod</code>","text":"<p>Correct the year in snapshot timestamp column labels.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The DataFrame with timestamps to be adjusted.</p> required <code>year</code> <code>int</code> <p>The correct year to use in the data frame columns.</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>The DataFrame with corrected timestamps.</p> Source code in <code>evals/plots/timeseries.py</code> <pre><code>@staticmethod\ndef fix_snapshots(df: pd.DataFrame, year: int) -&gt; pd.DataFrame:\n    \"\"\"\n    Correct the year in snapshot timestamp column labels.\n\n    Parameters\n    ----------\n    df\n        The DataFrame with timestamps to be adjusted.\n    year\n        The correct year to use in the data frame columns.\n\n    Returns\n    -------\n    :\n        The DataFrame with corrected timestamps.\n    \"\"\"\n    if isinstance(df.columns, pd.DatetimeIndex):\n        df.columns = [s.replace(year=year) for s in df.columns]\n    return df\n</code></pre>"},{"location":"reference/evals/plots/timeseries/#evals.plots.timeseries.ESMTimeSeriesChart.plot","title":"<code>plot()</code>","text":"<p>Plot the data to the chart.</p> <p>This function iterates over the data series, adds traces to the figure, styles the inflexible demand, sets the layout, styles the title, legend, x-axis label, time series axes, and appends footnotes.</p> Source code in <code>evals/plots/timeseries.py</code> <pre><code>def plot(self) -&gt; None:\n    \"\"\"\n    Plot the data to the chart.\n\n    This function iterates over the data series, adds traces to the\n    figure, styles the inflexible demand, sets the layout, styles\n    the title, legend, x-axis label, time series axes, and appends\n    footnotes.\n    \"\"\"\n    title = self.cfg.title.format(\n        location=self.location, year=self.year, unit=self.unit\n    )\n    if self.empty_input:\n        self.fig = empty_figure(title)\n        return\n\n    stackgroup = None\n    for i, (name, series) in enumerate(self.df.items()):\n        if self.cfg.stacked:\n            stackgroup = \"supply\" if series.sum() &gt;= 0 else \"withdrawal\"\n        legendrank = 1000 + i if stackgroup == \"supply\" else 1000 - i\n        self.fig.add_trace(\n            go.Scatter(\n                x=series.index,\n                y=series.values,\n                hovertemplate=\"%{y:.2f} \" + self.unit,\n                name=name,\n                fill=self.cfg.fill.get(name, \"tonexty\"),\n                fillpattern_shape=self.cfg.pattern.get(name),\n                line_dash=self.cfg.line_dash.get(name, \"solid\"),\n                line_width=self.cfg.line_width.get(name, 1),\n                line_color=self.cfg.colors.get(name),\n                line_shape=self.cfg.line_shape,\n                fillcolor=self.cfg.colors.get(name),\n                stackgroup=stackgroup,\n                legendrank=legendrank,\n            )\n        )\n\n    self._style_inflexible_demand()\n    self._set_base_layout()\n    self._style_title_and_legend_and_xaxis_label()\n    self._style_time_series_axes_and_layout(title)\n    self._append_footnotes()\n</code></pre>"},{"location":"reference/evals/views/","title":"Index","text":"<p>Expose view functions from inside the views package to the module.</p>"},{"location":"reference/evals/views/#evals.views.view_balance_biomass","title":"<code>view_balance_biomass(result_path, networks, config)</code>","text":"<p>Evaluate the solid biomass balance.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/balances.py</code> <pre><code>def view_balance_biomass(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the solid biomass balance.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    simple_bus_balance(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_balance_carbon","title":"<code>view_balance_carbon(result_path, networks, config)</code>","text":"<p>Evaluate the carbon balance.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/balances.py</code> <pre><code>def view_balance_carbon(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the carbon balance.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    simple_bus_balance(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_balance_electricity","title":"<code>view_balance_electricity(result_path, networks, config)</code>","text":"<p>Evaluate the electricity production &amp; demand by country and year.</p> <p>Returns:</p> Type Description <code>None</code> Notes <p>Balances do nat add up to zero, because of transmission losses and storage cycling (probably).</p> Source code in <code>evals/views/balances.py</code> <pre><code>def view_balance_electricity(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the electricity production &amp; demand by country and year.\n\n    Returns\n    -------\n    :\n\n    Notes\n    -----\n    Balances do nat add up to zero, because of transmission losses and\n    storage cycling (probably).\n    \"\"\"\n    simple_bus_balance(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_balance_heat","title":"<code>view_balance_heat(result_path, networks, config)</code>","text":"<p>Evaluate the heat balance.</p> <p>Parameters:</p> Name Type Description Default <code>result_path</code> <code>str | pathlib.Path</code> required <code>networks</code> <code>dict</code> required <code>config</code> <code>dict</code> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/balances.py</code> <pre><code>def view_balance_heat(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the heat balance.\n\n    Parameters\n    ----------\n    result_path\n    networks\n    config\n\n    Returns\n    -------\n    :\n    \"\"\"\n    bus_carrier = config[\"view\"][\"bus_carrier\"]\n    # todo: storage links\n\n    link_energy_balance = collect_myopic_statistics(\n        networks,\n        comps=\"Link\",\n        statistic=\"energy_balance\",\n    )\n\n    # for every heat bus, calculate the amounts of supply for heat\n    to_concat = []\n    for bc in bus_carrier:\n        p = (\n            link_energy_balance.pipe(filter_for_carrier_connected_to, bc)\n            # CO2 supply are CO2 emissions that do not help heat production\n            .drop([\"co2\", \"co2 stored\"], level=DM.BUS_CARRIER)\n            .pipe(calculate_input_share, bc)\n            # drop technology names in favour of input bus carrier names:\n            .pipe(rename_aggregate, bc)\n            .swaplevel(DM.BUS_CARRIER, DM.CARRIER)\n        )\n        p.index = p.index.set_names(DM.YEAR_IDX_NAMES)\n        p.attrs[\"unit\"] = \"MWh_th\"\n        to_concat.append(p)\n\n    supply = pd.concat(to_concat)\n\n    heat_loss_factor = get_heat_loss_factor(networks)\n    demand = (\n        collect_myopic_statistics(\n            networks,\n            statistic=\"withdrawal\",\n            bus_carrier=bus_carrier,\n        )\n        .pipe(split_urban_central_heat_losses_and_consumption, heat_loss_factor)\n        .mul(-1)\n    )\n\n    exporter = Exporter(statistics=[supply, demand], view_config=config[\"view\"])\n\n    # static view settings:\n    chart_class = getattr(plots, config[\"view\"][\"chart\"])\n    exporter.defaults.plotly.chart = chart_class\n    exporter.defaults.plotly.xaxis_title = \"\"\n    exporter.defaults.plotly.pattern = {\"Demand\": \"/\"}\n\n    exporter.export(result_path, config[\"global\"][\"subdir\"])\n    chart_class = getattr(plots, config[\"view\"][\"chart\"])\n    exporter.defaults.plotly.chart = chart_class\n\n    if chart_class == plots.ESMGroupedBarChart:\n        exporter.defaults.plotly.xaxis_title = \"\"\n    elif chart_class == plots.ESMBarChart:\n        # combine bus carrier to export netted technologies, although\n        # they have difference bus_carrier in index , e.g.\n        # electricity distribution grid, (AC, low voltage)\n        exporter.statistics[0] = rename_aggregate(\n            demand, bus_carrier[0], level=DM.BUS_CARRIER\n        )\n        exporter.statistics[1] = rename_aggregate(\n            supply, bus_carrier[0], level=DM.BUS_CARRIER\n        )\n\n    exporter.export(result_path, config[\"global\"][\"subdir\"])\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_balance_hydrogen","title":"<code>view_balance_hydrogen(result_path, networks, config)</code>","text":"<p>Evaluate the Hydrogen balance.</p> <p>Returns:</p> Type Description <code>None</code> Notes <p>See eval module docstring for parameter description.</p> Source code in <code>evals/views/balances.py</code> <pre><code>def view_balance_hydrogen(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the Hydrogen balance.\n\n    Returns\n    -------\n    :\n\n    Notes\n    -----\n    See eval module docstring for parameter description.\n    \"\"\"\n    simple_bus_balance(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_balance_methane","title":"<code>view_balance_methane(result_path, networks, config)</code>","text":"<p>Evaluate the methane balance.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/balances.py</code> <pre><code>def view_balance_methane(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the methane balance.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    simple_bus_balance(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_capacity_electricity_production","title":"<code>view_capacity_electricity_production(result_path, networks, config)</code>","text":"<p>Evaluate the optimal capacity for AC technologies that produce electricity.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/capacities.py</code> <pre><code>def view_capacity_electricity_production(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the optimal capacity for AC technologies that produce electricity.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    simple_optimal_capacity(networks, config, result_path, kind=\"production\")\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_capacity_electricity_storage","title":"<code>view_capacity_electricity_storage(result_path, networks, config)</code>","text":"<p>Evaluate the optimal capacity for AC technologies that store electricity.</p> <p>Returns:</p> Type Description <code>None</code> Notes <p>Fixme: Run-of-River is much too high.</p> Source code in <code>evals/views/capacities.py</code> <pre><code>def view_capacity_electricity_storage(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the optimal capacity for AC technologies that store electricity.\n\n    Returns\n    -------\n    :\n\n    Notes\n    -----\n    Fixme: Run-of-River is much too high.\n    \"\"\"\n    simple_storage_capacity(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_capacity_gas_production","title":"<code>view_capacity_gas_production(result_path, networks, config)</code>","text":"<p>Evaluate the optimal capacity for technologies that produce Methane.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/capacities.py</code> <pre><code>def view_capacity_gas_production(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the optimal capacity for technologies that produce Methane.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    simple_optimal_capacity(networks, config, result_path, kind=\"production\")\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_capacity_gas_storage","title":"<code>view_capacity_gas_storage(result_path, networks, config)</code>","text":"<p>Evaluate optimal storage capacities for gas stores (CH4, H2).</p> <p>Returns:</p> Type Description <code>None</code> Notes <p>FixMe: No Hydrogen Storage with current config?</p> Source code in <code>evals/views/capacities.py</code> <pre><code>def view_capacity_gas_storage(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate optimal storage capacities for gas stores (CH4, H2).\n\n    Returns\n    -------\n    :\n\n    Notes\n    -----\n    FixMe: No Hydrogen Storage with current config?\n    \"\"\"\n    simple_storage_capacity(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_capacity_heat_demand","title":"<code>view_capacity_heat_demand(result_path, networks, config)</code>","text":"<p>Evaluate the optimal capacity for technologies that withdraw heat.</p> <p>Returns:</p> Type Description <code>None</code> <p>Writes 2 Excel files and 1 BarChart per country.</p> Source code in <code>evals/views/capacities.py</code> <pre><code>def view_capacity_heat_demand(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the optimal capacity for technologies that withdraw heat.\n\n    Returns\n    -------\n    :\n        Writes 2 Excel files and 1 BarChart per country.\n    \"\"\"\n    simple_optimal_capacity(networks, config, result_path, kind=\"demand\")\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_capacity_hydrogen_production","title":"<code>view_capacity_hydrogen_production(result_path, networks, config)</code>","text":"<p>Evaluate the optimal capacity for technologies that produce Hydrogen.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/capacities.py</code> <pre><code>def view_capacity_hydrogen_production(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the optimal capacity for technologies that produce Hydrogen.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    simple_optimal_capacity(networks, config, result_path, kind=\"production\")\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_demand_heat","title":"<code>view_demand_heat(result_path, networks, config, subdir='evaluation')</code>","text":"<p>Evaluate the energy required for heat production and generation.</p> <p>Results are grouped by bus_carrier and not by carrier as usual to show the input energy carrier mix.</p> <p>Returns:</p> Type Description <code>None</code> Notes <p>See eval docstring for parameter description.</p> Source code in <code>evals/views/demand.py</code> <pre><code>def view_demand_heat(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n    subdir: str | Path = \"evaluation\",\n) -&gt; None:\n    \"\"\"\n    Evaluate the energy required for heat production and generation.\n\n    Results are grouped by bus_carrier and not by carrier\n    as usual to show the input energy carrier mix.\n\n    Returns\n    -------\n    :\n\n    Notes\n    -----\n    See eval docstring for parameter description.\n    \"\"\"\n    energy_for_heat = (\n        collect_myopic_statistics(networks, comps=\"Link\", statistic=\"energy_balance\")\n        # todo: is dropping CO2 really justified? Discussions needed, or disclaimer in graph.\n        # .drop([\"co2\", \"co2 stored\"], level=DataModel.BUS_CARRIER)\n        .pipe(drop_from_multtindex_by_regex, \"water tanks\")\n        .pipe(filter_for_carrier_connected_to, BusCarrier.heat_buses(), kind=\"supply\")\n        .pipe(calculate_input_share, BusCarrier.HEAT_RURAL)\n    )\n\n    generator_supply = collect_myopic_statistics(\n        networks,\n        statistic=\"supply\",\n        comps=\"Generator\",\n        bus_carrier=BusCarrier.heat_buses(),\n    )\n\n    exporter = Exporter(\n        statistics=[energy_for_heat.mul(-1), generator_supply],\n        view_config=config[\"view\"],\n    )\n\n    # view specific static settings:\n    exporter.defaults.plotly.chart = ESMBarChart\n    exporter.defaults.excel.pivot_index = [DataModel.LOCATION, DataModel.BUS_CARRIER]\n    exporter.defaults.plotly.plot_category = DataModel.BUS_CARRIER\n    exporter.defaults.plotly.pivot_index = [\n        DataModel.YEAR,\n        DataModel.LOCATION,\n        DataModel.BUS_CARRIER,\n    ]\n\n    exporter.export(result_path, subdir=subdir)\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_final_energy_demand","title":"<code>view_final_energy_demand(result_path, networks, config, subdir='evaluation')</code>","text":"<p>Evaluate the final energy demand per country.</p> <p>Parameters:</p> Name Type Description Default <code>result_path</code> <code>str | pathlib.Path</code> required <code>networks</code> <code>dict</code> required <code>config</code> <code>dict</code> required <code>subdir</code> <code>str | pathlib.Path</code> <code>'evaluation'</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/demand_fed.py</code> <pre><code>def view_final_energy_demand(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n    subdir: str | Path = \"evaluation\",\n) -&gt; None:\n    \"\"\"\n    Evaluate the final energy demand per country.\n\n    Parameters\n    ----------\n    result_path\n    networks\n    config\n    subdir\n\n    Returns\n    -------\n    :\n    \"\"\"\n    link_supply_rural_heat = (\n        collect_myopic_statistics(\n            networks,\n            comps=\"Link\",\n            statistic=\"energy_balance\",\n        )\n        .pipe(filter_for_carrier_connected_to, BusCarrier.HEAT_RURAL)  # , kind=\"supply\"\n        .pipe(calculate_input_share, BusCarrier.HEAT_RURAL)\n    )\n\n    generator_supply_rural_heat = collect_myopic_statistics(\n        networks,\n        comps=\"Generator\",\n        statistic=\"supply\",\n        bus_carrier=BusCarrier.HEAT_RURAL,\n    )\n\n    load_withdrawal_urban_heat = collect_myopic_statistics(\n        networks,\n        \"withdrawal\",\n        comps=\"Load\",\n        bus_carrier=[BusCarrier.HEAT_URBAN_CENTRAL, BusCarrier.HEAT_URBAN_DECENTRAL],\n    ).drop(\n        Carrier.low_temperature_heat_for_industry,\n        level=DataModel.CARRIER,\n    )\n\n    # # The Toolbox drops Italian urban heat technologies for unknown reasons.\n    # load_withdrawal_urban_heat = load_withdrawal_urban_heat.drop([\"IT0\", \"IT1\", \"IT2\"], level=DataModel.LOCATION)\n    # # todo: Is this correct? They probably had a good reason for that, but I just can't see it.\n\n    loss_factor = get_heat_loss_factor(networks)\n    load_split_urban_heat = split_urban_central_heat_losses_and_consumption(\n        load_withdrawal_urban_heat, loss_factor\n    )\n\n    fed_homes_and_trade = collect_myopic_statistics(\n        networks, statistic=\"ac_load_split\"\n    ).pipe(filter_by, carrier=Carrier.domestic_homes_and_trade)\n\n    # todo: couldn't his be much easier? If we simply query for withdrawal at heat buses?\n    # from evals.utils import filter_by\n\n    # _ = (\n    #     collect_myopic_statistics(\n    #         networks,\n    #         comps=(\"Generator\", \"Load\", \"Link\"),\n    #         statistic=\"energy_balance\",\n    #         bus_carrier=BusCarrier.heat_buses(),\n    #         aggregate_time=False,\n    #     )\n    #     .pipe(\n    #         filter_by,\n    #         carrier=\"urban decentral biomass boiler\",\n    #         year=\"2020\",\n    #         location=\"CH\",\n    #     )\n    #     .T\n    # )\n    # # fixme: \"urban decentral biomass boiler\" supplies to solid biomass and draws from heat!!!\n    #\n    # n = networks[\"2030\"]\n    # gen = (\n    #     n.generators.assign(g=n.generators_t.p.mean())\n    #     .groupby([\"bus\", \"carrier\"])\n    #     .g.sum()\n    # )\n    # n.plot(\n    #     bus_sizes=gen / 5e4,\n    #     # bus_colors={\"gas\": \"indianred\", \"wind\": \"midnightblue\"},\n    #     margin=0.5,\n    #     line_widths=0.1,\n    #     line_flow=\"mean\",\n    #     link_widths=0,\n    # )\n    # plt.show()\n\n    # todo: need to map carrier names to sector names in grouped barchart\n    exporter = Exporter(\n        statistics=[\n            link_supply_rural_heat.mul(-1),\n            generator_supply_rural_heat,\n            load_split_urban_heat,\n            fed_homes_and_trade,\n        ],\n        view_config=config[\"view\"],\n    )\n\n    # view specific static settings:\n    exporter.defaults.plotly.chart = ESMBarChart\n    exporter.defaults.excel.pivot_index = [\n        DataModel.LOCATION,\n        DataModel.BUS_CARRIER,\n    ]\n    exporter.defaults.plotly.plot_category = DataModel.BUS_CARRIER\n    exporter.defaults.plotly.pivot_index = [\n        DataModel.YEAR,\n        DataModel.LOCATION,\n        DataModel.BUS_CARRIER,\n    ]\n\n    exporter.export(result_path, subdir=subdir)\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_grid_capacity","title":"<code>view_grid_capacity(result_path, networks, config, subdir='evaluation')</code>","text":"<p>Export transmission grids to file using Folium.</p> Source code in <code>evals/views/transmission.py</code> <pre><code>def view_grid_capacity(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n    subdir: str | Path = \"evaluation\",\n) -&gt; None:  # numpydoc ignore=PR01\n    \"\"\"Export transmission grids to file using Folium.\"\"\"\n\n    # update bus coordinates to improve map readability\n    for n in networks.values():\n        # Lower Austria\n        n.df(\"Bus\").loc[\"AT12\", \"x\"] -= 0.1  # Lon, left\n        n.df(\"Bus\").loc[\"AT12\", \"y\"] += 0.3  # Lat, up\n        # Burgenland\n        n.df(\"Bus\").loc[\"AT11\", \"x\"] -= 0.1  # Lon, left\n        n.df(\"Bus\").loc[\"AT11\", \"y\"] -= 0.26  # Lat, down\n        # Salzburg\n        n.df(\"Bus\").loc[\"AT32\", \"x\"] += 0.35  # Lon, right\n        n.df(\"Bus\").loc[\"AT32\", \"y\"] -= 0.05  # Lat, down\n        # Vienna\n        n.df(\"Bus\").loc[\"AT13\", \"x\"] += 0.1  # Lon, right\n        n.df(\"Bus\").loc[\"AT13\", \"y\"] += 0.1  # Lat, up\n\n    grid_capactiy = collect_myopic_statistics(\n        networks,\n        statistic=\"grid_capacity\",\n        drop_zeros=False,\n        comps=[\"Link\", \"Line\"],\n    )\n\n    # cannot use utils.scale(), because of the additional \"line\" column\n    col = \"Capacity (MW)\"\n    grid_capactiy[col] = grid_capactiy[col] * 1e-3\n    grid_capactiy = grid_capactiy.rename(columns={col: \"Capacity (GW)\"})\n    grid_capactiy.attrs[\"unit\"] = \"GW\"\n\n    import_energy = collect_myopic_statistics(\n        networks,\n        statistic=\"supply\",\n        comps=\"Generator\",\n        bus_carrier=[BusCarrier.CH4, BusCarrier.H2, \"biogas\", \"AC\"],  # \"gas primary\",\n    )\n    import_energy *= 1e-6\n    import_energy.attrs[\"name\"] = \"Import Energy\"\n    import_energy.attrs[\"unit\"] = \"TWh\"\n    metric_name = f\"{import_energy.attrs['name']} ({import_energy.attrs['unit']})\"\n    import_energy.name = metric_name\n\n    # the optimal capacity for pipelines is larger than the maximal\n    # energy flow in the time series, because pipelines are oversized.\n    # We use the maximal flow here since it is more interesting.\n    import_capacity = collect_myopic_statistics(\n        networks,\n        statistic=\"supply\",\n        comps=\"Generator\",\n        bus_carrier=[BusCarrier.CH4, BusCarrier.H2, \"biogas\"],  # \"gas primary\"\n        aggregate_time=\"max\",\n    ).drop(\"2015\", level=DataModel.YEAR, errors=\"ignore\")\n    import_capacity *= 1e-3\n    import_capacity.attrs[\"name\"] = \"Import Capacity\"\n    import_capacity.attrs[\"unit\"] = \"GW\"\n    metric_name = f\"{import_capacity.attrs['name']} ({import_capacity.attrs['unit']})\"\n    import_capacity.name = metric_name\n\n    config = GridMapConfig(show_year=\"2030\")  # fixme: show_year is broken =(\n    buses = networks[next(reversed(networks))].df(\"Bus\")\n\n    # every list item will become one HTML file with a map for the\n    # specified carrier and bus_carrier\n    # ToDo: Add CO2 once it works properly\n    carriers_bus_carrier_groups = (\n        ([Carrier.AC, Carrier.DC], BusCarrier.AC),\n        ([Carrier.gas_pipepline, Carrier.gas_pipepline_new], BusCarrier.CH4),\n        (\n            [  # todo: use get_transmission_techs() instead of hardcoding\n                Carrier.h2_pipeline,\n                Carrier.h2_pipeline_retro,\n                Carrier.h2_pipeline_kernnetz,\n            ],\n            BusCarrier.H2,\n        ),\n    )\n\n    for carriers, bus_carrier in carriers_bus_carrier_groups:\n        df_grid = filter_by(grid_capactiy, carrier=carriers)\n        df_import_energy = filter_by(import_energy, bus_carrier=bus_carrier)\n        df_import_energy = df_import_energy[df_import_energy &gt; 0]\n        df_import_capacity = filter_by(import_capacity, bus_carrier=bus_carrier)\n        df_import_capacity = df_import_capacity[df_import_capacity &gt; 0]\n        grid_map = TransmissionGridMap(\n            df_grid, df_import_energy, df_import_capacity, buses, config\n        )\n        grid_map.draw_grid_by_carrier_groups_myopic()\n        grid_map.save(result_path, f\"gridmap_{bus_carrier}\", subdir)\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_timeseries_carbon","title":"<code>view_timeseries_carbon(result_path, networks, config)</code>","text":"<p>Evaluate the Carbon balance time series.</p> Source code in <code>evals/views/balances_timeseries.py</code> <pre><code>def view_timeseries_carbon(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"Evaluate the Carbon balance time series.\"\"\"\n    simple_timeseries(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_timeseries_electricity","title":"<code>view_timeseries_electricity(result_path, networks, config)</code>","text":"<p>Evaluate the electricity balance time series.</p> Source code in <code>evals/views/balances_timeseries.py</code> <pre><code>def view_timeseries_electricity(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"Evaluate the electricity balance time series.\"\"\"\n    simple_timeseries(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_timeseries_hydrogen","title":"<code>view_timeseries_hydrogen(result_path, networks, config)</code>","text":"<p>Evaluate the Hydrogen balance time series.</p> Source code in <code>evals/views/balances_timeseries.py</code> <pre><code>def view_timeseries_hydrogen(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"Evaluate the Hydrogen balance time series.\"\"\"\n    simple_timeseries(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/#evals.views.view_timeseries_methane","title":"<code>view_timeseries_methane(result_path, networks, config)</code>","text":"<p>Evaluate the Methane balance time series.</p> Source code in <code>evals/views/balances_timeseries.py</code> <pre><code>def view_timeseries_methane(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"Evaluate the Methane balance time series.\"\"\"\n    simple_timeseries(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/balances/","title":"Balances","text":""},{"location":"reference/evals/views/balances/#evals.views.balances.view_balance_biomass","title":"<code>view_balance_biomass(result_path, networks, config)</code>","text":"<p>Evaluate the solid biomass balance.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/balances.py</code> <pre><code>def view_balance_biomass(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the solid biomass balance.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    simple_bus_balance(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/balances/#evals.views.balances.view_balance_carbon","title":"<code>view_balance_carbon(result_path, networks, config)</code>","text":"<p>Evaluate the carbon balance.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/balances.py</code> <pre><code>def view_balance_carbon(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the carbon balance.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    simple_bus_balance(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/balances/#evals.views.balances.view_balance_electricity","title":"<code>view_balance_electricity(result_path, networks, config)</code>","text":"<p>Evaluate the electricity production &amp; demand by country and year.</p> <p>Returns:</p> Type Description <code>None</code> Notes <p>Balances do nat add up to zero, because of transmission losses and storage cycling (probably).</p> Source code in <code>evals/views/balances.py</code> <pre><code>def view_balance_electricity(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the electricity production &amp; demand by country and year.\n\n    Returns\n    -------\n    :\n\n    Notes\n    -----\n    Balances do nat add up to zero, because of transmission losses and\n    storage cycling (probably).\n    \"\"\"\n    simple_bus_balance(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/balances/#evals.views.balances.view_balance_heat","title":"<code>view_balance_heat(result_path, networks, config)</code>","text":"<p>Evaluate the heat balance.</p> <p>Parameters:</p> Name Type Description Default <code>result_path</code> <code>str | pathlib.Path</code> required <code>networks</code> <code>dict</code> required <code>config</code> <code>dict</code> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/balances.py</code> <pre><code>def view_balance_heat(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the heat balance.\n\n    Parameters\n    ----------\n    result_path\n    networks\n    config\n\n    Returns\n    -------\n    :\n    \"\"\"\n    bus_carrier = config[\"view\"][\"bus_carrier\"]\n    # todo: storage links\n\n    link_energy_balance = collect_myopic_statistics(\n        networks,\n        comps=\"Link\",\n        statistic=\"energy_balance\",\n    )\n\n    # for every heat bus, calculate the amounts of supply for heat\n    to_concat = []\n    for bc in bus_carrier:\n        p = (\n            link_energy_balance.pipe(filter_for_carrier_connected_to, bc)\n            # CO2 supply are CO2 emissions that do not help heat production\n            .drop([\"co2\", \"co2 stored\"], level=DM.BUS_CARRIER)\n            .pipe(calculate_input_share, bc)\n            # drop technology names in favour of input bus carrier names:\n            .pipe(rename_aggregate, bc)\n            .swaplevel(DM.BUS_CARRIER, DM.CARRIER)\n        )\n        p.index = p.index.set_names(DM.YEAR_IDX_NAMES)\n        p.attrs[\"unit\"] = \"MWh_th\"\n        to_concat.append(p)\n\n    supply = pd.concat(to_concat)\n\n    heat_loss_factor = get_heat_loss_factor(networks)\n    demand = (\n        collect_myopic_statistics(\n            networks,\n            statistic=\"withdrawal\",\n            bus_carrier=bus_carrier,\n        )\n        .pipe(split_urban_central_heat_losses_and_consumption, heat_loss_factor)\n        .mul(-1)\n    )\n\n    exporter = Exporter(statistics=[supply, demand], view_config=config[\"view\"])\n\n    # static view settings:\n    chart_class = getattr(plots, config[\"view\"][\"chart\"])\n    exporter.defaults.plotly.chart = chart_class\n    exporter.defaults.plotly.xaxis_title = \"\"\n    exporter.defaults.plotly.pattern = {\"Demand\": \"/\"}\n\n    exporter.export(result_path, config[\"global\"][\"subdir\"])\n    chart_class = getattr(plots, config[\"view\"][\"chart\"])\n    exporter.defaults.plotly.chart = chart_class\n\n    if chart_class == plots.ESMGroupedBarChart:\n        exporter.defaults.plotly.xaxis_title = \"\"\n    elif chart_class == plots.ESMBarChart:\n        # combine bus carrier to export netted technologies, although\n        # they have difference bus_carrier in index , e.g.\n        # electricity distribution grid, (AC, low voltage)\n        exporter.statistics[0] = rename_aggregate(\n            demand, bus_carrier[0], level=DM.BUS_CARRIER\n        )\n        exporter.statistics[1] = rename_aggregate(\n            supply, bus_carrier[0], level=DM.BUS_CARRIER\n        )\n\n    exporter.export(result_path, config[\"global\"][\"subdir\"])\n</code></pre>"},{"location":"reference/evals/views/balances/#evals.views.balances.view_balance_hydrogen","title":"<code>view_balance_hydrogen(result_path, networks, config)</code>","text":"<p>Evaluate the Hydrogen balance.</p> <p>Returns:</p> Type Description <code>None</code> Notes <p>See eval module docstring for parameter description.</p> Source code in <code>evals/views/balances.py</code> <pre><code>def view_balance_hydrogen(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the Hydrogen balance.\n\n    Returns\n    -------\n    :\n\n    Notes\n    -----\n    See eval module docstring for parameter description.\n    \"\"\"\n    simple_bus_balance(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/balances/#evals.views.balances.view_balance_methane","title":"<code>view_balance_methane(result_path, networks, config)</code>","text":"<p>Evaluate the methane balance.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/balances.py</code> <pre><code>def view_balance_methane(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the methane balance.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    simple_bus_balance(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/balances_timeseries/","title":"Balances timeseries","text":""},{"location":"reference/evals/views/balances_timeseries/#evals.views.balances_timeseries.view_timeseries_carbon","title":"<code>view_timeseries_carbon(result_path, networks, config)</code>","text":"<p>Evaluate the Carbon balance time series.</p> Source code in <code>evals/views/balances_timeseries.py</code> <pre><code>def view_timeseries_carbon(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"Evaluate the Carbon balance time series.\"\"\"\n    simple_timeseries(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/balances_timeseries/#evals.views.balances_timeseries.view_timeseries_electricity","title":"<code>view_timeseries_electricity(result_path, networks, config)</code>","text":"<p>Evaluate the electricity balance time series.</p> Source code in <code>evals/views/balances_timeseries.py</code> <pre><code>def view_timeseries_electricity(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"Evaluate the electricity balance time series.\"\"\"\n    simple_timeseries(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/balances_timeseries/#evals.views.balances_timeseries.view_timeseries_hydrogen","title":"<code>view_timeseries_hydrogen(result_path, networks, config)</code>","text":"<p>Evaluate the Hydrogen balance time series.</p> Source code in <code>evals/views/balances_timeseries.py</code> <pre><code>def view_timeseries_hydrogen(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"Evaluate the Hydrogen balance time series.\"\"\"\n    simple_timeseries(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/balances_timeseries/#evals.views.balances_timeseries.view_timeseries_methane","title":"<code>view_timeseries_methane(result_path, networks, config)</code>","text":"<p>Evaluate the Methane balance time series.</p> Source code in <code>evals/views/balances_timeseries.py</code> <pre><code>def view_timeseries_methane(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"Evaluate the Methane balance time series.\"\"\"\n    simple_timeseries(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/capacities/","title":"Capacities","text":""},{"location":"reference/evals/views/capacities/#evals.views.capacities.view_capacity_electricity_demand","title":"<code>view_capacity_electricity_demand(result_path, networks, config)</code>","text":"<p>Evaluate the optimal capacity for AC technologies that withdraw electricity.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/capacities.py</code> <pre><code>def view_capacity_electricity_demand(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the optimal capacity for AC technologies that withdraw electricity.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    simple_optimal_capacity(networks, config, result_path, kind=\"demand\")\n</code></pre>"},{"location":"reference/evals/views/capacities/#evals.views.capacities.view_capacity_electricity_production","title":"<code>view_capacity_electricity_production(result_path, networks, config)</code>","text":"<p>Evaluate the optimal capacity for AC technologies that produce electricity.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/capacities.py</code> <pre><code>def view_capacity_electricity_production(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the optimal capacity for AC technologies that produce electricity.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    simple_optimal_capacity(networks, config, result_path, kind=\"production\")\n</code></pre>"},{"location":"reference/evals/views/capacities/#evals.views.capacities.view_capacity_electricity_storage","title":"<code>view_capacity_electricity_storage(result_path, networks, config)</code>","text":"<p>Evaluate the optimal capacity for AC technologies that store electricity.</p> <p>Returns:</p> Type Description <code>None</code> Notes <p>Fixme: Run-of-River is much too high.</p> Source code in <code>evals/views/capacities.py</code> <pre><code>def view_capacity_electricity_storage(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the optimal capacity for AC technologies that store electricity.\n\n    Returns\n    -------\n    :\n\n    Notes\n    -----\n    Fixme: Run-of-River is much too high.\n    \"\"\"\n    simple_storage_capacity(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/capacities/#evals.views.capacities.view_capacity_gas_production","title":"<code>view_capacity_gas_production(result_path, networks, config)</code>","text":"<p>Evaluate the optimal capacity for technologies that produce Methane.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/capacities.py</code> <pre><code>def view_capacity_gas_production(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the optimal capacity for technologies that produce Methane.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    simple_optimal_capacity(networks, config, result_path, kind=\"production\")\n</code></pre>"},{"location":"reference/evals/views/capacities/#evals.views.capacities.view_capacity_gas_storage","title":"<code>view_capacity_gas_storage(result_path, networks, config)</code>","text":"<p>Evaluate optimal storage capacities for gas stores (CH4, H2).</p> <p>Returns:</p> Type Description <code>None</code> Notes <p>FixMe: No Hydrogen Storage with current config?</p> Source code in <code>evals/views/capacities.py</code> <pre><code>def view_capacity_gas_storage(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate optimal storage capacities for gas stores (CH4, H2).\n\n    Returns\n    -------\n    :\n\n    Notes\n    -----\n    FixMe: No Hydrogen Storage with current config?\n    \"\"\"\n    simple_storage_capacity(networks, config, result_path)\n</code></pre>"},{"location":"reference/evals/views/capacities/#evals.views.capacities.view_capacity_heat_demand","title":"<code>view_capacity_heat_demand(result_path, networks, config)</code>","text":"<p>Evaluate the optimal capacity for technologies that withdraw heat.</p> <p>Returns:</p> Type Description <code>None</code> <p>Writes 2 Excel files and 1 BarChart per country.</p> Source code in <code>evals/views/capacities.py</code> <pre><code>def view_capacity_heat_demand(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the optimal capacity for technologies that withdraw heat.\n\n    Returns\n    -------\n    :\n        Writes 2 Excel files and 1 BarChart per country.\n    \"\"\"\n    simple_optimal_capacity(networks, config, result_path, kind=\"demand\")\n</code></pre>"},{"location":"reference/evals/views/capacities/#evals.views.capacities.view_capacity_heat_production","title":"<code>view_capacity_heat_production(result_path, networks, config)</code>","text":"<p>Evaluate the optimal capacity for technologies that produce heat.</p> <p>Returns:</p> Type Description <code>None</code> <p>Writes 2 Excel files and 1 BarChart per country.</p> Source code in <code>evals/views/capacities.py</code> <pre><code>def view_capacity_heat_production(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the optimal capacity for technologies that produce heat.\n\n    Returns\n    -------\n    :\n        Writes 2 Excel files and 1 BarChart per country.\n    \"\"\"\n    simple_optimal_capacity(networks, config, result_path, kind=\"production\")\n</code></pre>"},{"location":"reference/evals/views/capacities/#evals.views.capacities.view_capacity_hydrogen_production","title":"<code>view_capacity_hydrogen_production(result_path, networks, config)</code>","text":"<p>Evaluate the optimal capacity for technologies that produce Hydrogen.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/capacities.py</code> <pre><code>def view_capacity_hydrogen_production(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Evaluate the optimal capacity for technologies that produce Hydrogen.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    simple_optimal_capacity(networks, config, result_path, kind=\"production\")\n</code></pre>"},{"location":"reference/evals/views/common/","title":"Common","text":""},{"location":"reference/evals/views/common/#evals.views.common.simple_optimal_capacity","title":"<code>simple_optimal_capacity(networks, config, result_path, kind=None)</code>","text":"<p>Export optimal capacities for production or demand or both.</p> Source code in <code>evals/views/common.py</code> <pre><code>def simple_optimal_capacity(\n    networks: dict, config: dict, result_path: str | Path, kind: str = None\n) -&gt; None:\n    \"\"\"Export optimal capacities for production or demand or both.\"\"\"\n    (\n        bus_carrier,\n        transmission_comps,\n        transmission_carrier,\n        storage_links,\n    ) = _parse_view_config_items(networks, config)\n\n    optimal_capacity = (\n        collect_myopic_statistics(\n            networks,\n            statistic=\"optimal_capacity\",\n            bus_carrier=bus_carrier,\n            aggregate_components=None,\n        )\n        .pipe(\n            filter_by,\n            component=transmission_comps,\n            carrier=transmission_carrier,\n            exclude=True,\n        )\n        .pipe(filter_by, carrier=storage_links, exclude=True)\n        .droplevel(DM.COMPONENT)\n    )\n\n    if kind == \"production\":\n        optimal_capacity = optimal_capacity[optimal_capacity &gt; 0]\n    elif kind == \"demand\":\n        optimal_capacity = optimal_capacity[optimal_capacity &lt; 0]\n\n    # 'optimal_capacity' wrongly returns MWh as a unit, but it is MW.\n    optimal_capacity.attrs[\"unit\"] = optimal_capacity.attrs[\"unit\"].replace(\"MWh\", \"MW\")\n\n    exporter = Exporter(\n        statistics=[optimal_capacity],\n        view_config=config[\"view\"],\n    )\n\n    # view specific constant settings\n    chart_class = getattr(plots, config[\"view\"][\"chart\"])\n    exporter.defaults.plotly.chart = chart_class\n\n    if chart_class == plots.ESMGroupedBarChart:\n        exporter.defaults.plotly.xaxis_title = \"\"\n    elif chart_class == plots.ESMBarChart:\n        # combine bus carrier to export netted technologies, although\n        # they have difference bus_carrier in index , e.g.\n        # electricity distribution grid, (AC, low voltage)\n        exporter.statistics = [\n            rename_aggregate(s, bus_carrier[0], level=DM.BUS_CARRIER)\n            for s in exporter.statistics\n        ]\n\n    exporter.export(result_path, config[\"global\"][\"subdir\"])\n</code></pre>"},{"location":"reference/evals/views/common/#evals.views.common.simple_storage_capacity","title":"<code>simple_storage_capacity(networks, config, result_path)</code>","text":"<p>Export optimal storage capacities.</p> Source code in <code>evals/views/common.py</code> <pre><code>def simple_storage_capacity(\n    networks: dict, config: dict, result_path: str | Path\n) -&gt; None:\n    \"\"\"Export optimal storage capacities.\"\"\"\n    (\n        bus_carrier,\n        transmission_comps,\n        transmission_carrier,\n        storage_links,\n    ) = _parse_view_config_items(networks, config)\n\n    stores = collect_myopic_statistics(\n        networks,\n        statistic=\"optimal_capacity\",\n        bus_carrier=bus_carrier,\n        storage=True,\n    ).pipe(filter_by, carrier=storage_links)\n\n    exporter = Exporter(\n        statistics=[stores],\n        view_config=config[\"view\"],\n    )\n\n    exporter.defaults.plotly.chart = getattr(plots, config[\"view\"][\"chart\"])\n    exporter.defaults.plotly.cutoff_drop = False  # prevent dropping empty years\n\n    exporter.export(result_path, config[\"global\"][\"subdir\"])\n</code></pre>"},{"location":"reference/evals/views/common/#evals.views.common.simple_timeseries","title":"<code>simple_timeseries(networks, config, result_path)</code>","text":"<p>Export simple time series views.</p> Source code in <code>evals/views/common.py</code> <pre><code>def simple_timeseries(\n    networks: dict,\n    config: dict,\n    result_path: str | Path,\n) -&gt; None:\n    \"\"\"Export simple time series views.\"\"\"\n    (\n        bus_carrier,\n        transmission_comps,\n        transmission_carrier,\n        storage_links,\n    ) = _parse_view_config_items(networks, config)\n\n    supply = (\n        collect_myopic_statistics(\n            networks,\n            statistic=\"supply\",\n            bus_carrier=bus_carrier,\n            aggregate_time=False,\n            aggregate_components=None,\n        )\n        .pipe(\n            filter_by,\n            component=transmission_comps,\n            carrier=transmission_carrier,\n            exclude=True,\n        )\n        .pipe(rename_aggregate, dict.fromkeys(storage_links, Group.storage_out))\n        .droplevel(DM.COMPONENT)\n    )\n\n    demand = (\n        collect_myopic_statistics(\n            networks,\n            statistic=\"withdrawal\",\n            bus_carrier=bus_carrier,\n            aggregate_time=False,\n            aggregate_components=None,\n        )\n        .pipe(\n            filter_by,\n            component=transmission_comps,\n            carrier=transmission_carrier,\n            exclude=True,\n        )\n        .pipe(rename_aggregate, dict.fromkeys(storage_links, Group.storage_in))\n        .droplevel(DM.COMPONENT)\n        .mul(-1)\n    )\n\n    trade_saldo = (\n        collect_myopic_statistics(\n            networks,\n            statistic=\"trade_energy\",\n            scope=(TradeTypes.FOREIGN, TradeTypes.DOMESTIC),\n            direction=\"saldo\",\n            bus_carrier=bus_carrier,\n            aggregate_time=False,\n            aggregate_components=None,\n        )\n        .pipe(\n            filter_by,\n            component=transmission_comps,\n            carrier=transmission_carrier,\n        )\n        .droplevel(DM.COMPONENT)\n    )\n    trade_saldo.attrs[\"unit\"] = supply.attrs[\"unit\"]\n    trade_saldo = rename_aggregate(trade_saldo, trade_saldo.attrs[\"name\"])\n\n    exporter = Exporter(\n        statistics=[supply, demand, trade_saldo],\n        view_config=config[\"view\"],\n    )\n\n    # view specific settings\n    exporter.defaults.excel.chart = None  # charts bloat the xlsx file\n    chart_class = getattr(plots, config[\"view\"][\"chart\"])\n    exporter.defaults.plotly.chart = chart_class\n\n    exporter.defaults.plotly.plotby = [DM.YEAR, DM.LOCATION]\n    exporter.defaults.plotly.pivot_index = [\n        DM.YEAR,\n        DM.LOCATION,\n        DM.CARRIER,\n    ]\n    exporter.defaults.plotly.xaxis_title = \"\"\n\n    exporter.export(result_path, config[\"global\"][\"subdir\"])\n</code></pre>"},{"location":"reference/evals/views/demand/","title":"Demand","text":""},{"location":"reference/evals/views/demand/#evals.views.demand.view_demand_heat","title":"<code>view_demand_heat(result_path, networks, config, subdir='evaluation')</code>","text":"<p>Evaluate the energy required for heat production and generation.</p> <p>Results are grouped by bus_carrier and not by carrier as usual to show the input energy carrier mix.</p> <p>Returns:</p> Type Description <code>None</code> Notes <p>See eval docstring for parameter description.</p> Source code in <code>evals/views/demand.py</code> <pre><code>def view_demand_heat(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n    subdir: str | Path = \"evaluation\",\n) -&gt; None:\n    \"\"\"\n    Evaluate the energy required for heat production and generation.\n\n    Results are grouped by bus_carrier and not by carrier\n    as usual to show the input energy carrier mix.\n\n    Returns\n    -------\n    :\n\n    Notes\n    -----\n    See eval docstring for parameter description.\n    \"\"\"\n    energy_for_heat = (\n        collect_myopic_statistics(networks, comps=\"Link\", statistic=\"energy_balance\")\n        # todo: is dropping CO2 really justified? Discussions needed, or disclaimer in graph.\n        # .drop([\"co2\", \"co2 stored\"], level=DataModel.BUS_CARRIER)\n        .pipe(drop_from_multtindex_by_regex, \"water tanks\")\n        .pipe(filter_for_carrier_connected_to, BusCarrier.heat_buses(), kind=\"supply\")\n        .pipe(calculate_input_share, BusCarrier.HEAT_RURAL)\n    )\n\n    generator_supply = collect_myopic_statistics(\n        networks,\n        statistic=\"supply\",\n        comps=\"Generator\",\n        bus_carrier=BusCarrier.heat_buses(),\n    )\n\n    exporter = Exporter(\n        statistics=[energy_for_heat.mul(-1), generator_supply],\n        view_config=config[\"view\"],\n    )\n\n    # view specific static settings:\n    exporter.defaults.plotly.chart = ESMBarChart\n    exporter.defaults.excel.pivot_index = [DataModel.LOCATION, DataModel.BUS_CARRIER]\n    exporter.defaults.plotly.plot_category = DataModel.BUS_CARRIER\n    exporter.defaults.plotly.pivot_index = [\n        DataModel.YEAR,\n        DataModel.LOCATION,\n        DataModel.BUS_CARRIER,\n    ]\n\n    exporter.export(result_path, subdir=subdir)\n</code></pre>"},{"location":"reference/evals/views/demand_fed/","title":"Demand fed","text":""},{"location":"reference/evals/views/demand_fed/#evals.views.demand_fed.view_final_energy_demand","title":"<code>view_final_energy_demand(result_path, networks, config, subdir='evaluation')</code>","text":"<p>Evaluate the final energy demand per country.</p> <p>Parameters:</p> Name Type Description Default <code>result_path</code> <code>str | pathlib.Path</code> required <code>networks</code> <code>dict</code> required <code>config</code> <code>dict</code> required <code>subdir</code> <code>str | pathlib.Path</code> <code>'evaluation'</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>evals/views/demand_fed.py</code> <pre><code>def view_final_energy_demand(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n    subdir: str | Path = \"evaluation\",\n) -&gt; None:\n    \"\"\"\n    Evaluate the final energy demand per country.\n\n    Parameters\n    ----------\n    result_path\n    networks\n    config\n    subdir\n\n    Returns\n    -------\n    :\n    \"\"\"\n    link_supply_rural_heat = (\n        collect_myopic_statistics(\n            networks,\n            comps=\"Link\",\n            statistic=\"energy_balance\",\n        )\n        .pipe(filter_for_carrier_connected_to, BusCarrier.HEAT_RURAL)  # , kind=\"supply\"\n        .pipe(calculate_input_share, BusCarrier.HEAT_RURAL)\n    )\n\n    generator_supply_rural_heat = collect_myopic_statistics(\n        networks,\n        comps=\"Generator\",\n        statistic=\"supply\",\n        bus_carrier=BusCarrier.HEAT_RURAL,\n    )\n\n    load_withdrawal_urban_heat = collect_myopic_statistics(\n        networks,\n        \"withdrawal\",\n        comps=\"Load\",\n        bus_carrier=[BusCarrier.HEAT_URBAN_CENTRAL, BusCarrier.HEAT_URBAN_DECENTRAL],\n    ).drop(\n        Carrier.low_temperature_heat_for_industry,\n        level=DataModel.CARRIER,\n    )\n\n    # # The Toolbox drops Italian urban heat technologies for unknown reasons.\n    # load_withdrawal_urban_heat = load_withdrawal_urban_heat.drop([\"IT0\", \"IT1\", \"IT2\"], level=DataModel.LOCATION)\n    # # todo: Is this correct? They probably had a good reason for that, but I just can't see it.\n\n    loss_factor = get_heat_loss_factor(networks)\n    load_split_urban_heat = split_urban_central_heat_losses_and_consumption(\n        load_withdrawal_urban_heat, loss_factor\n    )\n\n    fed_homes_and_trade = collect_myopic_statistics(\n        networks, statistic=\"ac_load_split\"\n    ).pipe(filter_by, carrier=Carrier.domestic_homes_and_trade)\n\n    # todo: couldn't his be much easier? If we simply query for withdrawal at heat buses?\n    # from evals.utils import filter_by\n\n    # _ = (\n    #     collect_myopic_statistics(\n    #         networks,\n    #         comps=(\"Generator\", \"Load\", \"Link\"),\n    #         statistic=\"energy_balance\",\n    #         bus_carrier=BusCarrier.heat_buses(),\n    #         aggregate_time=False,\n    #     )\n    #     .pipe(\n    #         filter_by,\n    #         carrier=\"urban decentral biomass boiler\",\n    #         year=\"2020\",\n    #         location=\"CH\",\n    #     )\n    #     .T\n    # )\n    # # fixme: \"urban decentral biomass boiler\" supplies to solid biomass and draws from heat!!!\n    #\n    # n = networks[\"2030\"]\n    # gen = (\n    #     n.generators.assign(g=n.generators_t.p.mean())\n    #     .groupby([\"bus\", \"carrier\"])\n    #     .g.sum()\n    # )\n    # n.plot(\n    #     bus_sizes=gen / 5e4,\n    #     # bus_colors={\"gas\": \"indianred\", \"wind\": \"midnightblue\"},\n    #     margin=0.5,\n    #     line_widths=0.1,\n    #     line_flow=\"mean\",\n    #     link_widths=0,\n    # )\n    # plt.show()\n\n    # todo: need to map carrier names to sector names in grouped barchart\n    exporter = Exporter(\n        statistics=[\n            link_supply_rural_heat.mul(-1),\n            generator_supply_rural_heat,\n            load_split_urban_heat,\n            fed_homes_and_trade,\n        ],\n        view_config=config[\"view\"],\n    )\n\n    # view specific static settings:\n    exporter.defaults.plotly.chart = ESMBarChart\n    exporter.defaults.excel.pivot_index = [\n        DataModel.LOCATION,\n        DataModel.BUS_CARRIER,\n    ]\n    exporter.defaults.plotly.plot_category = DataModel.BUS_CARRIER\n    exporter.defaults.plotly.pivot_index = [\n        DataModel.YEAR,\n        DataModel.LOCATION,\n        DataModel.BUS_CARRIER,\n    ]\n\n    exporter.export(result_path, subdir=subdir)\n</code></pre>"},{"location":"reference/evals/views/price/","title":"Price","text":"<p>Evaluate nodal prices per energy bus carrier.</p>"},{"location":"reference/evals/views/price/#evals.views.price.view_price_map","title":"<code>view_price_map(result_path, networks, config)</code>","text":"<p>Export nodal prices to file using Folium.</p> <p>Parameters:</p> Name Type Description Default <code>result_path</code> <code>str | pathlib.Path</code> <p>The path to the results directory.</p> required <code>networks</code> <code>dict</code> <p>A dictionary of networks.</p> required <code>config</code> <code>dict</code> <p>Configuration dictionary.</p> required Source code in <code>evals/views/price.py</code> <pre><code>def view_price_map(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n) -&gt; None:\n    \"\"\"\n    Export nodal prices to file using Folium.\n\n    Parameters\n    ----------\n    result_path : str | Path\n        The path to the results directory.\n    networks : dict\n        A dictionary of networks.\n    config : dict\n        Configuration dictionary.\n    \"\"\"\n    statistics = []\n\n    # marginal_cost = collect_myopic_statistics(networks, \"\")\n\n    exporter = Exporter(statistics=statistics, view_config=config[\"view\"])\n    exporter.export(result_path, subdir=config[\"view\"][\"subdir\"])\n</code></pre>"},{"location":"reference/evals/views/transmission/","title":"Transmission","text":"<p>Module for trade evaluations.</p>"},{"location":"reference/evals/views/transmission/#evals.views.transmission.view_grid_capacity","title":"<code>view_grid_capacity(result_path, networks, config, subdir='evaluation')</code>","text":"<p>Export transmission grids to file using Folium.</p> Source code in <code>evals/views/transmission.py</code> <pre><code>def view_grid_capacity(\n    result_path: str | Path,\n    networks: dict,\n    config: dict,\n    subdir: str | Path = \"evaluation\",\n) -&gt; None:  # numpydoc ignore=PR01\n    \"\"\"Export transmission grids to file using Folium.\"\"\"\n\n    # update bus coordinates to improve map readability\n    for n in networks.values():\n        # Lower Austria\n        n.df(\"Bus\").loc[\"AT12\", \"x\"] -= 0.1  # Lon, left\n        n.df(\"Bus\").loc[\"AT12\", \"y\"] += 0.3  # Lat, up\n        # Burgenland\n        n.df(\"Bus\").loc[\"AT11\", \"x\"] -= 0.1  # Lon, left\n        n.df(\"Bus\").loc[\"AT11\", \"y\"] -= 0.26  # Lat, down\n        # Salzburg\n        n.df(\"Bus\").loc[\"AT32\", \"x\"] += 0.35  # Lon, right\n        n.df(\"Bus\").loc[\"AT32\", \"y\"] -= 0.05  # Lat, down\n        # Vienna\n        n.df(\"Bus\").loc[\"AT13\", \"x\"] += 0.1  # Lon, right\n        n.df(\"Bus\").loc[\"AT13\", \"y\"] += 0.1  # Lat, up\n\n    grid_capactiy = collect_myopic_statistics(\n        networks,\n        statistic=\"grid_capacity\",\n        drop_zeros=False,\n        comps=[\"Link\", \"Line\"],\n    )\n\n    # cannot use utils.scale(), because of the additional \"line\" column\n    col = \"Capacity (MW)\"\n    grid_capactiy[col] = grid_capactiy[col] * 1e-3\n    grid_capactiy = grid_capactiy.rename(columns={col: \"Capacity (GW)\"})\n    grid_capactiy.attrs[\"unit\"] = \"GW\"\n\n    import_energy = collect_myopic_statistics(\n        networks,\n        statistic=\"supply\",\n        comps=\"Generator\",\n        bus_carrier=[BusCarrier.CH4, BusCarrier.H2, \"biogas\", \"AC\"],  # \"gas primary\",\n    )\n    import_energy *= 1e-6\n    import_energy.attrs[\"name\"] = \"Import Energy\"\n    import_energy.attrs[\"unit\"] = \"TWh\"\n    metric_name = f\"{import_energy.attrs['name']} ({import_energy.attrs['unit']})\"\n    import_energy.name = metric_name\n\n    # the optimal capacity for pipelines is larger than the maximal\n    # energy flow in the time series, because pipelines are oversized.\n    # We use the maximal flow here since it is more interesting.\n    import_capacity = collect_myopic_statistics(\n        networks,\n        statistic=\"supply\",\n        comps=\"Generator\",\n        bus_carrier=[BusCarrier.CH4, BusCarrier.H2, \"biogas\"],  # \"gas primary\"\n        aggregate_time=\"max\",\n    ).drop(\"2015\", level=DataModel.YEAR, errors=\"ignore\")\n    import_capacity *= 1e-3\n    import_capacity.attrs[\"name\"] = \"Import Capacity\"\n    import_capacity.attrs[\"unit\"] = \"GW\"\n    metric_name = f\"{import_capacity.attrs['name']} ({import_capacity.attrs['unit']})\"\n    import_capacity.name = metric_name\n\n    config = GridMapConfig(show_year=\"2030\")  # fixme: show_year is broken =(\n    buses = networks[next(reversed(networks))].df(\"Bus\")\n\n    # every list item will become one HTML file with a map for the\n    # specified carrier and bus_carrier\n    # ToDo: Add CO2 once it works properly\n    carriers_bus_carrier_groups = (\n        ([Carrier.AC, Carrier.DC], BusCarrier.AC),\n        ([Carrier.gas_pipepline, Carrier.gas_pipepline_new], BusCarrier.CH4),\n        (\n            [  # todo: use get_transmission_techs() instead of hardcoding\n                Carrier.h2_pipeline,\n                Carrier.h2_pipeline_retro,\n                Carrier.h2_pipeline_kernnetz,\n            ],\n            BusCarrier.H2,\n        ),\n    )\n\n    for carriers, bus_carrier in carriers_bus_carrier_groups:\n        df_grid = filter_by(grid_capactiy, carrier=carriers)\n        df_import_energy = filter_by(import_energy, bus_carrier=bus_carrier)\n        df_import_energy = df_import_energy[df_import_energy &gt; 0]\n        df_import_capacity = filter_by(import_capacity, bus_carrier=bus_carrier)\n        df_import_capacity = df_import_capacity[df_import_capacity &gt; 0]\n        grid_map = TransmissionGridMap(\n            df_grid, df_import_energy, df_import_capacity, buses, config\n        )\n        grid_map.draw_grid_by_carrier_groups_myopic()\n        grid_map.save(result_path, f\"gridmap_{bus_carrier}\", subdir)\n</code></pre>"},{"location":"reference/mods/","title":"PyPSA-AT application layer modifications","text":""},{"location":"reference/mods/network_updates/","title":"network_updates.py","text":"<p>Functions to update resources during the <code>snakemake</code> workflow.</p>"},{"location":"reference/mods/network_updates/#mods.network_updates.electricity_base_load_split","title":"<code>electricity_base_load_split(n, snakemake)</code>","text":"<p>Split electricity base load to sectoral loads.</p> Source code in <code>mods/network_updates.py</code> <pre><code>def electricity_base_load_split(n: pypsa.Network, snakemake: Snakemake):\n    \"\"\"Split electricity base load to sectoral loads.\"\"\"\n</code></pre>"},{"location":"reference/mods/network_updates/#mods.network_updates.modify_austrian_gas_storage_capacities","title":"<code>modify_austrian_gas_storage_capacities()</code>","text":"<p>Update gas and H2 storage capacities for Austria.</p> Source code in <code>mods/network_updates.py</code> <pre><code>def modify_austrian_gas_storage_capacities():\n    \"\"\"Update gas and H2 storage capacities for Austria.\"\"\"\n</code></pre>"},{"location":"reference/mods/network_updates/#mods.network_updates.modify_austrian_industry_demand","title":"<code>modify_austrian_industry_demand(existing_industry, year)</code>","text":"<p>Update the industry demand in the PyPSA-AT model for Austria.</p> Source code in <code>mods/network_updates.py</code> <pre><code>def modify_austrian_industry_demand(existing_industry, year):\n    \"\"\"Update the industry demand in the PyPSA-AT model for Austria.\"\"\"\n\n    logger.info(\"Updating industry demand for Austria.\")\n\n    return existing_industry\n</code></pre>"},{"location":"reference/mods/network_updates/#mods.network_updates.modify_austrian_transmission_capacities","title":"<code>modify_austrian_transmission_capacities(n, austrian_transmission_capacities)</code>","text":"<p>Update transmission capacities for Austria.</p> <p>The function is expected to run on clustered pre-networks. It Will read capacities provided in a data file and update the respective values.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>pypsa.Network</code> <p>The pre-network to update during rule <code>modify_prenetwork</code>.</p> required <code>austrian_transmission_capacities</code> <code>str</code> <p>The path to the data file used to update the capacities.</p> required <p>Returns:</p> Type Description Source code in <code>mods/network_updates.py</code> <pre><code>def modify_austrian_transmission_capacities(\n    n: pypsa.Network, austrian_transmission_capacities: str\n):\n    \"\"\"\n    Update transmission capacities for Austria.\n\n    The function is expected to run on clustered pre-networks. It\n    Will read capacities provided in a data file and update the\n    respective values.\n\n    Parameters\n    ----------\n    n\n        The pre-network to update during rule `modify_prenetwork`.\n\n    austrian_transmission_capacities\n        The path to the data file used to update the capacities.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    logger.info(\"Modifying grid capacities for Austria.\")\n\n    # transmission_carrier = get_transmission_carriers(n)\n    # to_concat = []\n    # for component, carrier in transmission_carrier:\n    #     capacity_column = f\"{'p' if component == 'Link' else 's'}_nom\"\n    #     to_concat.append(\n    #         n.static(component).query(f\"carrier == @carrier \"\n    #                                   f\"&amp; (bus0.str.startswith('AT') \"\n    #                                   f\"| bus1.str.startswith('AT'))\")[[\"bus0\", \"bus1\", capacity_column]]\n    #     )\n    # template = pd.concat(to_concat).sort_index()\n    # template.to_csv(austrian_grid_capacities)\n\n    capacities = pd.read_csv(austrian_transmission_capacities, index_col=0).sort_index()\n\n    for c in n.branch_components:\n        p = f\"{'p' if c == 'Link' else 's'}_nom\"\n        overwrite = capacities[[\"bus0\", \"bus1\", p]].dropna(subset=[p])\n        n.static(c).update(overwrite)\n</code></pre>"},{"location":"reference/mods/network_updates/#mods.network_updates.modify_biomass_potentials","title":"<code>modify_biomass_potentials()</code>","text":"<p>Update biomass potentials.</p> Source code in <code>mods/network_updates.py</code> <pre><code>def modify_biomass_potentials():\n    \"\"\"Update biomass potentials.\"\"\"\n</code></pre>"},{"location":"reference/mods/network_updates/#mods.network_updates.modify_heat_demand","title":"<code>modify_heat_demand()</code>","text":"<p>Update heat demands.</p> Source code in <code>mods/network_updates.py</code> <pre><code>def modify_heat_demand():\n    \"\"\"Update heat demands.\"\"\"\n</code></pre>"},{"location":"reference/mods/network_updates/#mods.network_updates.unravel_electricity_base_load","title":"<code>unravel_electricity_base_load(n, snakemake)</code>","text":"<p>Split electricity baseload into sectoral loads.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>pypsa.Network</code> required <code>snakemake</code> <code>snakemake.script.Snakemake</code> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>mods/network_updates.py</code> <pre><code>def unravel_electricity_base_load(n: pypsa.Network, snakemake: Snakemake) -&gt; None:\n    \"\"\"\n    Split electricity baseload into sectoral loads.\n\n    Parameters\n    ----------\n    n\n    snakemake\n\n    Returns\n    -------\n    :\n    \"\"\"\n</code></pre>"},{"location":"reference/mods/network_updates/#mods.network_updates.unravel_gas_import_and_production","title":"<code>unravel_gas_import_and_production(n, snakemake, costs)</code>","text":"<p>Differentiate LNG, pipeline and production gas generators.</p> <p>Production is cheaper than pipeline gas and LNG is more expensive than pipeline gas.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>pypsa.Network</code> <p>The network before optimisation.</p> required <code>snakemake</code> <code>snakemake.script.Snakemake</code> <p>The snakemake workflow object.</p> required <code>costs</code> <code>pandas.DataFrame</code> <p>The costs data for the current planning horizon.</p> required <p>Returns:</p> Type Description Source code in <code>mods/network_updates.py</code> <pre><code>def unravel_gas_import_and_production(\n    n: pypsa.Network, snakemake: Snakemake, costs: pd.DataFrame\n):\n    \"\"\"\n    Differentiate LNG, pipeline and production gas generators.\n\n    Production is cheaper than pipeline gas and LNG is\n    more expensive than pipeline gas.\n\n    Parameters\n    ----------\n    n\n        The network before optimisation.\n    snakemake\n        The snakemake workflow object.\n    costs\n        The costs data for the current planning horizon.\n\n    Returns\n    -------\n    :\n    \"\"\"\n    config = snakemake.config\n    gas_generators = n.static(\"Generator\").query(\"carrier == 'gas'\")\n    if gas_generators.empty and config.get(\"gas_compression_losses\", 0):\n        logger.debug(\n            \"Skipping unravel gas generators because \"\n            \"industry.gas_compression_losses is set.\"\n        )\n        return\n\n    if not config.get(\"mods\", {}).get(\"unravel_natural_gas_imports\", {}).get(\"enable\"):\n        logger.debug(\n            \"Skipping unravel natural gas imports because \"\n            \"the modification was not requested.\"\n        )\n        return\n\n    logger.info(\"Unravel gas import types.\")\n    gas_input_nodes = pd.read_csv(\n        snakemake.input.gas_input_nodes_simplified, index_col=0\n    )\n\n    # remove combined gas generators\n    n.remove(\"Generator\", gas_generators.index)\n    ariadne_gas_fuel_price = costs.at[\"gas\", \"fuel\"]\n    cost_factors = config[\"mods\"][\"unravel_natural_gas_imports\"]\n\n    for import_type in (\"lng\", \"pipeline\", \"production\"):\n        cost_factor = cost_factors[import_type]\n        p_nom = gas_input_nodes[import_type].dropna()\n        p_nom.rename(lambda x: x + \" gas\", inplace=True)\n        nodes = p_nom.index\n        suffix = (\n            \" production\" if import_type == \"production\" else f\" {import_type} import\"\n        )\n        carrier = f\"{import_type} gas\"\n        marginal_cost = ariadne_gas_fuel_price * cost_factor\n        n.add(\n            \"Generator\",\n            nodes,\n            suffix=suffix,\n            bus=nodes,\n            carrier=carrier,\n            p_nom_extendable=False,\n            marginal_cost=marginal_cost,\n            p_nom=p_nom,\n        )\n\n    # make sure that the total gas generator capacity was not changed by this modification\n    old_p_nom = gas_generators[\"p_nom\"].sum()\n    new_p_nom = (\n        n.static(\"Generator\").query(\"carrier.str.endswith(' gas')\")[\"p_nom\"].sum()\n    )\n    assert old_p_nom.round(8) == new_p_nom.round(8), (\n        f\"Unraveling imports changed total capacities: old={old_p_nom}, new={new_p_nom}.\"\n    )\n</code></pre>"},{"location":"tutorials/","title":"Index","text":"<p>This part of the project documentation focuses on a learning-oriented approach. You'll learn how to get started with the code in this project.</p> <p>Note: Expand this section by considering the following points:</p> <ul> <li>Help newcomers with getting started</li> <li>Teach readers about your library by making them     write code</li> <li>Inspire confidence through examples that work for     everyone, repeatably</li> <li>Give readers an immediate sense of achievement</li> <li>Show concrete examples, no abstractions</li> <li>Provide the minimum necessary explanation</li> <li>Avoid any distractions</li> </ul>"},{"location":"tutorials/model-modifications/","title":"Model Modifications","text":""}]}